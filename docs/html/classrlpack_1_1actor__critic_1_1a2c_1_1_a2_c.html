<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RLPack: rlpack.actor_critic.a2c.A2C Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="RLPack-logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">RLPack
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Data Fields</a> &#124;
<a href="#pri-methods">Private Member Functions</a> &#124;
<a href="#pri-attribs">Private Attributes</a>  </div>
  <div class="headertitle"><div class="title">rlpack.actor_critic.a2c.A2C Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>The <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html" title="The A2C class implements the synchronous Actor-Critic method.">A2C</a> class implements the synchronous Actor-Critic method.  
 <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#details">More...</a></p>
<div id="dynsection-0" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-0-trigger" src="closed.png" alt="+"/> Inheritance diagram for rlpack.actor_critic.a2c.A2C:</div>
<div id="dynsection-0-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-0-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c__inherit__graph.png" border="0" usemap="#arlpack_8actor__critic_8a2c_8_a2_c_inherit__map" alt="Inheritance graph"/></div>
<map name="arlpack_8actor__critic_8a2c_8_a2_c_inherit__map" id="arlpack_8actor__critic_8a2c_8_a2_c_inherit__map">
<area shape="rect" title="The A2C class implements the synchronous Actor&#45;Critic method." alt="" coords="5,167,185,192"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html" title="The A2C class implements the synchronous Actor&#45;Critic method." alt="" coords="5,240,185,265"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="17,79,174,119"/>
<area shape="rect" title=" " alt="" coords="67,5,124,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<div id="dynsection-1" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-1-trigger" src="closed.png" alt="+"/> Collaboration diagram for rlpack.actor_critic.a2c.A2C:</div>
<div id="dynsection-1-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-1-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c__coll__graph.png" border="0" usemap="#arlpack_8actor__critic_8a2c_8_a2_c_coll__map" alt="Collaboration graph"/></div>
<map name="arlpack_8actor__critic_8a2c_8_a2_c_coll__map" id="arlpack_8actor__critic_8a2c_8_a2_c_coll__map">
<area shape="rect" title="The A2C class implements the synchronous Actor&#45;Critic method." alt="" coords="5,167,185,192"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="17,79,174,119"/>
<area shape="rect" title=" " alt="" coords="67,5,124,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:adba1d3b5950c18986254b7d1c88e5670"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#adba1d3b5950c18986254b7d1c88e5670">__init__</a> (self, pytorch.nn.Module <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a186663b4b9a13fdff2d3a6c068573a4b">policy_model</a>, pytorch.optim.Optimizer <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a531fa341471a4ad1f97d36c00f221981">optimizer</a>, Union[LRScheduler, None] <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67fd595af5ce5ef1a935083e8383f6af">lr_scheduler</a>, LossFunction <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ace16f409552ad6b3d8274b91be4b5157">loss_function</a>, Distribution <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ad1b2cb43a9ea064b3809838bbb0270f4">distribution</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a335067fc595b3c6bf62395024fd0008a">gamma</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a6def13b25087f4b35685140b0a8e31ff">entropy_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a808c41752e5a391ccccc5eaa820abc09">state_value_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#abca9e9f63b00391c576378683d000d8a">lr_threshold</a>, Union[int, List[Union[int, List[int]]]] <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a4d1c1b160e9cd0cf5ae5a21ace351b56">action_space</a>, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a26e70f7a784551fa58fa81b1f2dcc894">backup_frequency</a>, str <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#affac024bad4b94c555d8299db2e7a64c">save_path</a>, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67a8def97a97cef04686199250556bd0">bootstrap_rounds</a>=1, str <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f3c23d256a14d2b124fb866732051b6">device</a>=&quot;cpu&quot;, Union[int, str] <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a57416c3545397ddaaa31ee69142e67da">apply_norm</a>=-1, Union[int, List[str]] <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeef17e7ec0e0b81e2b47b0c4663bad2c">apply_norm_to</a>=-1, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ab79962d551f8a638f440166462456111">eps_for_norm</a>=5e-12, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af4f2a8721367dc7865955f308cc6149b">p_for_norm</a>=2, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac3f70110b14a2bc1354fadaa02f3bdff">dim_for_norm</a>=0, Optional[float] <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af326d53cbea6b99ed2792a0b88f73539">max_grad_norm</a>=None, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f1d962e32fb21e733c9161a3e5ec206">grad_norm_p</a>=2.0, Optional[Tuple[float, Callable[[float, bool, int], float]]] variance=None)</td></tr>
<tr class="separator:adba1d3b5950c18986254b7d1c88e5670"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0b97082eda91653d7a2511c3325b70f"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af0b97082eda91653d7a2511c3325b70f">load</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:af0b97082eda91653d7a2511c3325b70f"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af0b97082eda91653d7a2511c3325b70f">More...</a><br /></td></tr>
<tr class="separator:af0b97082eda91653d7a2511c3325b70f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afb6171b51be4c79316278355e78fb25e"><td class="memItemLeft" align="right" valign="top">Union[int, np.ndarray]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#afb6171b51be4c79316278355e78fb25e">policy</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, **kwargs)</td></tr>
<tr class="memdesc:afb6171b51be4c79316278355e78fb25e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The policy method to evaluate the agent.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#afb6171b51be4c79316278355e78fb25e">More...</a><br /></td></tr>
<tr class="separator:afb6171b51be4c79316278355e78fb25e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7793678505da4a71178d7a1043c4023d"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a7793678505da4a71178d7a1043c4023d">save</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:a7793678505da4a71178d7a1043c4023d"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a7793678505da4a71178d7a1043c4023d">More...</a><br /></td></tr>
<tr class="separator:a7793678505da4a71178d7a1043c4023d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa8082ac2128495030e29f87cb454ab44"><td class="memItemLeft" align="right" valign="top">Union[int, np.ndarray]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aa8082ac2128495030e29f87cb454ab44">train</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, Union[int, float] reward, Union[bool, int] done, **kwargs)</td></tr>
<tr class="memdesc:aa8082ac2128495030e29f87cb454ab44"><td class="mdescLeft">&#160;</td><td class="mdescRight">The train method to train the agent and underlying policy model.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aa8082ac2128495030e29f87cb454ab44">More...</a><br /></td></tr>
<tr class="separator:aa8082ac2128495030e29f87cb454ab44"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Dict[str, Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">__getstate__</a> (self)</td></tr>
<tr class="memdesc:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To get the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">More...</a><br /></td></tr>
<tr class="separator:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">__init__</a> (self)</td></tr>
<tr class="memdesc:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The class initializer.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">More...</a><br /></td></tr>
<tr class="separator:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">__setstate__</a> (self, Dict[str, Any] state)</td></tr>
<tr class="memdesc:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To load the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">More...</a><br /></td></tr>
<tr class="separator:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">load</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">More...</a><br /></td></tr>
<tr class="separator:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">policy</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Policy method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">More...</a><br /></td></tr>
<tr class="separator:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">save</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">More...</a><br /></td></tr>
<tr class="separator:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">train</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Training method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">More...</a><br /></td></tr>
<tr class="separator:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Data Fields</h2></td></tr>
<tr class="memitem:a989698a1260432ca0fcbd1668fe9c7ac"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a989698a1260432ca0fcbd1668fe9c7ac">action_log_probabilities</a></td></tr>
<tr class="memdesc:a989698a1260432ca0fcbd1668fe9c7ac"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of sampled actions from each timestep from the action distribution.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a989698a1260432ca0fcbd1668fe9c7ac">More...</a><br /></td></tr>
<tr class="separator:a989698a1260432ca0fcbd1668fe9c7ac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4d1c1b160e9cd0cf5ae5a21ace351b56"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a4d1c1b160e9cd0cf5ae5a21ace351b56">action_space</a></td></tr>
<tr class="memdesc:a4d1c1b160e9cd0cf5ae5a21ace351b56"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input number of actions.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a4d1c1b160e9cd0cf5ae5a21ace351b56">More...</a><br /></td></tr>
<tr class="separator:a4d1c1b160e9cd0cf5ae5a21ace351b56"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a57416c3545397ddaaa31ee69142e67da"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a57416c3545397ddaaa31ee69142e67da">apply_norm</a></td></tr>
<tr class="memdesc:a57416c3545397ddaaa31ee69142e67da"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm</code> argument; indicating the normalisation to be used.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a57416c3545397ddaaa31ee69142e67da">More...</a><br /></td></tr>
<tr class="separator:a57416c3545397ddaaa31ee69142e67da"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeef17e7ec0e0b81e2b47b0c4663bad2c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeef17e7ec0e0b81e2b47b0c4663bad2c">apply_norm_to</a></td></tr>
<tr class="memdesc:aeef17e7ec0e0b81e2b47b0c4663bad2c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm_to</code> argument; indicating the quantity to normalise.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeef17e7ec0e0b81e2b47b0c4663bad2c">More...</a><br /></td></tr>
<tr class="separator:aeef17e7ec0e0b81e2b47b0c4663bad2c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a26e70f7a784551fa58fa81b1f2dcc894"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a26e70f7a784551fa58fa81b1f2dcc894">backup_frequency</a></td></tr>
<tr class="memdesc:a26e70f7a784551fa58fa81b1f2dcc894"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input model backup frequency in terms of timesteps.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a26e70f7a784551fa58fa81b1f2dcc894">More...</a><br /></td></tr>
<tr class="separator:a26e70f7a784551fa58fa81b1f2dcc894"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67a8def97a97cef04686199250556bd0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67a8def97a97cef04686199250556bd0">bootstrap_rounds</a></td></tr>
<tr class="memdesc:a67a8def97a97cef04686199250556bd0"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input boostrap rounds.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67a8def97a97cef04686199250556bd0">More...</a><br /></td></tr>
<tr class="separator:a67a8def97a97cef04686199250556bd0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3f3c23d256a14d2b124fb866732051b6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f3c23d256a14d2b124fb866732051b6">device</a></td></tr>
<tr class="memdesc:a3f3c23d256a14d2b124fb866732051b6"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>device</code> argument; indicating the device name.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f3c23d256a14d2b124fb866732051b6">More...</a><br /></td></tr>
<tr class="separator:a3f3c23d256a14d2b124fb866732051b6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac3f70110b14a2bc1354fadaa02f3bdff"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac3f70110b14a2bc1354fadaa02f3bdff">dim_for_norm</a></td></tr>
<tr class="memdesc:ac3f70110b14a2bc1354fadaa02f3bdff"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac3f70110b14a2bc1354fadaa02f3bdff">More...</a><br /></td></tr>
<tr class="separator:ac3f70110b14a2bc1354fadaa02f3bdff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad1b2cb43a9ea064b3809838bbb0270f4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ad1b2cb43a9ea064b3809838bbb0270f4">distribution</a></td></tr>
<tr class="memdesc:ad1b2cb43a9ea064b3809838bbb0270f4"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input distribution object.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ad1b2cb43a9ea064b3809838bbb0270f4">More...</a><br /></td></tr>
<tr class="separator:ad1b2cb43a9ea064b3809838bbb0270f4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac030e4955c3b5317db360d8539e51163"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac030e4955c3b5317db360d8539e51163">entropies</a></td></tr>
<tr class="memdesc:ac030e4955c3b5317db360d8539e51163"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of entropies from each timestep.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac030e4955c3b5317db360d8539e51163">More...</a><br /></td></tr>
<tr class="separator:ac030e4955c3b5317db360d8539e51163"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6def13b25087f4b35685140b0a8e31ff"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a6def13b25087f4b35685140b0a8e31ff">entropy_coefficient</a></td></tr>
<tr class="memdesc:a6def13b25087f4b35685140b0a8e31ff"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input entropy coefficient.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a6def13b25087f4b35685140b0a8e31ff">More...</a><br /></td></tr>
<tr class="separator:a6def13b25087f4b35685140b0a8e31ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab79962d551f8a638f440166462456111"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ab79962d551f8a638f440166462456111">eps_for_norm</a></td></tr>
<tr class="memdesc:ab79962d551f8a638f440166462456111"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ab79962d551f8a638f440166462456111">More...</a><br /></td></tr>
<tr class="separator:ab79962d551f8a638f440166462456111"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a335067fc595b3c6bf62395024fd0008a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a335067fc595b3c6bf62395024fd0008a">gamma</a></td></tr>
<tr class="memdesc:a335067fc595b3c6bf62395024fd0008a"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input discounting factor.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a335067fc595b3c6bf62395024fd0008a">More...</a><br /></td></tr>
<tr class="separator:a335067fc595b3c6bf62395024fd0008a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3f1d962e32fb21e733c9161a3e5ec206"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f1d962e32fb21e733c9161a3e5ec206">grad_norm_p</a></td></tr>
<tr class="memdesc:a3f1d962e32fb21e733c9161a3e5ec206"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f1d962e32fb21e733c9161a3e5ec206">More...</a><br /></td></tr>
<tr class="separator:a3f1d962e32fb21e733c9161a3e5ec206"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0959df6d9aaba07b171fa0bc30d9c306"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a0959df6d9aaba07b171fa0bc30d9c306">is_continuous_action_space</a></td></tr>
<tr class="memdesc:a0959df6d9aaba07b171fa0bc30d9c306"><td class="mdescLeft">&#160;</td><td class="mdescRight">Flag indicating if action space is continuous or discrete.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a0959df6d9aaba07b171fa0bc30d9c306">More...</a><br /></td></tr>
<tr class="separator:a0959df6d9aaba07b171fa0bc30d9c306"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace16f409552ad6b3d8274b91be4b5157"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ace16f409552ad6b3d8274b91be4b5157">loss_function</a></td></tr>
<tr class="memdesc:ace16f409552ad6b3d8274b91be4b5157"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input loss function.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ace16f409552ad6b3d8274b91be4b5157">More...</a><br /></td></tr>
<tr class="separator:ace16f409552ad6b3d8274b91be4b5157"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67fd595af5ce5ef1a935083e8383f6af"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67fd595af5ce5ef1a935083e8383f6af">lr_scheduler</a></td></tr>
<tr class="memdesc:a67fd595af5ce5ef1a935083e8383f6af"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optional LR Scheduler (this can be None).  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67fd595af5ce5ef1a935083e8383f6af">More...</a><br /></td></tr>
<tr class="separator:a67fd595af5ce5ef1a935083e8383f6af"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abca9e9f63b00391c576378683d000d8a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#abca9e9f63b00391c576378683d000d8a">lr_threshold</a></td></tr>
<tr class="memdesc:abca9e9f63b00391c576378683d000d8a"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input LR Threshold.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#abca9e9f63b00391c576378683d000d8a">More...</a><br /></td></tr>
<tr class="separator:abca9e9f63b00391c576378683d000d8a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af326d53cbea6b99ed2792a0b88f73539"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af326d53cbea6b99ed2792a0b88f73539">max_grad_norm</a></td></tr>
<tr class="memdesc:af326d53cbea6b99ed2792a0b88f73539"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af326d53cbea6b99ed2792a0b88f73539">More...</a><br /></td></tr>
<tr class="separator:af326d53cbea6b99ed2792a0b88f73539"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a531fa341471a4ad1f97d36c00f221981"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a531fa341471a4ad1f97d36c00f221981">optimizer</a></td></tr>
<tr class="memdesc:a531fa341471a4ad1f97d36c00f221981"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optimizer wrapped with policy_model parameters.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a531fa341471a4ad1f97d36c00f221981">More...</a><br /></td></tr>
<tr class="separator:a531fa341471a4ad1f97d36c00f221981"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af4f2a8721367dc7865955f308cc6149b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af4f2a8721367dc7865955f308cc6149b">p_for_norm</a></td></tr>
<tr class="memdesc:af4f2a8721367dc7865955f308cc6149b"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af4f2a8721367dc7865955f308cc6149b">More...</a><br /></td></tr>
<tr class="separator:af4f2a8721367dc7865955f308cc6149b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a186663b4b9a13fdff2d3a6c068573a4b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a186663b4b9a13fdff2d3a6c068573a4b">policy_model</a></td></tr>
<tr class="memdesc:a186663b4b9a13fdff2d3a6c068573a4b"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input policy model moved to desired device.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a186663b4b9a13fdff2d3a6c068573a4b">More...</a><br /></td></tr>
<tr class="separator:a186663b4b9a13fdff2d3a6c068573a4b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1cc2aee9a3fdd63cd263a8600f19eaae"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a1cc2aee9a3fdd63cd263a8600f19eaae">rewards</a></td></tr>
<tr class="memdesc:a1cc2aee9a3fdd63cd263a8600f19eaae"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of rewards from each timestep.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a1cc2aee9a3fdd63cd263a8600f19eaae">More...</a><br /></td></tr>
<tr class="separator:a1cc2aee9a3fdd63cd263a8600f19eaae"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:affac024bad4b94c555d8299db2e7a64c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#affac024bad4b94c555d8299db2e7a64c">save_path</a></td></tr>
<tr class="memdesc:affac024bad4b94c555d8299db2e7a64c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input save path for backing up agent models.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#affac024bad4b94c555d8299db2e7a64c">More...</a><br /></td></tr>
<tr class="separator:affac024bad4b94c555d8299db2e7a64c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a808c41752e5a391ccccc5eaa820abc09"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a808c41752e5a391ccccc5eaa820abc09">state_value_coefficient</a></td></tr>
<tr class="memdesc:a808c41752e5a391ccccc5eaa820abc09"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input state value coefficient.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a808c41752e5a391ccccc5eaa820abc09">More...</a><br /></td></tr>
<tr class="separator:a808c41752e5a391ccccc5eaa820abc09"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf2d6eee242ba098e6b977b9b39e5cda"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aaf2d6eee242ba098e6b977b9b39e5cda">states_current_values</a></td></tr>
<tr class="memdesc:aaf2d6eee242ba098e6b977b9b39e5cda"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of state values at each timestep.This is cleared after each episode.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aaf2d6eee242ba098e6b977b9b39e5cda">More...</a><br /></td></tr>
<tr class="separator:aaf2d6eee242ba098e6b977b9b39e5cda"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa40ba92a0c825186f61c04e9ac713a55"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aa40ba92a0c825186f61c04e9ac713a55">step_counter</a></td></tr>
<tr class="memdesc:aa40ba92a0c825186f61c04e9ac713a55"><td class="mdescLeft">&#160;</td><td class="mdescRight">The step counter; counting the total timesteps done so far.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aa40ba92a0c825186f61c04e9ac713a55">More...</a><br /></td></tr>
<tr class="separator:aa40ba92a0c825186f61c04e9ac713a55"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9002f6e78992ea0a133036ac944fc971"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a9002f6e78992ea0a133036ac944fc971">variance_decay_fn</a></td></tr>
<tr class="memdesc:a9002f6e78992ea0a133036ac944fc971"><td class="mdescLeft">&#160;</td><td class="mdescRight">The variance decay method.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a9002f6e78992ea0a133036ac944fc971">More...</a><br /></td></tr>
<tr class="separator:a9002f6e78992ea0a133036ac944fc971"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a87e1de2217cd2e1d25827d6fc4d3f4ca"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a87e1de2217cd2e1d25827d6fc4d3f4ca">variance_value</a></td></tr>
<tr class="memdesc:a87e1de2217cd2e1d25827d6fc4d3f4ca"><td class="mdescLeft">&#160;</td><td class="mdescRight">The current variance value.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a87e1de2217cd2e1d25827d6fc4d3f4ca">More...</a><br /></td></tr>
<tr class="separator:a87e1de2217cd2e1d25827d6fc4d3f4ca"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Data Fields inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">loss</a></td></tr>
<tr class="memdesc:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of losses accumulated after each backward call.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">More...</a><br /></td></tr>
<tr class="separator:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">save_path</a></td></tr>
<tr class="memdesc:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The path to save agent states and models.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">More...</a><br /></td></tr>
<tr class="separator:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-methods" name="pri-methods"></a>
Private Member Functions</h2></td></tr>
<tr class="memitem:a487c58b125fb7cd2e14972552bdf5ea6"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a487c58b125fb7cd2e14972552bdf5ea6">_call_to_save</a> (self)</td></tr>
<tr class="memdesc:a487c58b125fb7cd2e14972552bdf5ea6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method calling the save method when required.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a487c58b125fb7cd2e14972552bdf5ea6">More...</a><br /></td></tr>
<tr class="separator:a487c58b125fb7cd2e14972552bdf5ea6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad43ca7f87560723e0c6e448723e9512"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aad43ca7f87560723e0c6e448723e9512">_call_to_train_policy_model</a> (self, Union[bool, int] done)</td></tr>
<tr class="memdesc:aad43ca7f87560723e0c6e448723e9512"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method to train the policy model.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aad43ca7f87560723e0c6e448723e9512">More...</a><br /></td></tr>
<tr class="separator:aad43ca7f87560723e0c6e448723e9512"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a77355981bba89741f7b8a15e76cf02f8"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a77355981bba89741f7b8a15e76cf02f8">_clear</a> (self)</td></tr>
<tr class="memdesc:a77355981bba89741f7b8a15e76cf02f8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to clear the lists of rewards, action_log_probs and state_values.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a77355981bba89741f7b8a15e76cf02f8">More...</a><br /></td></tr>
<tr class="separator:a77355981bba89741f7b8a15e76cf02f8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeb6b2e6aa1311b4c7850ef224a27f373"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeb6b2e6aa1311b4c7850ef224a27f373">_compute_advantage</a> (self, pytorch.Tensor returns, pytorch.Tensor state_current_values)</td></tr>
<tr class="memdesc:aeb6b2e6aa1311b4c7850ef224a27f373"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the advantage from returns and state values.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeb6b2e6aa1311b4c7850ef224a27f373">More...</a><br /></td></tr>
<tr class="separator:aeb6b2e6aa1311b4c7850ef224a27f373"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c4782c1085d9741f74a5dfc0488e480"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3c4782c1085d9741f74a5dfc0488e480">_compute_loss</a> (self)</td></tr>
<tr class="memdesc:a3c4782c1085d9741f74a5dfc0488e480"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method to compute total loss (from actor and critic).  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3c4782c1085d9741f74a5dfc0488e480">More...</a><br /></td></tr>
<tr class="separator:a3c4782c1085d9741f74a5dfc0488e480"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae6f84e2eed2d807cda9fa3f9334d81a8"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ae6f84e2eed2d807cda9fa3f9334d81a8">_compute_returns</a> (self)</td></tr>
<tr class="memdesc:ae6f84e2eed2d807cda9fa3f9334d81a8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the discounted returns iteratively.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ae6f84e2eed2d807cda9fa3f9334d81a8">More...</a><br /></td></tr>
<tr class="separator:ae6f84e2eed2d807cda9fa3f9334d81a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abddfe99b1ebe51dc9f930cc45bc4d1f8"><td class="memItemLeft" align="right" valign="top">Distribution&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#abddfe99b1ebe51dc9f930cc45bc4d1f8">_create_action_distribution</a> (self, pytorch.Tensor action_values)</td></tr>
<tr class="memdesc:abddfe99b1ebe51dc9f930cc45bc4d1f8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected static method to create distributions from action logits.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#abddfe99b1ebe51dc9f930cc45bc4d1f8">More...</a><br /></td></tr>
<tr class="separator:abddfe99b1ebe51dc9f930cc45bc4d1f8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a93e3f2cd3bef82ef0eb36667c27c13af"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a93e3f2cd3bef82ef0eb36667c27c13af">_grad_mean_reduction</a> (self)</td></tr>
<tr class="memdesc:a93e3f2cd3bef82ef0eb36667c27c13af"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs mean reduction and assigns the policy model's parameter the mean reduced gradients.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a93e3f2cd3bef82ef0eb36667c27c13af">More...</a><br /></td></tr>
<tr class="separator:a93e3f2cd3bef82ef0eb36667c27c13af"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1ff4a067c80053f646906c6603e196a8"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a1ff4a067c80053f646906c6603e196a8">_run_optimizer</a> (self, <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">loss</a>)</td></tr>
<tr class="memdesc:a1ff4a067c80053f646906c6603e196a8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to train the model or accumulate the gradients for training.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a1ff4a067c80053f646906c6603e196a8">More...</a><br /></td></tr>
<tr class="separator:a1ff4a067c80053f646906c6603e196a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-attribs" name="pri-attribs"></a>
Private Attributes</h2></td></tr>
<tr class="memitem:af21943ec874f47fba4ea44ade7f02a5f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af21943ec874f47fba4ea44ade7f02a5f">_grad_accumulator</a></td></tr>
<tr class="memdesc:af21943ec874f47fba4ea44ade7f02a5f"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of gradients from each backward call.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af21943ec874f47fba4ea44ade7f02a5f">More...</a><br /></td></tr>
<tr class="separator:af21943ec874f47fba4ea44ade7f02a5f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a297c096e4155c3ac4815f7ce95ca6767"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a297c096e4155c3ac4815f7ce95ca6767">_normalization</a></td></tr>
<tr class="memdesc:a297c096e4155c3ac4815f7ce95ca6767"><td class="mdescLeft">&#160;</td><td class="mdescRight">The normalisation tool to be used for agent.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a297c096e4155c3ac4815f7ce95ca6767">More...</a><br /></td></tr>
<tr class="separator:a297c096e4155c3ac4815f7ce95ca6767"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a35562483f002903358c95096accfec89"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a35562483f002903358c95096accfec89">_operate_with_variance</a></td></tr>
<tr class="memdesc:a35562483f002903358c95096accfec89"><td class="mdescLeft">&#160;</td><td class="mdescRight">The boolean flag indicating if variance operations are to be used.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a35562483f002903358c95096accfec89">More...</a><br /></td></tr>
<tr class="separator:a35562483f002903358c95096accfec89"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >The <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html" title="The A2C class implements the synchronous Actor-Critic method.">A2C</a> class implements the synchronous Actor-Critic method. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="adba1d3b5950c18986254b7d1c88e5670" name="adba1d3b5950c18986254b7d1c88e5670"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adba1d3b5950c18986254b7d1c88e5670">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.actor_critic.a2c.A2C.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.nn.Module&#160;</td>
          <td class="paramname"><em>policy_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.optim.Optimizer&#160;</td>
          <td class="paramname"><em>optimizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[LRScheduler, None]&#160;</td>
          <td class="paramname"><em>lr_scheduler</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">LossFunction&#160;</td>
          <td class="paramname"><em>loss_function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Distribution&#160;</td>
          <td class="paramname"><em>distribution</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>entropy_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>state_value_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>lr_threshold</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, List[Union[int, List[int]]]]&#160;</td>
          <td class="paramname"><em>action_space</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>backup_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>save_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>bootstrap_rounds</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>device</em> = <code>&quot;cpu&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, str] &#160;</td>
          <td class="paramname"><em>apply_norm</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, List[str]] &#160;</td>
          <td class="paramname"><em>apply_norm_to</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>eps_for_norm</em> = <code>5e-12</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>p_for_norm</em> = <code>2</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>dim_for_norm</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>max_grad_norm</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>grad_norm_p</em> = <code>2.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Tuple[float, Callable[[float, bool, int], float]]] &#160;</td>
          <td class="paramname"><em>variance</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">policy_model</td><td><em>pytorch.nn.Module</em>: The policy model to be used. Policy model must return a tuple of action logits and state values. </td></tr>
    <tr><td class="paramname">optimizer</td><td>pytorch.optim.Optimizer: The optimizer to be used for policy model. Optimizer must be initialized and wrapped with policy model parameters. </td></tr>
    <tr><td class="paramname">lr_scheduler</td><td>Union[LRScheduler, None]: The LR Scheduler to be used to decay the learning rate. LR Scheduler must be initialized and wrapped with passed optimizer. </td></tr>
    <tr><td class="paramname">loss_function</td><td>LossFunction: A PyTorch loss function. </td></tr>
    <tr><td class="paramname">distribution</td><td>: dist_math.distribution.Distribution: The distribution of PyTorch to be used to sampled actions in action space. (See <code>action_space</code>). </td></tr>
    <tr><td class="paramname">gamma</td><td>float: The discounting factor for rewards. </td></tr>
    <tr><td class="paramname">entropy_coefficient</td><td>float: The coefficient to be used for entropy in policy loss computation. </td></tr>
    <tr><td class="paramname">state_value_coefficient</td><td>float: The coefficient to be used for state value in final loss computation. </td></tr>
    <tr><td class="paramname">lr_threshold</td><td>float: The threshold LR which once reached LR scheduler is not called further. </td></tr>
    <tr><td class="paramname">action_space</td><td>Union[int, List[Union[int, List[int]]]]: The action space of the environment. If discrete action set is used, number of actions can be passed. If continuous action space is used, a list must be passed with first element representing the output features from model, second representing the shape of action to be sampled. </td></tr>
    <tr><td class="paramname">backup_frequency</td><td>int: The timesteps after which policy model, optimizer states and lr scheduler states are backed up. </td></tr>
    <tr><td class="paramname">save_path</td><td>str: The path where policy model, optimizer states and lr scheduler states are to be saved. </td></tr>
    <tr><td class="paramname">bootstrap_rounds</td><td>int: The number of rounds until which gradients are to be accumulated before performing calling optimizer step. Gradients are mean reduced for bootstrap_rounds &gt; 1. Default: 1. </td></tr>
    <tr><td class="paramname">device</td><td>str: The device on which models are run. Default: "cpu". </td></tr>
    <tr><td class="paramname">apply_norm</td><td>Union[int, str]: The code to select the normalization procedure to be applied on selected quantities; selected by <code>apply_norm_to</code>: see below)). Direct string can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1 </td></tr>
    <tr><td class="paramname">apply_norm_to</td><td>Union[int, List[str]]: The code to select the quantity to which normalization is to be applied. Direct list of quantities can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1. </td></tr>
    <tr><td class="paramname">eps_for_norm</td><td>float: Epsilon value for normalization; for numeric stability. For min-max normalization and standardized normalization. Default: 5e-12. </td></tr>
    <tr><td class="paramname">p_for_norm</td><td>int: The p value for p-normalization. Default: 2; L2 Norm. </td></tr>
    <tr><td class="paramname">dim_for_norm</td><td>int: The dimension across which normalization is to be performed. Default: 0. </td></tr>
    <tr><td class="paramname">max_grad_norm</td><td>Optional[float]: The max norm for gradients for gradient clipping. Default: None </td></tr>
    <tr><td class="paramname">grad_norm_p</td><td>float: The p-value for p-normalization of gradients. Default: 2.0 </td></tr>
    <tr><td class="paramname">variance</td><td>Optional[Tuple[float, Callable[[float, bool, int], float]]]: The tuple of variance to be used to sample actions for continuous action space and a method to be used to decay it. The passed method have the signature Callable[[float, int], float]. The first argument would be the variance value and second value be the boolean, done flag indicating if the state is terminal or not and third will be the timestep; returning the updated variance value. Default: None</td></tr>
  </table>
  </dd>
</dl>
<p><b>Notes</b></p>
<p >The codes for <code>apply_norm</code> are given as follows: -</p><ul>
<li>No Normalization: -1; (<code>"none"</code>)</li>
<li>Min-Max Normalization: 0; (<code>"min_max"</code>)</li>
<li>Standardization: 1; (<code>"standardize"</code>)</li>
<li>P-Normalization: 2; (<code>"p_norm"</code>)</li>
</ul>
<p >The codes for <code>apply_norm_to</code> are given as follows:</p><ul>
<li>No Normalization: -1; (<code>["none"]</code>)</li>
<li>On States only: 0; (<code>["states"]</code>)</li>
<li>On Rewards only: 1; (<code>["rewards"]</code>)</li>
<li>On TD value only: 2; (<code>["advantage"]</code>)</li>
<li>On States and Rewards: 3; (<code>["states", "rewards"]</code>)</li>
<li>On States and TD: 4; (<code>["states", "advantage"]</code>)</li>
</ul>
<p >If a valid <code>max_norm_grad</code> is passed, then gradient clipping takes place else gradient clipping step is skipped. If <code>max_norm_grad</code> value was invalid, error will be raised from PyTorch. :param distribution: </p>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">rlpack.utils.base.agent.Agent</a>.</p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#aeeade381614a6fc5eb01aff4600bc59a">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a487c58b125fb7cd2e14972552bdf5ea6" name="a487c58b125fb7cd2e14972552bdf5ea6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a487c58b125fb7cd2e14972552bdf5ea6">&#9670;&#160;</a></span>_call_to_save()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._call_to_save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method calling the save method when required. </p>
<p >This method is to be overriden by asynchronous methods. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#ac2f6faa17ef5e68d665f2e085636b5d2">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<a id="aad43ca7f87560723e0c6e448723e9512" name="aad43ca7f87560723e0c6e448723e9512"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad43ca7f87560723e0c6e448723e9512">&#9670;&#160;</a></span>_call_to_train_policy_model()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._call_to_train_policy_model </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method to train the policy model. </p>
<p >If done flag is True, will compute the loss and run the optimizer. This method is meant to periodically check if episode hsa been terminated or and train policy models if episode has terminated. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">done</td><td>Union[bool, int]: Flag indicating if episode has terminated or not </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a77355981bba89741f7b8a15e76cf02f8" name="a77355981bba89741f7b8a15e76cf02f8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a77355981bba89741f7b8a15e76cf02f8">&#9670;&#160;</a></span>_clear()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._clear </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to clear the lists of rewards, action_log_probs and state_values. </p>

</div>
</div>
<a id="aeb6b2e6aa1311b4c7850ef224a27f373" name="aeb6b2e6aa1311b4c7850ef224a27f373"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeb6b2e6aa1311b4c7850ef224a27f373">&#9670;&#160;</a></span>_compute_advantage()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.a2c.A2C._compute_advantage </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>returns</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor
    &#160;</td>
          <td class="paramname"><em>state_current_values</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the advantage from returns and state values. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">returns</td><td>pytorch.Tensor: The discounted returns; computed from _compute_returns method </td></tr>
    <tr><td class="paramname">state_current_values</td><td>pytorch.Tensor: The corresponding state values </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The advantage for the given returns and state values </dd></dl>

</div>
</div>
<a id="a3c4782c1085d9741f74a5dfc0488e480" name="a3c4782c1085d9741f74a5dfc0488e480"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3c4782c1085d9741f74a5dfc0488e480">&#9670;&#160;</a></span>_compute_loss()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.a2c.A2C._compute_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method to compute total loss (from actor and critic). </p>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The loss tensor. </dd></dl>

</div>
</div>
<a id="ae6f84e2eed2d807cda9fa3f9334d81a8" name="ae6f84e2eed2d807cda9fa3f9334d81a8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6f84e2eed2d807cda9fa3f9334d81a8">&#9670;&#160;</a></span>_compute_returns()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.a2c.A2C._compute_returns </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the discounted returns iteratively. </p>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The discounted returns </dd></dl>

</div>
</div>
<a id="abddfe99b1ebe51dc9f930cc45bc4d1f8" name="abddfe99b1ebe51dc9f930cc45bc4d1f8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abddfe99b1ebe51dc9f930cc45bc4d1f8">&#9670;&#160;</a></span>_create_action_distribution()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> Distribution rlpack.actor_critic.a2c.A2C._create_action_distribution </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>action_values</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected static method to create distributions from action logits. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">action_values</td><td>pytorch.Tensor: The action values from policy model </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Distribution: A Distribution object initialized with given action logits </dd></dl>

</div>
</div>
<a id="a93e3f2cd3bef82ef0eb36667c27c13af" name="a93e3f2cd3bef82ef0eb36667c27c13af"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a93e3f2cd3bef82ef0eb36667c27c13af">&#9670;&#160;</a></span>_grad_mean_reduction()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._grad_mean_reduction </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Performs mean reduction and assigns the policy model's parameter the mean reduced gradients. </p>

</div>
</div>
<a id="a1ff4a067c80053f646906c6603e196a8" name="a1ff4a067c80053f646906c6603e196a8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1ff4a067c80053f646906c6603e196a8">&#9670;&#160;</a></span>_run_optimizer()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._run_optimizer </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to train the model or accumulate the gradients for training. </p>
<ul>
<li>If bootstrap_rounds is passed as 1 (default), model is trained each time the method is called.</li>
<li>If bootstrap_rounds &gt; 1, the gradients are accumulated in grad_accumulator and model is trained via _train_models method. </li>
</ul>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#a4436f7743be9e692d44b3af582c25193">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<a id="af0b97082eda91653d7a2511c3325b70f" name="af0b97082eda91653d7a2511c3325b70f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af0b97082eda91653d7a2511c3325b70f">&#9670;&#160;</a></span>load()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C.load </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to load the best model by a custom suffix supplied for evaluation. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="afb6171b51be4c79316278355e78fb25e" name="afb6171b51be4c79316278355e78fb25e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afb6171b51be4c79316278355e78fb25e">&#9670;&#160;</a></span>policy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Union[int, np.ndarray] rlpack.actor_critic.a2c.A2C.policy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The policy method to evaluate the agent. </p>
<p >This runs in pure inference mode. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned from gym environment </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>int: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="a7793678505da4a71178d7a1043c4023d" name="a7793678505da4a71178d7a1043c4023d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7793678505da4a71178d7a1043c4023d">&#9670;&#160;</a></span>save()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C.save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<p >agent_states includes current memory and epsilon values in a dictionary. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to save best model by a custom suffix supplied during a train run. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="aa8082ac2128495030e29f87cb454ab44" name="aa8082ac2128495030e29f87cb454ab44"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa8082ac2128495030e29f87cb454ab44">&#9670;&#160;</a></span>train()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Union[int, np.ndarray] rlpack.actor_critic.a2c.A2C.train </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, float]&#160;</td>
          <td class="paramname"><em>reward</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The train method to train the agent and underlying policy model. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned </td></tr>
    <tr><td class="paramname">reward</td><td>Union[int, float]: The reward returned from previous action </td></tr>
    <tr><td class="paramname">done</td><td>Union[bool, int]: Flag indicating if episode has terminated or not </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>int: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<h2 class="groupheader">Field Documentation</h2>
<a id="af21943ec874f47fba4ea44ade7f02a5f" name="af21943ec874f47fba4ea44ade7f02a5f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af21943ec874f47fba4ea44ade7f02a5f">&#9670;&#160;</a></span>_grad_accumulator</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C._grad_accumulator</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The list of gradients from each backward call. </p>
<p >This is only used when boostrap_rounds &gt; 1 and is cleared after each boostrap round. The <a class="el" href="classrlpack_1_1___c_1_1grad__accumulator_1_1_grad_accumulator.html" title="This class provides the python interface to C_GradAccumulator, the C++ class which performs heavier w...">rlpack._C.grad_accumulator.GradAccumulator</a> object for grad accumulation. </p>

</div>
</div>
<a id="a297c096e4155c3ac4815f7ce95ca6767" name="a297c096e4155c3ac4815f7ce95ca6767"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a297c096e4155c3ac4815f7ce95ca6767">&#9670;&#160;</a></span>_normalization</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C._normalization</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The normalisation tool to be used for agent. </p>
<p >An instance of <a class="el" href="classrlpack_1_1utils_1_1normalization_1_1_normalization.html" title="Normalization class providing methods for normalization techniques.">rlpack.utils.normalization.Normalization</a>. </p>

</div>
</div>
<a id="a35562483f002903358c95096accfec89" name="a35562483f002903358c95096accfec89"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a35562483f002903358c95096accfec89">&#9670;&#160;</a></span>_operate_with_variance</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C._operate_with_variance</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The boolean flag indicating if variance operations are to be used. </p>

</div>
</div>
<a id="a989698a1260432ca0fcbd1668fe9c7ac" name="a989698a1260432ca0fcbd1668fe9c7ac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a989698a1260432ca0fcbd1668fe9c7ac">&#9670;&#160;</a></span>action_log_probabilities</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.action_log_probabilities</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The list of sampled actions from each timestep from the action distribution. </p>
<p >This is cleared after each episode. </p>

</div>
</div>
<a id="a4d1c1b160e9cd0cf5ae5a21ace351b56" name="a4d1c1b160e9cd0cf5ae5a21ace351b56"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4d1c1b160e9cd0cf5ae5a21ace351b56">&#9670;&#160;</a></span>action_space</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.action_space</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input number of actions. </p>

</div>
</div>
<a id="a57416c3545397ddaaa31ee69142e67da" name="a57416c3545397ddaaa31ee69142e67da"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a57416c3545397ddaaa31ee69142e67da">&#9670;&#160;</a></span>apply_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.apply_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm</code> argument; indicating the normalisation to be used. </p>

</div>
</div>
<a id="aeef17e7ec0e0b81e2b47b0c4663bad2c" name="aeef17e7ec0e0b81e2b47b0c4663bad2c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeef17e7ec0e0b81e2b47b0c4663bad2c">&#9670;&#160;</a></span>apply_norm_to</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.apply_norm_to</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm_to</code> argument; indicating the quantity to normalise. </p>

</div>
</div>
<a id="a26e70f7a784551fa58fa81b1f2dcc894" name="a26e70f7a784551fa58fa81b1f2dcc894"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a26e70f7a784551fa58fa81b1f2dcc894">&#9670;&#160;</a></span>backup_frequency</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.backup_frequency</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input model backup frequency in terms of timesteps. </p>

</div>
</div>
<a id="a67a8def97a97cef04686199250556bd0" name="a67a8def97a97cef04686199250556bd0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a67a8def97a97cef04686199250556bd0">&#9670;&#160;</a></span>bootstrap_rounds</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.bootstrap_rounds</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input boostrap rounds. </p>

</div>
</div>
<a id="a3f3c23d256a14d2b124fb866732051b6" name="a3f3c23d256a14d2b124fb866732051b6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3f3c23d256a14d2b124fb866732051b6">&#9670;&#160;</a></span>device</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.device</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>device</code> argument; indicating the device name. </p>

</div>
</div>
<a id="ac3f70110b14a2bc1354fadaa02f3bdff" name="ac3f70110b14a2bc1354fadaa02f3bdff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac3f70110b14a2bc1354fadaa02f3bdff">&#9670;&#160;</a></span>dim_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.dim_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise. </p>

</div>
</div>
<a id="ad1b2cb43a9ea064b3809838bbb0270f4" name="ad1b2cb43a9ea064b3809838bbb0270f4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad1b2cb43a9ea064b3809838bbb0270f4">&#9670;&#160;</a></span>distribution</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.distribution</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input distribution object. </p>

</div>
</div>
<a id="ac030e4955c3b5317db360d8539e51163" name="ac030e4955c3b5317db360d8539e51163"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac030e4955c3b5317db360d8539e51163">&#9670;&#160;</a></span>entropies</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.entropies</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The list of entropies from each timestep. </p>
<p >This is cleared after each episode. </p>

</div>
</div>
<a id="a6def13b25087f4b35685140b0a8e31ff" name="a6def13b25087f4b35685140b0a8e31ff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6def13b25087f4b35685140b0a8e31ff">&#9670;&#160;</a></span>entropy_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.entropy_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input entropy coefficient. </p>

</div>
</div>
<a id="ab79962d551f8a638f440166462456111" name="ab79962d551f8a638f440166462456111"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab79962d551f8a638f440166462456111">&#9670;&#160;</a></span>eps_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.eps_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation. </p>

</div>
</div>
<a id="a335067fc595b3c6bf62395024fd0008a" name="a335067fc595b3c6bf62395024fd0008a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a335067fc595b3c6bf62395024fd0008a">&#9670;&#160;</a></span>gamma</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.gamma</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input discounting factor. </p>

</div>
</div>
<a id="a3f1d962e32fb21e733c9161a3e5ec206" name="a3f1d962e32fb21e733c9161a3e5ec206"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3f1d962e32fb21e733c9161a3e5ec206">&#9670;&#160;</a></span>grad_norm_p</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.grad_norm_p</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings. </p>

</div>
</div>
<a id="a0959df6d9aaba07b171fa0bc30d9c306" name="a0959df6d9aaba07b171fa0bc30d9c306"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0959df6d9aaba07b171fa0bc30d9c306">&#9670;&#160;</a></span>is_continuous_action_space</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.is_continuous_action_space</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Flag indicating if action space is continuous or discrete. </p>

</div>
</div>
<a id="ace16f409552ad6b3d8274b91be4b5157" name="ace16f409552ad6b3d8274b91be4b5157"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ace16f409552ad6b3d8274b91be4b5157">&#9670;&#160;</a></span>loss_function</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.loss_function</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input loss function. </p>

</div>
</div>
<a id="a67fd595af5ce5ef1a935083e8383f6af" name="a67fd595af5ce5ef1a935083e8383f6af"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a67fd595af5ce5ef1a935083e8383f6af">&#9670;&#160;</a></span>lr_scheduler</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.lr_scheduler</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optional LR Scheduler (this can be None). </p>

</div>
</div>
<a id="abca9e9f63b00391c576378683d000d8a" name="abca9e9f63b00391c576378683d000d8a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abca9e9f63b00391c576378683d000d8a">&#9670;&#160;</a></span>lr_threshold</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.lr_threshold</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input LR Threshold. </p>

</div>
</div>
<a id="af326d53cbea6b99ed2792a0b88f73539" name="af326d53cbea6b99ed2792a0b88f73539"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af326d53cbea6b99ed2792a0b88f73539">&#9670;&#160;</a></span>max_grad_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.max_grad_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings. </p>

</div>
</div>
<a id="a531fa341471a4ad1f97d36c00f221981" name="a531fa341471a4ad1f97d36c00f221981"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a531fa341471a4ad1f97d36c00f221981">&#9670;&#160;</a></span>optimizer</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.optimizer</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optimizer wrapped with policy_model parameters. </p>

</div>
</div>
<a id="af4f2a8721367dc7865955f308cc6149b" name="af4f2a8721367dc7865955f308cc6149b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af4f2a8721367dc7865955f308cc6149b">&#9670;&#160;</a></span>p_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.p_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation. </p>

</div>
</div>
<a id="a186663b4b9a13fdff2d3a6c068573a4b" name="a186663b4b9a13fdff2d3a6c068573a4b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a186663b4b9a13fdff2d3a6c068573a4b">&#9670;&#160;</a></span>policy_model</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.policy_model</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input policy model moved to desired device. </p>

</div>
</div>
<a id="a1cc2aee9a3fdd63cd263a8600f19eaae" name="a1cc2aee9a3fdd63cd263a8600f19eaae"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1cc2aee9a3fdd63cd263a8600f19eaae">&#9670;&#160;</a></span>rewards</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.rewards</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The list of rewards from each timestep. </p>
<p >This is cleared after each episode. </p>

</div>
</div>
<a id="affac024bad4b94c555d8299db2e7a64c" name="affac024bad4b94c555d8299db2e7a64c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#affac024bad4b94c555d8299db2e7a64c">&#9670;&#160;</a></span>save_path</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.save_path</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input save path for backing up agent models. </p>

</div>
</div>
<a id="a808c41752e5a391ccccc5eaa820abc09" name="a808c41752e5a391ccccc5eaa820abc09"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a808c41752e5a391ccccc5eaa820abc09">&#9670;&#160;</a></span>state_value_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.state_value_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input state value coefficient. </p>

</div>
</div>
<a id="aaf2d6eee242ba098e6b977b9b39e5cda" name="aaf2d6eee242ba098e6b977b9b39e5cda"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aaf2d6eee242ba098e6b977b9b39e5cda">&#9670;&#160;</a></span>states_current_values</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.states_current_values</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The list of state values at each timestep.This is cleared after each episode. </p>
<p ><br  />
 </p>

</div>
</div>
<a id="aa40ba92a0c825186f61c04e9ac713a55" name="aa40ba92a0c825186f61c04e9ac713a55"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa40ba92a0c825186f61c04e9ac713a55">&#9670;&#160;</a></span>step_counter</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.step_counter</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The step counter; counting the total timesteps done so far. </p>

</div>
</div>
<a id="a9002f6e78992ea0a133036ac944fc971" name="a9002f6e78992ea0a133036ac944fc971"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9002f6e78992ea0a133036ac944fc971">&#9670;&#160;</a></span>variance_decay_fn</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.variance_decay_fn</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The variance decay method. </p>
<p >This will be None if <code>variance</code> argument was not passed </p>

</div>
</div>
<a id="a87e1de2217cd2e1d25827d6fc4d3f4ca" name="a87e1de2217cd2e1d25827d6fc4d3f4ca"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a87e1de2217cd2e1d25827d6fc4d3f4ca">&#9670;&#160;</a></span>variance_value</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.variance_value</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The current variance value. </p>
<p >This will be None if <code>variance</code> argument was not passed </p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacerlpack.html">rlpack</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic.html">actor_critic</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic_1_1a2c.html">a2c</a></li><li class="navelem"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html">A2C</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
