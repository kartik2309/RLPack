<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RLPack: rlpack.actor_critic.a2c.A2C Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="RLPack-logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">RLPack
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pri-methods">Private Member Functions</a> &#124;
<a href="#pri-static-methods">Static Private Member Functions</a> &#124;
<a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">rlpack.actor_critic.a2c.A2C Class Reference</div></div>
</div><!--header-->
<div class="contents">
<div id="dynsection-0" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-0-trigger" src="closed.png" alt="+"/> Inheritance diagram for rlpack.actor_critic.a2c.A2C:</div>
<div id="dynsection-0-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-0-content" class="dyncontent" style="display:none;">
 <div class="center">
  <img src="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.png" usemap="#rlpack.actor_5Fcritic.a2c.A2C_map" alt=""/>
  <map id="rlpack.actor_5Fcritic.a2c.A2C_map" name="rlpack.actor_5Fcritic.a2c.A2C_map">
<area href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" alt="rlpack.utils.base.agent.Agent" shape="rect" coords="0,56,176,80"/>
  </map>
</div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a09da18ac5db6ca9cb283dec0015b63f6"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a09da18ac5db6ca9cb283dec0015b63f6">__init__</a> (self, pytorch.nn.Module policy_model, pytorch.optim.Optimizer optimizer, Union[LRScheduler, None] lr_scheduler, LossFunction loss_function, float gamma, float entropy_coefficient, float state_value_coefficient, float lr_threshold, int num_actions, int model_backup_frequency, str save_path, int bootstrap_rounds=1, str device=&quot;cpu&quot;, int apply_norm=-1, int apply_norm_to=-1, float eps_for_norm=5e-12, int p_for_norm=2, int dim_for_norm=0)</td></tr>
<tr class="separator:a09da18ac5db6ca9cb283dec0015b63f6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aae67c23042778a7f6c3e515939b6b4f4"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aae67c23042778a7f6c3e515939b6b4f4">compute_advantage</a> (self, pytorch.Tensor returns, pytorch.Tensor state_current_values)</td></tr>
<tr class="separator:aae67c23042778a7f6c3e515939b6b4f4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5e480b8b9914e283440e5278a97b42cf"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a5e480b8b9914e283440e5278a97b42cf">policy</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, **kwargs)</td></tr>
<tr class="separator:a5e480b8b9914e283440e5278a97b42cf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae2c4c474849c70cdedf6b98aea2f63c4"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ae2c4c474849c70cdedf6b98aea2f63c4">train</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, Union[int, float] reward, Union[bool, int] done, **kwargs)</td></tr>
<tr class="separator:ae2c4c474849c70cdedf6b98aea2f63c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Dict[str, Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">__getstate__</a> (self)</td></tr>
<tr class="separator:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">__init__</a> (self)</td></tr>
<tr class="separator:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">__setstate__</a> (self, Dict[str, Any] state)</td></tr>
<tr class="separator:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">load</a> (self, *args, **kwargs)</td></tr>
<tr class="separator:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">policy</a> (self, *args, **kwargs)</td></tr>
<tr class="separator:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">save</a> (self, *args, **kwargs)</td></tr>
<tr class="separator:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">train</a> (self, *args, **kwargs)</td></tr>
<tr class="separator:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-methods" name="pri-methods"></a>
Private Member Functions</h2></td></tr>
<tr class="memitem:acd82c3f9cd2ad2d4e0b91d7e3fb5bb5f"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#acd82c3f9cd2ad2d4e0b91d7e3fb5bb5f">__accumulate_gradients</a> (self)</td></tr>
<tr class="separator:acd82c3f9cd2ad2d4e0b91d7e3fb5bb5f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afe13948ea80cf5b000ec053af95e2c37"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#afe13948ea80cf5b000ec053af95e2c37">__call_train_policy_model</a> (self, Union[bool, int] done)</td></tr>
<tr class="separator:afe13948ea80cf5b000ec053af95e2c37"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d0e4e113f005db362a2193915d3523d"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a7d0e4e113f005db362a2193915d3523d">__clear</a> (self)</td></tr>
<tr class="separator:a7d0e4e113f005db362a2193915d3523d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad37dea33877ba397af1804dee08b894b"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ad37dea33877ba397af1804dee08b894b">__compute_returns</a> (self)</td></tr>
<tr class="separator:ad37dea33877ba397af1804dee08b894b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af54c8f30f17dabd5d410161107711f0d"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af54c8f30f17dabd5d410161107711f0d">__train_models</a> (self)</td></tr>
<tr class="separator:af54c8f30f17dabd5d410161107711f0d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-static-methods" name="pri-static-methods"></a>
Static Private Member Functions</h2></td></tr>
<tr class="memitem:a197c5334abb53f7852eba483ab4585fd"><td class="memItemLeft" align="right" valign="top">Categorical&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a197c5334abb53f7852eba483ab4585fd">__create_action_distribution</a> (pytorch.Tensor actions_logits)</td></tr>
<tr class="separator:a197c5334abb53f7852eba483ab4585fd"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">The A2C class implements the synchronous Actor-Critic method.
</pre> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a09da18ac5db6ca9cb283dec0015b63f6" name="a09da18ac5db6ca9cb283dec0015b63f6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a09da18ac5db6ca9cb283dec0015b63f6">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.actor_critic.a2c.A2C.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.nn.Module&#160;</td>
          <td class="paramname"><em>policy_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.optim.Optimizer&#160;</td>
          <td class="paramname"><em>optimizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[LRScheduler, None]&#160;</td>
          <td class="paramname"><em>lr_scheduler</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">LossFunction&#160;</td>
          <td class="paramname"><em>loss_function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>entropy_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>state_value_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>lr_threshold</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>model_backup_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>save_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>bootstrap_rounds</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>device</em> = <code>&quot;cpu&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>apply_norm</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>apply_norm_to</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>eps_for_norm</em> = <code>5e-12</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>p_for_norm</em> = <code>2</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>dim_for_norm</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">:param policy_model: pytorch.nn.Module: The policy model to be used. Policy model must return a tuple of
    action logits and state values.
:param optimizer: pytorch.optim.Optimizer: The optimizer to be used for policy model. Optimizer must be
    initialized and wrapped with policy model parameters.
:param lr_scheduler: Union[LRScheduler, None]: The LR Scheduler to be used to decay the learning rate.
    LR Scheduler must be initialized and wrapped with passed optimizer.
:param loss_function: LossFunction: A PyTorch loss function.
:param gamma: float: The discounting factor for rewards.
:param entropy_coefficient: float: The coefficient to be used for entropy in policy loss computation.
:param state_value_coefficient: float: The coefficient to be used for state value in final loss computation.
:param lr_threshold: float: The threshold LR which once reached LR scheduler is not called further.
:param num_actions: int: Number of actions for the environment.
:param model_backup_frequency: int: The timesteps after which policy model, optimizer states and lr
    scheduler states are backed up.
:param save_path: str: The path where policy model, optimizer states and lr scheduler states are to be saved.
:param bootstrap_rounds: int: The number of rounds until which gradients are to be accumulated before
    performing calling optimizer step. Gradients are mean reduced for bootstrap_rounds &gt; 1. Default: 1.
:param device: str: The device on which models are run. Default: "cpu".
:param apply_norm: int: The code to select the normalization procedure to be applied on selected quantities;
    selected by `apply_norm_to`: see below)). Default: -1.
:param apply_norm_to: int: The code to select the quantity to which normalization is to be applied.
    Default: -1.
:param eps_for_norm: float: Epsilon value for normalization; for numeric stability. For min-max normalization
    and standardized normalization. Default: 5e-12.
:param p_for_norm: int: The p value for p-normalization. Default: 2; L2 Norm.
:param dim_for_norm: int: The dimension across which normalization is to be performed. Default: 0.

The codes for `apply_norm` are given as follows: -
    - No Normalization: -1
    - Min-Max Normalization: 0
    - Standardization: 1
    - P-Normalization: 2
The codes for `apply_norm_to` are given as follows:
    No Normalization: -1
    On States only: 0
    On Rewards only: 1
    On Advantage value only: 2
    On States and Rewards: 3
    On States and Advantage: 4
</pre> 
<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="acd82c3f9cd2ad2d4e0b91d7e3fb5bb5f" name="acd82c3f9cd2ad2d4e0b91d7e3fb5bb5f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acd82c3f9cd2ad2d4e0b91d7e3fb5bb5f">&#9670;&#160;</a></span>__accumulate_gradients()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C.__accumulate_gradients </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Private void method to train the model or accumulate the gradients for training.
- If bootstrap_rounds is passed as 1 (default), model is trained each time the method is called.
- If bootstrap_rounds &gt; 1, the gradients are accumulated in grad_accumulator and model is trained via
    __train_models method.
</pre> 
</div>
</div>
<a id="afe13948ea80cf5b000ec053af95e2c37" name="afe13948ea80cf5b000ec053af95e2c37"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afe13948ea80cf5b000ec053af95e2c37">&#9670;&#160;</a></span>__call_train_policy_model()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C.__call_train_policy_model </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Private method to call the appropriate method for training policy model based on initialization of A2C agent
:param done: Union[bool, int]: Flag indicating if episode has terminated or not
</pre> 
</div>
</div>
<a id="a7d0e4e113f005db362a2193915d3523d" name="a7d0e4e113f005db362a2193915d3523d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7d0e4e113f005db362a2193915d3523d">&#9670;&#160;</a></span>__clear()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C.__clear </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Private void method to clear the lists of rewards, action_log_probs and state_values.
</pre> 
</div>
</div>
<a id="ad37dea33877ba397af1804dee08b894b" name="ad37dea33877ba397af1804dee08b894b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad37dea33877ba397af1804dee08b894b">&#9670;&#160;</a></span>__compute_returns()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.a2c.A2C.__compute_returns </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Computes the discounted returns iteratively.
:return: pytorch.Tensor: The discounted returns
</pre> 
</div>
</div>
<a id="a197c5334abb53f7852eba483ab4585fd" name="a197c5334abb53f7852eba483ab4585fd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a197c5334abb53f7852eba483ab4585fd">&#9670;&#160;</a></span>__create_action_distribution()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> Categorical rlpack.actor_critic.a2c.A2C.__create_action_distribution </td>
          <td>(</td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>actions_logits</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Private static method to create distributions from action logits
:param actions_logits: pytorch.Tensor: The action logits from policy model
:return: Categorical: A Categorical object initialized with given action logits
</pre> 
</div>
</div>
<a id="af54c8f30f17dabd5d410161107711f0d" name="af54c8f30f17dabd5d410161107711f0d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af54c8f30f17dabd5d410161107711f0d">&#9670;&#160;</a></span>__train_models()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C.__train_models </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Private method to policy model if boostrap_rounds &gt; 1. In such cases the gradients are accumulated in
grad_accumulator. This method collects the accumulated gradients and performs mean reduction and runs
optimizer step.
</pre> 
</div>
</div>
<a id="aae67c23042778a7f6c3e515939b6b4f4" name="aae67c23042778a7f6c3e515939b6b4f4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aae67c23042778a7f6c3e515939b6b4f4">&#9670;&#160;</a></span>compute_advantage()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.a2c.A2C.compute_advantage </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>returns</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor
    &#160;</td>
          <td class="paramname"><em>state_current_values</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Computes the advantage from returns and state values
:param returns: pytorch.Tensor: The discounted returns; computed from __compute_returns method
:param state_current_values: pytorch.Tensor: The corresponding state values
:return: pytorch.Tensor: The advantage for the given returns and state values
</pre> 
</div>
</div>
<a id="a5e480b8b9914e283440e5278a97b42cf" name="a5e480b8b9914e283440e5278a97b42cf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5e480b8b9914e283440e5278a97b42cf">&#9670;&#160;</a></span>policy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> int rlpack.actor_critic.a2c.A2C.policy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The policy method to evaluate the agent. This runs in pure inference mode.
:param state_current: Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned
    from gym environment
:param kwargs: Other keyword arguments
:return: int: The action to be taken
</pre> 
<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="ae2c4c474849c70cdedf6b98aea2f63c4" name="ae2c4c474849c70cdedf6b98aea2f63c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae2c4c474849c70cdedf6b98aea2f63c4">&#9670;&#160;</a></span>train()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> int rlpack.actor_critic.a2c.A2C.train </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, float]&#160;</td>
          <td class="paramname"><em>reward</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The train method to train the agent and underlying policy model.
:param state_current: Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned
:param reward: Union[int, float]: The reward returned from previous action
:param done: Union[bool, int]: Flag indicating if episode has terminated or not
:param kwargs: Other keyword arguments.
:return: int: The action to be taken
</pre> 
<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><b>rlpack</b></li><li class="navelem"><b>actor_critic</b></li><li class="navelem"><b>a2c</b></li><li class="navelem"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html">A2C</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
