<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RLPack: rlpack.actor_critic.a2c.A2C Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="RLPack-logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">RLPack
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Data Fields</a> &#124;
<a href="#pri-methods">Private Member Functions</a> &#124;
<a href="#pri-static-methods">Static Private Member Functions</a> &#124;
<a href="#pri-attribs">Private Attributes</a>  </div>
  <div class="headertitle"><div class="title">rlpack.actor_critic.a2c.A2C Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>The <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html" title="The A2C class implements the synchronous Actor-Critic method.">A2C</a> class implements the synchronous Actor-Critic method.  
 <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#details">More...</a></p>
<div id="dynsection-0" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-0-trigger" src="closed.png" alt="+"/> Inheritance diagram for rlpack.actor_critic.a2c.A2C:</div>
<div id="dynsection-0-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-0-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c__inherit__graph.png" border="0" usemap="#arlpack_8actor__critic_8a2c_8_a2_c_inherit__map" alt="Inheritance graph"/></div>
<map name="arlpack_8actor__critic_8a2c_8_a2_c_inherit__map" id="arlpack_8actor__critic_8a2c_8_a2_c_inherit__map">
<area shape="rect" title="The A2C class implements the synchronous Actor&#45;Critic method." alt="" coords="5,167,185,192"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html" title="The A2C class implements the synchronous Actor&#45;Critic method." alt="" coords="5,240,185,265"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="17,79,174,119"/>
<area shape="rect" title=" " alt="" coords="67,5,124,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<div id="dynsection-1" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-1-trigger" src="closed.png" alt="+"/> Collaboration diagram for rlpack.actor_critic.a2c.A2C:</div>
<div id="dynsection-1-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-1-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c__coll__graph.png" border="0" usemap="#arlpack_8actor__critic_8a2c_8_a2_c_coll__map" alt="Collaboration graph"/></div>
<map name="arlpack_8actor__critic_8a2c_8_a2_c_coll__map" id="arlpack_8actor__critic_8a2c_8_a2_c_coll__map">
<area shape="rect" title="The A2C class implements the synchronous Actor&#45;Critic method." alt="" coords="5,167,185,192"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="17,79,174,119"/>
<area shape="rect" title=" " alt="" coords="67,5,124,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a85826dab437582b48e1d1919e78e325f"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a85826dab437582b48e1d1919e78e325f">__init__</a> (self, pytorch.nn.Module <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a186663b4b9a13fdff2d3a6c068573a4b">policy_model</a>, pytorch.optim.Optimizer <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a531fa341471a4ad1f97d36c00f221981">optimizer</a>, Union[LRScheduler, None] <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67fd595af5ce5ef1a935083e8383f6af">lr_scheduler</a>, LossFunction <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ace16f409552ad6b3d8274b91be4b5157">loss_function</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a335067fc595b3c6bf62395024fd0008a">gamma</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a6def13b25087f4b35685140b0a8e31ff">entropy_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a808c41752e5a391ccccc5eaa820abc09">state_value_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#abca9e9f63b00391c576378683d000d8a">lr_threshold</a>, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a6b420f1579a75aae43c82105a98c311e">num_actions</a>, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a26e70f7a784551fa58fa81b1f2dcc894">backup_frequency</a>, str <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#affac024bad4b94c555d8299db2e7a64c">save_path</a>, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67a8def97a97cef04686199250556bd0">bootstrap_rounds</a>=1, str <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f3c23d256a14d2b124fb866732051b6">device</a>=&quot;cpu&quot;, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a57416c3545397ddaaa31ee69142e67da">apply_norm</a>=-1, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeef17e7ec0e0b81e2b47b0c4663bad2c">apply_norm_to</a>=-1, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ab79962d551f8a638f440166462456111">eps_for_norm</a>=5e-12, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af4f2a8721367dc7865955f308cc6149b">p_for_norm</a>=2, int <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac3f70110b14a2bc1354fadaa02f3bdff">dim_for_norm</a>=0, Optional[float] <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af326d53cbea6b99ed2792a0b88f73539">max_grad_norm</a>=None, float <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f1d962e32fb21e733c9161a3e5ec206">grad_norm_p</a>=2.0)</td></tr>
<tr class="separator:a85826dab437582b48e1d1919e78e325f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0b97082eda91653d7a2511c3325b70f"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af0b97082eda91653d7a2511c3325b70f">load</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:af0b97082eda91653d7a2511c3325b70f"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af0b97082eda91653d7a2511c3325b70f">More...</a><br /></td></tr>
<tr class="separator:af0b97082eda91653d7a2511c3325b70f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5e480b8b9914e283440e5278a97b42cf"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a5e480b8b9914e283440e5278a97b42cf">policy</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, **kwargs)</td></tr>
<tr class="memdesc:a5e480b8b9914e283440e5278a97b42cf"><td class="mdescLeft">&#160;</td><td class="mdescRight">The policy method to evaluate the agent.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a5e480b8b9914e283440e5278a97b42cf">More...</a><br /></td></tr>
<tr class="separator:a5e480b8b9914e283440e5278a97b42cf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7793678505da4a71178d7a1043c4023d"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a7793678505da4a71178d7a1043c4023d">save</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:a7793678505da4a71178d7a1043c4023d"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a7793678505da4a71178d7a1043c4023d">More...</a><br /></td></tr>
<tr class="separator:a7793678505da4a71178d7a1043c4023d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae2c4c474849c70cdedf6b98aea2f63c4"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ae2c4c474849c70cdedf6b98aea2f63c4">train</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, Union[int, float] reward, Union[bool, int] done, **kwargs)</td></tr>
<tr class="memdesc:ae2c4c474849c70cdedf6b98aea2f63c4"><td class="mdescLeft">&#160;</td><td class="mdescRight">The train method to train the agent and underlying policy model.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ae2c4c474849c70cdedf6b98aea2f63c4">More...</a><br /></td></tr>
<tr class="separator:ae2c4c474849c70cdedf6b98aea2f63c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Dict[str, Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">__getstate__</a> (self)</td></tr>
<tr class="memdesc:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To get the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">More...</a><br /></td></tr>
<tr class="separator:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">__init__</a> (self)</td></tr>
<tr class="memdesc:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The class initializer.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">More...</a><br /></td></tr>
<tr class="separator:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">__setstate__</a> (self, Dict[str, Any] state)</td></tr>
<tr class="memdesc:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To load the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">More...</a><br /></td></tr>
<tr class="separator:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">load</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">More...</a><br /></td></tr>
<tr class="separator:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">policy</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Policy method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">More...</a><br /></td></tr>
<tr class="separator:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">save</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">More...</a><br /></td></tr>
<tr class="separator:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">train</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Training method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">More...</a><br /></td></tr>
<tr class="separator:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Data Fields</h2></td></tr>
<tr class="memitem:a989698a1260432ca0fcbd1668fe9c7ac"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a989698a1260432ca0fcbd1668fe9c7ac">action_log_probabilities</a></td></tr>
<tr class="memdesc:a989698a1260432ca0fcbd1668fe9c7ac"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of sampled actions from each timestep from the action distribution.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a989698a1260432ca0fcbd1668fe9c7ac">More...</a><br /></td></tr>
<tr class="separator:a989698a1260432ca0fcbd1668fe9c7ac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a57416c3545397ddaaa31ee69142e67da"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a57416c3545397ddaaa31ee69142e67da">apply_norm</a></td></tr>
<tr class="memdesc:a57416c3545397ddaaa31ee69142e67da"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm</code> argument; indicating the normalisation to be used.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a57416c3545397ddaaa31ee69142e67da">More...</a><br /></td></tr>
<tr class="separator:a57416c3545397ddaaa31ee69142e67da"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeef17e7ec0e0b81e2b47b0c4663bad2c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeef17e7ec0e0b81e2b47b0c4663bad2c">apply_norm_to</a></td></tr>
<tr class="memdesc:aeef17e7ec0e0b81e2b47b0c4663bad2c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm_to</code> argument; indicating the quantity to normalise.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeef17e7ec0e0b81e2b47b0c4663bad2c">More...</a><br /></td></tr>
<tr class="separator:aeef17e7ec0e0b81e2b47b0c4663bad2c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a26e70f7a784551fa58fa81b1f2dcc894"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a26e70f7a784551fa58fa81b1f2dcc894">backup_frequency</a></td></tr>
<tr class="memdesc:a26e70f7a784551fa58fa81b1f2dcc894"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input model backup frequency in terms of timesteps.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a26e70f7a784551fa58fa81b1f2dcc894">More...</a><br /></td></tr>
<tr class="separator:a26e70f7a784551fa58fa81b1f2dcc894"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67a8def97a97cef04686199250556bd0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67a8def97a97cef04686199250556bd0">bootstrap_rounds</a></td></tr>
<tr class="memdesc:a67a8def97a97cef04686199250556bd0"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input boostrap rounds.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67a8def97a97cef04686199250556bd0">More...</a><br /></td></tr>
<tr class="separator:a67a8def97a97cef04686199250556bd0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3f3c23d256a14d2b124fb866732051b6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f3c23d256a14d2b124fb866732051b6">device</a></td></tr>
<tr class="memdesc:a3f3c23d256a14d2b124fb866732051b6"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>device</code> argument; indicating the device name.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f3c23d256a14d2b124fb866732051b6">More...</a><br /></td></tr>
<tr class="separator:a3f3c23d256a14d2b124fb866732051b6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac3f70110b14a2bc1354fadaa02f3bdff"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac3f70110b14a2bc1354fadaa02f3bdff">dim_for_norm</a></td></tr>
<tr class="memdesc:ac3f70110b14a2bc1354fadaa02f3bdff"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac3f70110b14a2bc1354fadaa02f3bdff">More...</a><br /></td></tr>
<tr class="separator:ac3f70110b14a2bc1354fadaa02f3bdff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac030e4955c3b5317db360d8539e51163"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac030e4955c3b5317db360d8539e51163">entropies</a></td></tr>
<tr class="memdesc:ac030e4955c3b5317db360d8539e51163"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of entropies from each timestep.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ac030e4955c3b5317db360d8539e51163">More...</a><br /></td></tr>
<tr class="separator:ac030e4955c3b5317db360d8539e51163"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6def13b25087f4b35685140b0a8e31ff"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a6def13b25087f4b35685140b0a8e31ff">entropy_coefficient</a></td></tr>
<tr class="memdesc:a6def13b25087f4b35685140b0a8e31ff"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input entropy coefficient.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a6def13b25087f4b35685140b0a8e31ff">More...</a><br /></td></tr>
<tr class="separator:a6def13b25087f4b35685140b0a8e31ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad806193a87f792ffdc0e349f0fa5f3e2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ad806193a87f792ffdc0e349f0fa5f3e2">episode_counter</a></td></tr>
<tr class="memdesc:ad806193a87f792ffdc0e349f0fa5f3e2"><td class="mdescLeft">&#160;</td><td class="mdescRight">The episode counter; counting the total episodes done so far.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ad806193a87f792ffdc0e349f0fa5f3e2">More...</a><br /></td></tr>
<tr class="separator:ad806193a87f792ffdc0e349f0fa5f3e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab79962d551f8a638f440166462456111"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ab79962d551f8a638f440166462456111">eps_for_norm</a></td></tr>
<tr class="memdesc:ab79962d551f8a638f440166462456111"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ab79962d551f8a638f440166462456111">More...</a><br /></td></tr>
<tr class="separator:ab79962d551f8a638f440166462456111"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a335067fc595b3c6bf62395024fd0008a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a335067fc595b3c6bf62395024fd0008a">gamma</a></td></tr>
<tr class="memdesc:a335067fc595b3c6bf62395024fd0008a"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input discounting factor.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a335067fc595b3c6bf62395024fd0008a">More...</a><br /></td></tr>
<tr class="separator:a335067fc595b3c6bf62395024fd0008a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3f1d962e32fb21e733c9161a3e5ec206"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f1d962e32fb21e733c9161a3e5ec206">grad_norm_p</a></td></tr>
<tr class="memdesc:a3f1d962e32fb21e733c9161a3e5ec206"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3f1d962e32fb21e733c9161a3e5ec206">More...</a><br /></td></tr>
<tr class="separator:a3f1d962e32fb21e733c9161a3e5ec206"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace16f409552ad6b3d8274b91be4b5157"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ace16f409552ad6b3d8274b91be4b5157">loss_function</a></td></tr>
<tr class="memdesc:ace16f409552ad6b3d8274b91be4b5157"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input loss function.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ace16f409552ad6b3d8274b91be4b5157">More...</a><br /></td></tr>
<tr class="separator:ace16f409552ad6b3d8274b91be4b5157"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67fd595af5ce5ef1a935083e8383f6af"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67fd595af5ce5ef1a935083e8383f6af">lr_scheduler</a></td></tr>
<tr class="memdesc:a67fd595af5ce5ef1a935083e8383f6af"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optional LR Scheduler (this can be None).  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a67fd595af5ce5ef1a935083e8383f6af">More...</a><br /></td></tr>
<tr class="separator:a67fd595af5ce5ef1a935083e8383f6af"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abca9e9f63b00391c576378683d000d8a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#abca9e9f63b00391c576378683d000d8a">lr_threshold</a></td></tr>
<tr class="memdesc:abca9e9f63b00391c576378683d000d8a"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input LR Threshold.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#abca9e9f63b00391c576378683d000d8a">More...</a><br /></td></tr>
<tr class="separator:abca9e9f63b00391c576378683d000d8a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af326d53cbea6b99ed2792a0b88f73539"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af326d53cbea6b99ed2792a0b88f73539">max_grad_norm</a></td></tr>
<tr class="memdesc:af326d53cbea6b99ed2792a0b88f73539"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af326d53cbea6b99ed2792a0b88f73539">More...</a><br /></td></tr>
<tr class="separator:af326d53cbea6b99ed2792a0b88f73539"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6b420f1579a75aae43c82105a98c311e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a6b420f1579a75aae43c82105a98c311e">num_actions</a></td></tr>
<tr class="memdesc:a6b420f1579a75aae43c82105a98c311e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input number of actions.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a6b420f1579a75aae43c82105a98c311e">More...</a><br /></td></tr>
<tr class="separator:a6b420f1579a75aae43c82105a98c311e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a531fa341471a4ad1f97d36c00f221981"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a531fa341471a4ad1f97d36c00f221981">optimizer</a></td></tr>
<tr class="memdesc:a531fa341471a4ad1f97d36c00f221981"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optimizer wrapped with policy_model parameters.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a531fa341471a4ad1f97d36c00f221981">More...</a><br /></td></tr>
<tr class="separator:a531fa341471a4ad1f97d36c00f221981"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af4f2a8721367dc7865955f308cc6149b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af4f2a8721367dc7865955f308cc6149b">p_for_norm</a></td></tr>
<tr class="memdesc:af4f2a8721367dc7865955f308cc6149b"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af4f2a8721367dc7865955f308cc6149b">More...</a><br /></td></tr>
<tr class="separator:af4f2a8721367dc7865955f308cc6149b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a186663b4b9a13fdff2d3a6c068573a4b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a186663b4b9a13fdff2d3a6c068573a4b">policy_model</a></td></tr>
<tr class="memdesc:a186663b4b9a13fdff2d3a6c068573a4b"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input policy model moved to desired device.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a186663b4b9a13fdff2d3a6c068573a4b">More...</a><br /></td></tr>
<tr class="separator:a186663b4b9a13fdff2d3a6c068573a4b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1cc2aee9a3fdd63cd263a8600f19eaae"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a1cc2aee9a3fdd63cd263a8600f19eaae">rewards</a></td></tr>
<tr class="memdesc:a1cc2aee9a3fdd63cd263a8600f19eaae"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of rewards from each timestep.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a1cc2aee9a3fdd63cd263a8600f19eaae">More...</a><br /></td></tr>
<tr class="separator:a1cc2aee9a3fdd63cd263a8600f19eaae"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:affac024bad4b94c555d8299db2e7a64c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#affac024bad4b94c555d8299db2e7a64c">save_path</a></td></tr>
<tr class="memdesc:affac024bad4b94c555d8299db2e7a64c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input save path for backing up agent models.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#affac024bad4b94c555d8299db2e7a64c">More...</a><br /></td></tr>
<tr class="separator:affac024bad4b94c555d8299db2e7a64c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a808c41752e5a391ccccc5eaa820abc09"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a808c41752e5a391ccccc5eaa820abc09">state_value_coefficient</a></td></tr>
<tr class="memdesc:a808c41752e5a391ccccc5eaa820abc09"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input state value coefficient.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a808c41752e5a391ccccc5eaa820abc09">More...</a><br /></td></tr>
<tr class="separator:a808c41752e5a391ccccc5eaa820abc09"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf2d6eee242ba098e6b977b9b39e5cda"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aaf2d6eee242ba098e6b977b9b39e5cda">states_current_values</a></td></tr>
<tr class="memdesc:aaf2d6eee242ba098e6b977b9b39e5cda"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of state values at each timestep.This is cleared after each episode.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aaf2d6eee242ba098e6b977b9b39e5cda">More...</a><br /></td></tr>
<tr class="separator:aaf2d6eee242ba098e6b977b9b39e5cda"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa40ba92a0c825186f61c04e9ac713a55"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aa40ba92a0c825186f61c04e9ac713a55">step_counter</a></td></tr>
<tr class="memdesc:aa40ba92a0c825186f61c04e9ac713a55"><td class="mdescLeft">&#160;</td><td class="mdescRight">The step counter; counting the total timesteps done so far.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aa40ba92a0c825186f61c04e9ac713a55">More...</a><br /></td></tr>
<tr class="separator:aa40ba92a0c825186f61c04e9ac713a55"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Data Fields inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:aa4b5b7a651697524896373ca24d0ba16 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa4b5b7a651697524896373ca24d0ba16">advantage_norm_codes</a></td></tr>
<tr class="memdesc:aa4b5b7a651697524896373ca24d0ba16 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The Advantage normalisation codes; indicating the codes to normalise Advantages.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa4b5b7a651697524896373ca24d0ba16">More...</a><br /></td></tr>
<tr class="separator:aa4b5b7a651697524896373ca24d0ba16 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">loss</a></td></tr>
<tr class="memdesc:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of losses accumulated after each backward call.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">More...</a><br /></td></tr>
<tr class="separator:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4084a4f3b18536d1c0a871b151971bd7 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4084a4f3b18536d1c0a871b151971bd7">reward_norm_codes</a></td></tr>
<tr class="memdesc:a4084a4f3b18536d1c0a871b151971bd7 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The reward normalisation codes; indicating the codes to normalise rewards.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4084a4f3b18536d1c0a871b151971bd7">More...</a><br /></td></tr>
<tr class="separator:a4084a4f3b18536d1c0a871b151971bd7 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">save_path</a></td></tr>
<tr class="memdesc:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The path to save agent states and models.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">More...</a><br /></td></tr>
<tr class="separator:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae0500457a682bad272e8e28b9a475fcb inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ae0500457a682bad272e8e28b9a475fcb">state_norm_codes</a></td></tr>
<tr class="memdesc:ae0500457a682bad272e8e28b9a475fcb inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The state normalisation codes; indicating the codes to normalise states.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ae0500457a682bad272e8e28b9a475fcb">More...</a><br /></td></tr>
<tr class="separator:ae0500457a682bad272e8e28b9a475fcb inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa672333065b88d4734b58ad8bc1433eb inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa672333065b88d4734b58ad8bc1433eb">td_norm_codes</a></td></tr>
<tr class="memdesc:aa672333065b88d4734b58ad8bc1433eb inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The TD normalisation codes; indicating the codes to normalise TD Errors.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa672333065b88d4734b58ad8bc1433eb">More...</a><br /></td></tr>
<tr class="separator:aa672333065b88d4734b58ad8bc1433eb inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-methods" name="pri-methods"></a>
Private Member Functions</h2></td></tr>
<tr class="memitem:a0cf691d30076733d7a4584287f6bc2d5"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a0cf691d30076733d7a4584287f6bc2d5">_accumulate_gradients</a> (self)</td></tr>
<tr class="memdesc:a0cf691d30076733d7a4584287f6bc2d5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to train the model or accumulate the gradients for training.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a0cf691d30076733d7a4584287f6bc2d5">More...</a><br /></td></tr>
<tr class="separator:a0cf691d30076733d7a4584287f6bc2d5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a487c58b125fb7cd2e14972552bdf5ea6"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a487c58b125fb7cd2e14972552bdf5ea6">_call_to_save</a> (self)</td></tr>
<tr class="separator:a487c58b125fb7cd2e14972552bdf5ea6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2f5dd7eeab7ee5f523490c1ec65e03e8"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a2f5dd7eeab7ee5f523490c1ec65e03e8">_call_train_policy_model</a> (self, Union[bool, int] done)</td></tr>
<tr class="memdesc:a2f5dd7eeab7ee5f523490c1ec65e03e8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method to call the appropriate method for training policy model based on initialization of <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html" title="The A2C class implements the synchronous Actor-Critic method.">A2C</a> agent.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a2f5dd7eeab7ee5f523490c1ec65e03e8">More...</a><br /></td></tr>
<tr class="separator:a2f5dd7eeab7ee5f523490c1ec65e03e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a77355981bba89741f7b8a15e76cf02f8"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a77355981bba89741f7b8a15e76cf02f8">_clear</a> (self)</td></tr>
<tr class="memdesc:a77355981bba89741f7b8a15e76cf02f8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to clear the lists of rewards, action_log_probs and state_values.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a77355981bba89741f7b8a15e76cf02f8">More...</a><br /></td></tr>
<tr class="separator:a77355981bba89741f7b8a15e76cf02f8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeb6b2e6aa1311b4c7850ef224a27f373"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeb6b2e6aa1311b4c7850ef224a27f373">_compute_advantage</a> (self, pytorch.Tensor returns, pytorch.Tensor state_current_values)</td></tr>
<tr class="memdesc:aeb6b2e6aa1311b4c7850ef224a27f373"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the advantage from returns and state values.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#aeb6b2e6aa1311b4c7850ef224a27f373">More...</a><br /></td></tr>
<tr class="separator:aeb6b2e6aa1311b4c7850ef224a27f373"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae6f84e2eed2d807cda9fa3f9334d81a8"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ae6f84e2eed2d807cda9fa3f9334d81a8">_compute_returns</a> (self)</td></tr>
<tr class="memdesc:ae6f84e2eed2d807cda9fa3f9334d81a8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the discounted returns iteratively.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#ae6f84e2eed2d807cda9fa3f9334d81a8">More...</a><br /></td></tr>
<tr class="separator:ae6f84e2eed2d807cda9fa3f9334d81a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a02e856e459348c7ddd286b6a9d559d9e"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a02e856e459348c7ddd286b6a9d559d9e">_train_models</a> (self)</td></tr>
<tr class="memdesc:a02e856e459348c7ddd286b6a9d559d9e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method to policy model if boostrap_rounds &gt; 1.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a02e856e459348c7ddd286b6a9d559d9e">More...</a><br /></td></tr>
<tr class="separator:a02e856e459348c7ddd286b6a9d559d9e"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-static-methods" name="pri-static-methods"></a>
Static Private Member Functions</h2></td></tr>
<tr class="memitem:a0eb5d7ae878ab995b910f89bf60620ea"><td class="memItemLeft" align="right" valign="top">Categorical&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a0eb5d7ae878ab995b910f89bf60620ea">_create_action_distribution</a> (pytorch.Tensor actions_logits)</td></tr>
<tr class="memdesc:a0eb5d7ae878ab995b910f89bf60620ea"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected static method to create distributions from action logits.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a0eb5d7ae878ab995b910f89bf60620ea">More...</a><br /></td></tr>
<tr class="separator:a0eb5d7ae878ab995b910f89bf60620ea"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-attribs" name="pri-attribs"></a>
Private Attributes</h2></td></tr>
<tr class="memitem:af21943ec874f47fba4ea44ade7f02a5f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af21943ec874f47fba4ea44ade7f02a5f">_grad_accumulator</a></td></tr>
<tr class="memdesc:af21943ec874f47fba4ea44ade7f02a5f"><td class="mdescLeft">&#160;</td><td class="mdescRight">The policy model parameters names.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af21943ec874f47fba4ea44ade7f02a5f">More...</a><br /></td></tr>
<tr class="separator:af21943ec874f47fba4ea44ade7f02a5f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a297c096e4155c3ac4815f7ce95ca6767"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a297c096e4155c3ac4815f7ce95ca6767">_normalization</a></td></tr>
<tr class="memdesc:a297c096e4155c3ac4815f7ce95ca6767"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of gradients from each backward call.  <a href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a297c096e4155c3ac4815f7ce95ca6767">More...</a><br /></td></tr>
<tr class="separator:a297c096e4155c3ac4815f7ce95ca6767"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >The <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html" title="The A2C class implements the synchronous Actor-Critic method.">A2C</a> class implements the synchronous Actor-Critic method. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a85826dab437582b48e1d1919e78e325f" name="a85826dab437582b48e1d1919e78e325f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a85826dab437582b48e1d1919e78e325f">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.actor_critic.a2c.A2C.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.nn.Module&#160;</td>
          <td class="paramname"><em>policy_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.optim.Optimizer&#160;</td>
          <td class="paramname"><em>optimizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[LRScheduler, None]&#160;</td>
          <td class="paramname"><em>lr_scheduler</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">LossFunction&#160;</td>
          <td class="paramname"><em>loss_function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>entropy_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>state_value_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>lr_threshold</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>backup_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>save_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>bootstrap_rounds</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>device</em> = <code>&quot;cpu&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>apply_norm</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>apply_norm_to</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>eps_for_norm</em> = <code>5e-12</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>p_for_norm</em> = <code>2</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>dim_for_norm</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>max_grad_norm</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>grad_norm_p</em> = <code>2.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">policy_model</td><td><em>pytorch.nn.Module</em>: The policy model to be used. Policy model must return a tuple of action logits and state values. </td></tr>
    <tr><td class="paramname">optimizer</td><td>pytorch.optim.Optimizer: The optimizer to be used for policy model. Optimizer must be initialized and wrapped with policy model parameters. </td></tr>
    <tr><td class="paramname">lr_scheduler</td><td>Union[LRScheduler, None]: The LR Scheduler to be used to decay the learning rate. LR Scheduler must be initialized and wrapped with passed optimizer. </td></tr>
    <tr><td class="paramname">loss_function</td><td>LossFunction: A PyTorch loss function. </td></tr>
    <tr><td class="paramname">gamma</td><td>float: The discounting factor for rewards. </td></tr>
    <tr><td class="paramname">entropy_coefficient</td><td>float: The coefficient to be used for entropy in policy loss computation. </td></tr>
    <tr><td class="paramname">state_value_coefficient</td><td>float: The coefficient to be used for state value in final loss computation. </td></tr>
    <tr><td class="paramname">lr_threshold</td><td>float: The threshold LR which once reached LR scheduler is not called further. </td></tr>
    <tr><td class="paramname">num_actions</td><td>int: Number of actions for the environment. </td></tr>
    <tr><td class="paramname">backup_frequency</td><td>int: The timesteps after which policy model, optimizer states and lr scheduler states are backed up. </td></tr>
    <tr><td class="paramname">save_path</td><td>str: The path where policy model, optimizer states and lr scheduler states are to be saved. </td></tr>
    <tr><td class="paramname">bootstrap_rounds</td><td>int: The number of rounds until which gradients are to be accumulated before performing calling optimizer step. Gradients are mean reduced for bootstrap_rounds &gt; 1. Default: 1. </td></tr>
    <tr><td class="paramname">device</td><td>str: The device on which models are run. Default: "cpu". </td></tr>
    <tr><td class="paramname">apply_norm</td><td>Union[int, str]: The code to select the normalization procedure to be applied on selected quantities; selected by <code>apply_norm_to</code>: see below)). Direct string can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1 </td></tr>
    <tr><td class="paramname">apply_norm_to</td><td>Union[int, List[str]]: The code to select the quantity to which normalization is to be applied. Direct list of quantities can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1. </td></tr>
    <tr><td class="paramname">eps_for_norm</td><td>float: Epsilon value for normalization; for numeric stability. For min-max normalization and standardized normalization. Default: 5e-12. </td></tr>
    <tr><td class="paramname">p_for_norm</td><td>int: The p value for p-normalization. Default: 2; L2 Norm. </td></tr>
    <tr><td class="paramname">dim_for_norm</td><td>int: The dimension across which normalization is to be performed. Default: 0. </td></tr>
    <tr><td class="paramname">max_grad_norm</td><td>Optional[float]: The max norm for gradients for gradient clipping. Default: None </td></tr>
    <tr><td class="paramname">grad_norm_p</td><td>Optional[float]: The p-value for p-normalization of gradients. Default: 2.0</td></tr>
  </table>
  </dd>
</dl>
<p><b>Notes</b></p>
<p >The codes for <code>apply_norm</code> are given as follows: -</p><ul>
<li>No Normalization: -1; (<code>"none"</code>)</li>
<li>Min-Max Normalization: 0; (<code>"min_max"</code>)</li>
<li>Standardization: 1; (<code>"standardize"</code>)</li>
<li>P-Normalization: 2; (<code>"p_norm"</code>)</li>
</ul>
<p >The codes for <code>apply_norm_to</code> are given as follows:</p><ul>
<li>No Normalization: -1; (<code>["none"]</code>)</li>
<li>On States only: 0; (<code>["states"]</code>)</li>
<li>On Rewards only: 1; (<code>["rewards"]</code>)</li>
<li>On TD value only: 2; (<code>["td"]</code>)</li>
<li>On States and Rewards: 3; (<code>["states", "rewards"]</code>)</li>
<li>On States and TD: 4; (<code>["states", "td"]</code>)</li>
</ul>
<p >If a valid <code>max_norm_grad</code> is passed, then gradient clipping takes place else gradient clipping step is skipped. If <code>max_norm_grad</code> value was invalid, error will be raised from PyTorch. </p>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">rlpack.utils.base.agent.Agent</a>.</p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#aaf18b9e96dfef0377b41a5852c990bd0">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a0cf691d30076733d7a4584287f6bc2d5" name="a0cf691d30076733d7a4584287f6bc2d5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0cf691d30076733d7a4584287f6bc2d5">&#9670;&#160;</a></span>_accumulate_gradients()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._accumulate_gradients </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to train the model or accumulate the gradients for training. </p>
<ul>
<li>If bootstrap_rounds is passed as 1 (default), model is trained each time the method is called.</li>
<li>If bootstrap_rounds &gt; 1, the gradients are accumulated in grad_accumulator and model is trained via _train_models method. </li>
</ul>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#aee080ba6ab8d2ec99714e1b1ae43366b">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<a id="a487c58b125fb7cd2e14972552bdf5ea6" name="a487c58b125fb7cd2e14972552bdf5ea6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a487c58b125fb7cd2e14972552bdf5ea6">&#9670;&#160;</a></span>_call_to_save()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._call_to_save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#ac2f6faa17ef5e68d665f2e085636b5d2">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<a id="a2f5dd7eeab7ee5f523490c1ec65e03e8" name="a2f5dd7eeab7ee5f523490c1ec65e03e8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2f5dd7eeab7ee5f523490c1ec65e03e8">&#9670;&#160;</a></span>_call_train_policy_model()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._call_train_policy_model </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method to call the appropriate method for training policy model based on initialization of <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html" title="The A2C class implements the synchronous Actor-Critic method.">A2C</a> agent. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">done</td><td>Union[bool, int]: Flag indicating if episode has terminated or not </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a77355981bba89741f7b8a15e76cf02f8" name="a77355981bba89741f7b8a15e76cf02f8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a77355981bba89741f7b8a15e76cf02f8">&#9670;&#160;</a></span>_clear()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._clear </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to clear the lists of rewards, action_log_probs and state_values. </p>

</div>
</div>
<a id="aeb6b2e6aa1311b4c7850ef224a27f373" name="aeb6b2e6aa1311b4c7850ef224a27f373"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeb6b2e6aa1311b4c7850ef224a27f373">&#9670;&#160;</a></span>_compute_advantage()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.a2c.A2C._compute_advantage </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>returns</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor
    &#160;</td>
          <td class="paramname"><em>state_current_values</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the advantage from returns and state values. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">returns</td><td>pytorch.Tensor: The discounted returns; computed from _compute_returns method </td></tr>
    <tr><td class="paramname">state_current_values</td><td>pytorch.Tensor: The corresponding state values </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The advantage for the given returns and state values </dd></dl>

</div>
</div>
<a id="ae6f84e2eed2d807cda9fa3f9334d81a8" name="ae6f84e2eed2d807cda9fa3f9334d81a8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6f84e2eed2d807cda9fa3f9334d81a8">&#9670;&#160;</a></span>_compute_returns()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.a2c.A2C._compute_returns </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the discounted returns iteratively. </p>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The discounted returns </dd></dl>

</div>
</div>
<a id="a0eb5d7ae878ab995b910f89bf60620ea" name="a0eb5d7ae878ab995b910f89bf60620ea"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0eb5d7ae878ab995b910f89bf60620ea">&#9670;&#160;</a></span>_create_action_distribution()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> Categorical rlpack.actor_critic.a2c.A2C._create_action_distribution </td>
          <td>(</td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>actions_logits</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected static method to create distributions from action logits. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">actions_logits</td><td>pytorch.Tensor: The action logits from policy model </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Categorical: A Categorical object initialized with given action logits </dd></dl>

</div>
</div>
<a id="a02e856e459348c7ddd286b6a9d559d9e" name="a02e856e459348c7ddd286b6a9d559d9e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a02e856e459348c7ddd286b6a9d559d9e">&#9670;&#160;</a></span>_train_models()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C._train_models </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method to policy model if boostrap_rounds &gt; 1. </p>
<p >In such cases the gradients are accumulated in grad_accumulator. This method collects the accumulated gradients and performs mean reduction and runs optimizer step. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#abaf1718c366649b1ba9567062d292201">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<a id="af0b97082eda91653d7a2511c3325b70f" name="af0b97082eda91653d7a2511c3325b70f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af0b97082eda91653d7a2511c3325b70f">&#9670;&#160;</a></span>load()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C.load </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to load the best model by a custom suffix supplied for evaluation. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="a5e480b8b9914e283440e5278a97b42cf" name="a5e480b8b9914e283440e5278a97b42cf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5e480b8b9914e283440e5278a97b42cf">&#9670;&#160;</a></span>policy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> int rlpack.actor_critic.a2c.A2C.policy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The policy method to evaluate the agent. </p>
<p >This runs in pure inference mode. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned from gym environment </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>int: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="a7793678505da4a71178d7a1043c4023d" name="a7793678505da4a71178d7a1043c4023d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7793678505da4a71178d7a1043c4023d">&#9670;&#160;</a></span>save()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.a2c.A2C.save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<p >agent_states includes current memory and epsilon values in a dictionary. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to save best model by a custom suffix supplied during a train run. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="ae2c4c474849c70cdedf6b98aea2f63c4" name="ae2c4c474849c70cdedf6b98aea2f63c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae2c4c474849c70cdedf6b98aea2f63c4">&#9670;&#160;</a></span>train()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> int rlpack.actor_critic.a2c.A2C.train </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, float]&#160;</td>
          <td class="paramname"><em>reward</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The train method to train the agent and underlying policy model. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned </td></tr>
    <tr><td class="paramname">reward</td><td>Union[int, float]: The reward returned from previous action </td></tr>
    <tr><td class="paramname">done</td><td>Union[bool, int]: Flag indicating if episode has terminated or not </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>int: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<h2 class="groupheader">Field Documentation</h2>
<a id="af21943ec874f47fba4ea44ade7f02a5f" name="af21943ec874f47fba4ea44ade7f02a5f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af21943ec874f47fba4ea44ade7f02a5f">&#9670;&#160;</a></span>_grad_accumulator</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C._grad_accumulator</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The policy model parameters names. </p>
<p >self._policy_model_parameter_keys = OrderedDict( self.policy_model.named_parameters() ).keys() </p>

</div>
</div>
<a id="a297c096e4155c3ac4815f7ce95ca6767" name="a297c096e4155c3ac4815f7ce95ca6767"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a297c096e4155c3ac4815f7ce95ca6767">&#9670;&#160;</a></span>_normalization</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C._normalization</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The list of gradients from each backward call. </p>
<p >This is only used when boostrap_rounds &gt; 1 and is cleared after each boostrap round. self._grad_accumulator = list() The normalisation tool to be used for agent. An instance of <a class="el" href="classrlpack_1_1utils_1_1normalization_1_1_normalization.html" title="Normalization class providing methods for normalization techniques.">rlpack.utils.normalization.Normalization</a>. </p>

</div>
</div>
<a id="a989698a1260432ca0fcbd1668fe9c7ac" name="a989698a1260432ca0fcbd1668fe9c7ac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a989698a1260432ca0fcbd1668fe9c7ac">&#9670;&#160;</a></span>action_log_probabilities</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.action_log_probabilities</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The list of sampled actions from each timestep from the action distribution. </p>
<p >This is cleared after each episode. </p>

</div>
</div>
<a id="a57416c3545397ddaaa31ee69142e67da" name="a57416c3545397ddaaa31ee69142e67da"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a57416c3545397ddaaa31ee69142e67da">&#9670;&#160;</a></span>apply_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.apply_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm</code> argument; indicating the normalisation to be used. </p>

</div>
</div>
<a id="aeef17e7ec0e0b81e2b47b0c4663bad2c" name="aeef17e7ec0e0b81e2b47b0c4663bad2c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeef17e7ec0e0b81e2b47b0c4663bad2c">&#9670;&#160;</a></span>apply_norm_to</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.apply_norm_to</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm_to</code> argument; indicating the quantity to normalise. </p>

</div>
</div>
<a id="a26e70f7a784551fa58fa81b1f2dcc894" name="a26e70f7a784551fa58fa81b1f2dcc894"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a26e70f7a784551fa58fa81b1f2dcc894">&#9670;&#160;</a></span>backup_frequency</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.backup_frequency</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input model backup frequency in terms of timesteps. </p>

</div>
</div>
<a id="a67a8def97a97cef04686199250556bd0" name="a67a8def97a97cef04686199250556bd0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a67a8def97a97cef04686199250556bd0">&#9670;&#160;</a></span>bootstrap_rounds</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.bootstrap_rounds</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input boostrap rounds. </p>

</div>
</div>
<a id="a3f3c23d256a14d2b124fb866732051b6" name="a3f3c23d256a14d2b124fb866732051b6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3f3c23d256a14d2b124fb866732051b6">&#9670;&#160;</a></span>device</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.device</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>device</code> argument; indicating the device name. </p>

</div>
</div>
<a id="ac3f70110b14a2bc1354fadaa02f3bdff" name="ac3f70110b14a2bc1354fadaa02f3bdff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac3f70110b14a2bc1354fadaa02f3bdff">&#9670;&#160;</a></span>dim_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.dim_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise. </p>

</div>
</div>
<a id="ac030e4955c3b5317db360d8539e51163" name="ac030e4955c3b5317db360d8539e51163"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac030e4955c3b5317db360d8539e51163">&#9670;&#160;</a></span>entropies</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.entropies</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The list of entropies from each timestep. </p>
<p >This is cleared after each episode. </p>

</div>
</div>
<a id="a6def13b25087f4b35685140b0a8e31ff" name="a6def13b25087f4b35685140b0a8e31ff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6def13b25087f4b35685140b0a8e31ff">&#9670;&#160;</a></span>entropy_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.entropy_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input entropy coefficient. </p>

</div>
</div>
<a id="ad806193a87f792ffdc0e349f0fa5f3e2" name="ad806193a87f792ffdc0e349f0fa5f3e2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad806193a87f792ffdc0e349f0fa5f3e2">&#9670;&#160;</a></span>episode_counter</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.episode_counter</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The episode counter; counting the total episodes done so far. </p>

</div>
</div>
<a id="ab79962d551f8a638f440166462456111" name="ab79962d551f8a638f440166462456111"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab79962d551f8a638f440166462456111">&#9670;&#160;</a></span>eps_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.eps_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation. </p>

</div>
</div>
<a id="a335067fc595b3c6bf62395024fd0008a" name="a335067fc595b3c6bf62395024fd0008a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a335067fc595b3c6bf62395024fd0008a">&#9670;&#160;</a></span>gamma</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.gamma</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input discounting factor. </p>

</div>
</div>
<a id="a3f1d962e32fb21e733c9161a3e5ec206" name="a3f1d962e32fb21e733c9161a3e5ec206"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3f1d962e32fb21e733c9161a3e5ec206">&#9670;&#160;</a></span>grad_norm_p</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.grad_norm_p</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings. </p>

</div>
</div>
<a id="ace16f409552ad6b3d8274b91be4b5157" name="ace16f409552ad6b3d8274b91be4b5157"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ace16f409552ad6b3d8274b91be4b5157">&#9670;&#160;</a></span>loss_function</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.loss_function</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input loss function. </p>

</div>
</div>
<a id="a67fd595af5ce5ef1a935083e8383f6af" name="a67fd595af5ce5ef1a935083e8383f6af"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a67fd595af5ce5ef1a935083e8383f6af">&#9670;&#160;</a></span>lr_scheduler</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.lr_scheduler</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optional LR Scheduler (this can be None). </p>

</div>
</div>
<a id="abca9e9f63b00391c576378683d000d8a" name="abca9e9f63b00391c576378683d000d8a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abca9e9f63b00391c576378683d000d8a">&#9670;&#160;</a></span>lr_threshold</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.lr_threshold</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input LR Threshold. </p>

</div>
</div>
<a id="af326d53cbea6b99ed2792a0b88f73539" name="af326d53cbea6b99ed2792a0b88f73539"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af326d53cbea6b99ed2792a0b88f73539">&#9670;&#160;</a></span>max_grad_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.max_grad_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings. </p>

</div>
</div>
<a id="a6b420f1579a75aae43c82105a98c311e" name="a6b420f1579a75aae43c82105a98c311e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6b420f1579a75aae43c82105a98c311e">&#9670;&#160;</a></span>num_actions</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.num_actions</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input number of actions. </p>

</div>
</div>
<a id="a531fa341471a4ad1f97d36c00f221981" name="a531fa341471a4ad1f97d36c00f221981"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a531fa341471a4ad1f97d36c00f221981">&#9670;&#160;</a></span>optimizer</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.optimizer</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optimizer wrapped with policy_model parameters. </p>

</div>
</div>
<a id="af4f2a8721367dc7865955f308cc6149b" name="af4f2a8721367dc7865955f308cc6149b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af4f2a8721367dc7865955f308cc6149b">&#9670;&#160;</a></span>p_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.p_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation. </p>

</div>
</div>
<a id="a186663b4b9a13fdff2d3a6c068573a4b" name="a186663b4b9a13fdff2d3a6c068573a4b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a186663b4b9a13fdff2d3a6c068573a4b">&#9670;&#160;</a></span>policy_model</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.policy_model</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input policy model moved to desired device. </p>

</div>
</div>
<a id="a1cc2aee9a3fdd63cd263a8600f19eaae" name="a1cc2aee9a3fdd63cd263a8600f19eaae"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1cc2aee9a3fdd63cd263a8600f19eaae">&#9670;&#160;</a></span>rewards</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.rewards</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The list of rewards from each timestep. </p>
<p >This is cleared after each episode. </p>

</div>
</div>
<a id="affac024bad4b94c555d8299db2e7a64c" name="affac024bad4b94c555d8299db2e7a64c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#affac024bad4b94c555d8299db2e7a64c">&#9670;&#160;</a></span>save_path</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.save_path</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input save path for backing up agent models. </p>

</div>
</div>
<a id="a808c41752e5a391ccccc5eaa820abc09" name="a808c41752e5a391ccccc5eaa820abc09"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a808c41752e5a391ccccc5eaa820abc09">&#9670;&#160;</a></span>state_value_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.state_value_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input state value coefficient. </p>

</div>
</div>
<a id="aaf2d6eee242ba098e6b977b9b39e5cda" name="aaf2d6eee242ba098e6b977b9b39e5cda"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aaf2d6eee242ba098e6b977b9b39e5cda">&#9670;&#160;</a></span>states_current_values</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.states_current_values</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The list of state values at each timestep.This is cleared after each episode. </p>
<p ><br  />
 </p>

</div>
</div>
<a id="aa40ba92a0c825186f61c04e9ac713a55" name="aa40ba92a0c825186f61c04e9ac713a55"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa40ba92a0c825186f61c04e9ac713a55">&#9670;&#160;</a></span>step_counter</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.a2c.A2C.step_counter</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The step counter; counting the total timesteps done so far. </p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacerlpack.html">rlpack</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic.html">actor_critic</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic_1_1a2c.html">a2c</a></li><li class="navelem"><a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html">A2C</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
