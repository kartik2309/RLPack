<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RLPack: rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="RLPack-logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">RLPack
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Data Fields</a> &#124;
<a href="#pri-methods">Private Member Functions</a> &#124;
<a href="#pri-static-methods">Static Private Member Functions</a> &#124;
<a href="#pri-attribs">Private Attributes</a>  </div>
  <div class="headertitle"><div class="title">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>The <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html" title="The ActorCriticAgent is the base class for actor-critic methods.">ActorCriticAgent</a> is the base class for actor-critic methods.  
 <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#details">More...</a></p>
<div id="dynsection-0" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-0-trigger" src="closed.png" alt="+"/> Inheritance diagram for rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent:</div>
<div id="dynsection-0-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-0-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent__inherit__graph.png" border="0" usemap="#arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_inherit__map" alt="Inheritance graph"/></div>
<map name="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_inherit__map" id="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_inherit__map">
<area shape="rect" title="The ActorCriticAgent is the base class for actor&#45;critic methods." alt="" coords="201,167,397,207"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html" title="The A2C class implements the asynchronous Actor&#45;Critic method with synchronization." alt="" coords="5,255,185,280"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html" title="The A3C class implements the asynchronous Actor&#45;Critic method." alt="" coords="209,255,389,280"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html" title="The ActorCritic class implements the basic Actor&#45;Critic method with single agent." alt="" coords="413,255,580,280"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="221,79,378,119"/>
<area shape="rect" title=" " alt="" coords="274,5,325,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<div id="dynsection-1" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-1-trigger" src="closed.png" alt="+"/> Collaboration diagram for rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent:</div>
<div id="dynsection-1-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-1-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent__coll__graph.png" border="0" usemap="#arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_coll__map" alt="Collaboration graph"/></div>
<map name="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_coll__map" id="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_coll__map">
<area shape="rect" title="The ActorCriticAgent is the base class for actor&#45;critic methods." alt="" coords="5,167,201,207"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="25,79,182,119"/>
<area shape="rect" title=" " alt="" coords="78,5,129,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a0f56b79ec5543b3708e82ba2154ec206"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0f56b79ec5543b3708e82ba2154ec206">__init__</a> (self, <a class="el" href="classrlpack_1_1utils_1_1base_1_1model_1_1_model.html">Model</a> <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3c0e7b3a8cc0a96cebe026d304c5590c">policy_model</a>, pytorch.optim.Optimizer <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2f798b74bd64640ef35da8f672f3e95">optimizer</a>, Union[<a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_l_r_scheduler.html">LRScheduler</a>, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a081dcbd51017df1d26abbf15821089d7">lr_scheduler</a>, <a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_loss_function.html">LossFunction</a> <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af5485148744d68c770b872e62361ca10">loss_function</a>, Type[pytorch_distributions.Distribution] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">distribution</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab09c920ad70ce8b4cd3fa485b2f1bd55">gamma</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#affc2a52ddeb68b4c0a6c05793fa58261">entropy_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a073e3f0455059d575ce1bc9a28b3eee5">state_value_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2238587b60c04bf2a7e83e497a030b1e">lr_threshold</a>, Union[int, Tuple[int, Union[List[int], None]]] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a31e3fa147ea1d19490fb388ffe61aee3">action_space</a>, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af19d87d26f908a6938f3b3ec1d25c70c">backup_frequency</a>, str <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a00c1fb74f19174eecd6fcd03dc0b9843">save_path</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#afee0ff0f5722297192f97483defd926b">gae_lambda</a>=1.0, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7b67cf1d9ca70e89e39fa80c4be140ba">batch_size</a>=1, Union[int, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2e03df778b8d4d48cc85892818d2a6e6">exploration_steps</a>=None, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a724bcb1e4d32cd2cd880911eed207af6">grad_accumulation_rounds</a>=1, Union[int, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#abb8397a2c8306113ec191e551fab2f64">training_frequency</a>=None, Union[<a class="el" href="classrlpack_1_1exploration_1_1utils_1_1exploration_1_1_exploration.html">Exploration</a>, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac76827b34c015165373ec76358b60dea">exploration_tool</a>=None, str <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a05e8b2d000bd94f5fb854516c84200e5">device</a>=&quot;cpu&quot;, str <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a6dc53028e64988f4a9442a047736acb8">dtype</a>=&quot;float32&quot;, Union[<a class="el" href="classrlpack_1_1utils_1_1normalization_1_1_normalization.html">Normalization</a>, None] normalization_tool=None, Union[int, List[str]] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7e6b39931e27a1d569945fdb177d40cc">apply_norm_to</a>=-1, Optional[float] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae98b91d40567e6e9990304977809d997">max_grad_norm</a>=None, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2bfe032740a7146997162d125274fe9">grad_norm_p</a>=2.0, Optional[float] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa7a4a5ac2156c6877cbc0533601724e7">clip_grad_value</a>=None, timedelta <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a341810c1e41d795b3c82dc375e606d08">timeout</a>=timedelta(minutes=30))</td></tr>
<tr class="separator:a0f56b79ec5543b3708e82ba2154ec206"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a369d886df0d6762cb825f10682607581"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a369d886df0d6762cb825f10682607581">load</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:a369d886df0d6762cb825f10682607581"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a369d886df0d6762cb825f10682607581">More...</a><br /></td></tr>
<tr class="separator:a369d886df0d6762cb825f10682607581"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae8e3441052d7b47fa91188a684e18659"><td class="memItemLeft" align="right" valign="top">np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae8e3441052d7b47fa91188a684e18659">policy</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, **kwargs)</td></tr>
<tr class="memdesc:ae8e3441052d7b47fa91188a684e18659"><td class="mdescLeft">&#160;</td><td class="mdescRight">The policy method to evaluate the agent.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae8e3441052d7b47fa91188a684e18659">More...</a><br /></td></tr>
<tr class="separator:ae8e3441052d7b47fa91188a684e18659"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad0e78685793f897be63b701cb75b6fc5"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ad0e78685793f897be63b701cb75b6fc5">save</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:ad0e78685793f897be63b701cb75b6fc5"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ad0e78685793f897be63b701cb75b6fc5">More...</a><br /></td></tr>
<tr class="separator:ad0e78685793f897be63b701cb75b6fc5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a378c9f9d11f38263c43bda87ca5d01ac"><td class="memItemLeft" align="right" valign="top">np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a378c9f9d11f38263c43bda87ca5d01ac">train</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_next, Union[int, float] reward, Union[bool, int] done, **kwargs)</td></tr>
<tr class="memdesc:a378c9f9d11f38263c43bda87ca5d01ac"><td class="mdescLeft">&#160;</td><td class="mdescRight">The train method to train the agent and underlying policy model.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a378c9f9d11f38263c43bda87ca5d01ac">More...</a><br /></td></tr>
<tr class="separator:a378c9f9d11f38263c43bda87ca5d01ac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Dict[str, Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">__getstate__</a> (self)</td></tr>
<tr class="memdesc:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To get the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">More...</a><br /></td></tr>
<tr class="separator:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">__init__</a> (self)</td></tr>
<tr class="memdesc:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The class initializer.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">More...</a><br /></td></tr>
<tr class="separator:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">__setstate__</a> (self, Dict[str, Any] state)</td></tr>
<tr class="memdesc:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To load the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">More...</a><br /></td></tr>
<tr class="separator:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">load</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">More...</a><br /></td></tr>
<tr class="separator:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">policy</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Policy method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">More...</a><br /></td></tr>
<tr class="separator:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">save</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">More...</a><br /></td></tr>
<tr class="separator:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">train</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Training method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">More...</a><br /></td></tr>
<tr class="separator:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Data Fields</h2></td></tr>
<tr class="memitem:a31e3fa147ea1d19490fb388ffe61aee3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a31e3fa147ea1d19490fb388ffe61aee3">action_space</a></td></tr>
<tr class="memdesc:a31e3fa147ea1d19490fb388ffe61aee3"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input number of actions.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a31e3fa147ea1d19490fb388ffe61aee3">More...</a><br /></td></tr>
<tr class="separator:a31e3fa147ea1d19490fb388ffe61aee3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7e6b39931e27a1d569945fdb177d40cc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7e6b39931e27a1d569945fdb177d40cc">apply_norm_to</a></td></tr>
<tr class="memdesc:a7e6b39931e27a1d569945fdb177d40cc"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm_to</code> argument; indicating the quantity to normalise.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7e6b39931e27a1d569945fdb177d40cc">More...</a><br /></td></tr>
<tr class="separator:a7e6b39931e27a1d569945fdb177d40cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af19d87d26f908a6938f3b3ec1d25c70c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af19d87d26f908a6938f3b3ec1d25c70c">backup_frequency</a></td></tr>
<tr class="memdesc:af19d87d26f908a6938f3b3ec1d25c70c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input model backup frequency in terms of timesteps.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af19d87d26f908a6938f3b3ec1d25c70c">More...</a><br /></td></tr>
<tr class="separator:af19d87d26f908a6938f3b3ec1d25c70c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7b67cf1d9ca70e89e39fa80c4be140ba"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7b67cf1d9ca70e89e39fa80c4be140ba">batch_size</a></td></tr>
<tr class="memdesc:a7b67cf1d9ca70e89e39fa80c4be140ba"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input batch size (from argument <code>batch_size</code>).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7b67cf1d9ca70e89e39fa80c4be140ba">More...</a><br /></td></tr>
<tr class="separator:a7b67cf1d9ca70e89e39fa80c4be140ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa7a4a5ac2156c6877cbc0533601724e7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa7a4a5ac2156c6877cbc0533601724e7">clip_grad_value</a></td></tr>
<tr class="memdesc:aa7a4a5ac2156c6877cbc0533601724e7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>clip_grad_value</code>; indicating the clipping range for gradients.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa7a4a5ac2156c6877cbc0533601724e7">More...</a><br /></td></tr>
<tr class="separator:aa7a4a5ac2156c6877cbc0533601724e7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a05e8b2d000bd94f5fb854516c84200e5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a05e8b2d000bd94f5fb854516c84200e5">device</a></td></tr>
<tr class="memdesc:a05e8b2d000bd94f5fb854516c84200e5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>device</code> argument; indicating the device name as device type class.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a05e8b2d000bd94f5fb854516c84200e5">More...</a><br /></td></tr>
<tr class="separator:a05e8b2d000bd94f5fb854516c84200e5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9244a2ba98485e2bedf6cee830ca1a22"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">distribution</a></td></tr>
<tr class="memdesc:a9244a2ba98485e2bedf6cee830ca1a22"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input distribution object.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">More...</a><br /></td></tr>
<tr class="separator:a9244a2ba98485e2bedf6cee830ca1a22"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6dc53028e64988f4a9442a047736acb8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a6dc53028e64988f4a9442a047736acb8">dtype</a></td></tr>
<tr class="memdesc:a6dc53028e64988f4a9442a047736acb8"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>dtype</code> argument; indicating the datatype class.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a6dc53028e64988f4a9442a047736acb8">More...</a><br /></td></tr>
<tr class="separator:a6dc53028e64988f4a9442a047736acb8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:affc2a52ddeb68b4c0a6c05793fa58261"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#affc2a52ddeb68b4c0a6c05793fa58261">entropy_coefficient</a></td></tr>
<tr class="memdesc:affc2a52ddeb68b4c0a6c05793fa58261"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input entropy coefficient.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#affc2a52ddeb68b4c0a6c05793fa58261">More...</a><br /></td></tr>
<tr class="separator:affc2a52ddeb68b4c0a6c05793fa58261"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2e03df778b8d4d48cc85892818d2a6e6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2e03df778b8d4d48cc85892818d2a6e6">exploration_steps</a></td></tr>
<tr class="memdesc:a2e03df778b8d4d48cc85892818d2a6e6"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>rollout_accumulation_size</code>.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2e03df778b8d4d48cc85892818d2a6e6">More...</a><br /></td></tr>
<tr class="separator:a2e03df778b8d4d48cc85892818d2a6e6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac76827b34c015165373ec76358b60dea"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac76827b34c015165373ec76358b60dea">exploration_tool</a></td></tr>
<tr class="memdesc:ac76827b34c015165373ec76358b60dea"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>exploration_tool</code>.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac76827b34c015165373ec76358b60dea">More...</a><br /></td></tr>
<tr class="separator:ac76827b34c015165373ec76358b60dea"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afee0ff0f5722297192f97483defd926b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#afee0ff0f5722297192f97483defd926b">gae_lambda</a></td></tr>
<tr class="memdesc:afee0ff0f5722297192f97483defd926b"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input GAE Lambda (from argument <code>gae_lambda</code>).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#afee0ff0f5722297192f97483defd926b">More...</a><br /></td></tr>
<tr class="separator:afee0ff0f5722297192f97483defd926b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab09c920ad70ce8b4cd3fa485b2f1bd55"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab09c920ad70ce8b4cd3fa485b2f1bd55">gamma</a></td></tr>
<tr class="memdesc:ab09c920ad70ce8b4cd3fa485b2f1bd55"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input discounting factor.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab09c920ad70ce8b4cd3fa485b2f1bd55">More...</a><br /></td></tr>
<tr class="separator:ab09c920ad70ce8b4cd3fa485b2f1bd55"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a724bcb1e4d32cd2cd880911eed207af6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a724bcb1e4d32cd2cd880911eed207af6">grad_accumulation_rounds</a></td></tr>
<tr class="memdesc:a724bcb1e4d32cd2cd880911eed207af6"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>grad_accumulation_rounds</code>.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a724bcb1e4d32cd2cd880911eed207af6">More...</a><br /></td></tr>
<tr class="separator:a724bcb1e4d32cd2cd880911eed207af6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2bfe032740a7146997162d125274fe9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2bfe032740a7146997162d125274fe9">grad_norm_p</a></td></tr>
<tr class="memdesc:aa2bfe032740a7146997162d125274fe9"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2bfe032740a7146997162d125274fe9">More...</a><br /></td></tr>
<tr class="separator:aa2bfe032740a7146997162d125274fe9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4b605080b32f1199d314ded5efa4230a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4b605080b32f1199d314ded5efa4230a">is_continuous_action_space</a></td></tr>
<tr class="memdesc:a4b605080b32f1199d314ded5efa4230a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Flag indicating if action space is continuous or discrete.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4b605080b32f1199d314ded5efa4230a">More...</a><br /></td></tr>
<tr class="separator:a4b605080b32f1199d314ded5efa4230a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af5485148744d68c770b872e62361ca10"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af5485148744d68c770b872e62361ca10">loss_function</a></td></tr>
<tr class="memdesc:af5485148744d68c770b872e62361ca10"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input loss function.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af5485148744d68c770b872e62361ca10">More...</a><br /></td></tr>
<tr class="separator:af5485148744d68c770b872e62361ca10"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a081dcbd51017df1d26abbf15821089d7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a081dcbd51017df1d26abbf15821089d7">lr_scheduler</a></td></tr>
<tr class="memdesc:a081dcbd51017df1d26abbf15821089d7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optional LR Scheduler (this can be None).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a081dcbd51017df1d26abbf15821089d7">More...</a><br /></td></tr>
<tr class="separator:a081dcbd51017df1d26abbf15821089d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2238587b60c04bf2a7e83e497a030b1e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2238587b60c04bf2a7e83e497a030b1e">lr_threshold</a></td></tr>
<tr class="memdesc:a2238587b60c04bf2a7e83e497a030b1e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input LR Threshold.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2238587b60c04bf2a7e83e497a030b1e">More...</a><br /></td></tr>
<tr class="separator:a2238587b60c04bf2a7e83e497a030b1e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae98b91d40567e6e9990304977809d997"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae98b91d40567e6e9990304977809d997">max_grad_norm</a></td></tr>
<tr class="memdesc:ae98b91d40567e6e9990304977809d997"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae98b91d40567e6e9990304977809d997">More...</a><br /></td></tr>
<tr class="separator:ae98b91d40567e6e9990304977809d997"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2f798b74bd64640ef35da8f672f3e95"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2f798b74bd64640ef35da8f672f3e95">optimizer</a></td></tr>
<tr class="memdesc:aa2f798b74bd64640ef35da8f672f3e95"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optimizer wrapped with policy_model parameters.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2f798b74bd64640ef35da8f672f3e95">More...</a><br /></td></tr>
<tr class="separator:aa2f798b74bd64640ef35da8f672f3e95"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c0e7b3a8cc0a96cebe026d304c5590c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3c0e7b3a8cc0a96cebe026d304c5590c">policy_model</a></td></tr>
<tr class="memdesc:a3c0e7b3a8cc0a96cebe026d304c5590c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input policy model moved to desired device.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3c0e7b3a8cc0a96cebe026d304c5590c">More...</a><br /></td></tr>
<tr class="separator:a3c0e7b3a8cc0a96cebe026d304c5590c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a00c1fb74f19174eecd6fcd03dc0b9843"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a00c1fb74f19174eecd6fcd03dc0b9843">save_path</a></td></tr>
<tr class="memdesc:a00c1fb74f19174eecd6fcd03dc0b9843"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input save path for backing up agent models.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a00c1fb74f19174eecd6fcd03dc0b9843">More...</a><br /></td></tr>
<tr class="separator:a00c1fb74f19174eecd6fcd03dc0b9843"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a073e3f0455059d575ce1bc9a28b3eee5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a073e3f0455059d575ce1bc9a28b3eee5">state_value_coefficient</a></td></tr>
<tr class="memdesc:a073e3f0455059d575ce1bc9a28b3eee5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input state value coefficient.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a073e3f0455059d575ce1bc9a28b3eee5">More...</a><br /></td></tr>
<tr class="separator:a073e3f0455059d575ce1bc9a28b3eee5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a42a2685035dad0c95918126682a0d4ff"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a42a2685035dad0c95918126682a0d4ff">step_counter</a></td></tr>
<tr class="memdesc:a42a2685035dad0c95918126682a0d4ff"><td class="mdescLeft">&#160;</td><td class="mdescRight">The step counter; counting the total timesteps done so far.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a42a2685035dad0c95918126682a0d4ff">More...</a><br /></td></tr>
<tr class="separator:a42a2685035dad0c95918126682a0d4ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a341810c1e41d795b3c82dc375e606d08"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a341810c1e41d795b3c82dc375e606d08">timeout</a></td></tr>
<tr class="memdesc:a341810c1e41d795b3c82dc375e606d08"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>timeout</code>; indicating the timeout for synchronous calls.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a341810c1e41d795b3c82dc375e606d08">More...</a><br /></td></tr>
<tr class="separator:a341810c1e41d795b3c82dc375e606d08"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abb8397a2c8306113ec191e551fab2f64"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#abb8397a2c8306113ec191e551fab2f64">training_frequency</a></td></tr>
<tr class="memdesc:abb8397a2c8306113ec191e551fab2f64"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>training_frequency</code>.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#abb8397a2c8306113ec191e551fab2f64">More...</a><br /></td></tr>
<tr class="separator:abb8397a2c8306113ec191e551fab2f64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Data Fields inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4104e76aed3a37b4df4ef1069383aee8">gamma</a></td></tr>
<tr class="memdesc:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The default discounting factor for agents.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4104e76aed3a37b4df4ef1069383aee8">More...</a><br /></td></tr>
<tr class="separator:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">loss</a></td></tr>
<tr class="memdesc:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of losses accumulated after each backward call.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">More...</a><br /></td></tr>
<tr class="separator:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">save_path</a></td></tr>
<tr class="memdesc:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The path to save agent states and models.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">More...</a><br /></td></tr>
<tr class="separator:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-methods" name="pri-methods"></a>
Private Member Functions</h2></td></tr>
<tr class="memitem:a85d179455a7941657e1d4cbd94e3522d"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a85d179455a7941657e1d4cbd94e3522d">_add_noise_to_actions</a> (self, pytorch.Tensor action)</td></tr>
<tr class="memdesc:a85d179455a7941657e1d4cbd94e3522d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds noise to the given action according to the exploration tool being used.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a85d179455a7941657e1d4cbd94e3522d">More...</a><br /></td></tr>
<tr class="separator:a85d179455a7941657e1d4cbd94e3522d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a22282972de3a62a2aa7b526c3e869a97"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a22282972de3a62a2aa7b526c3e869a97">_backward_pass</a> (self)</td></tr>
<tr class="memdesc:a22282972de3a62a2aa7b526c3e869a97"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method to perform backward pass.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a22282972de3a62a2aa7b526c3e869a97">More...</a><br /></td></tr>
<tr class="separator:a22282972de3a62a2aa7b526c3e869a97"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a047cbca9519b71255a812248a48c2cf6"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a047cbca9519b71255a812248a48c2cf6">_call_to_extend_transitions</a> (self)</td></tr>
<tr class="memdesc:a047cbca9519b71255a812248a48c2cf6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method calling the method <a class="el" href="classrlpack_1_1___c_1_1rollout__buffer_1_1_rollout_buffer.html#a9d61053d9e8d7f6883e8a2e53c79303e" title="Method to extend the transitions.">RolloutBuffer.extend_transitions</a>.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a047cbca9519b71255a812248a48c2cf6">More...</a><br /></td></tr>
<tr class="separator:a047cbca9519b71255a812248a48c2cf6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a79da689b8d6edfd4ae0f3128ac130d57"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a79da689b8d6edfd4ae0f3128ac130d57">_call_to_save</a> (self)</td></tr>
<tr class="memdesc:a79da689b8d6edfd4ae0f3128ac130d57"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method calling the save method when required.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a79da689b8d6edfd4ae0f3128ac130d57">More...</a><br /></td></tr>
<tr class="separator:a79da689b8d6edfd4ae0f3128ac130d57"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9811685e91f1fa1137029243b7f78101"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9811685e91f1fa1137029243b7f78101">_clear_rollout_buffer</a> (self)</td></tr>
<tr class="memdesc:a9811685e91f1fa1137029243b7f78101"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to clear the rollout buffer (transitions and policy outputs)  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9811685e91f1fa1137029243b7f78101">More...</a><br /></td></tr>
<tr class="separator:a9811685e91f1fa1137029243b7f78101"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0dddd26f1bfa9e729fb771a28b2cd4bb"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0dddd26f1bfa9e729fb771a28b2cd4bb">_compute_loss</a> (self)</td></tr>
<tr class="memdesc:a0dddd26f1bfa9e729fb771a28b2cd4bb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method to compute total loss (from actor and critic).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0dddd26f1bfa9e729fb771a28b2cd4bb">More...</a><br /></td></tr>
<tr class="separator:a0dddd26f1bfa9e729fb771a28b2cd4bb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a97bd95a17383eabb3d8cb6bc89ff1a45"><td class="memItemLeft" align="right" valign="top">pytorch_distributions.Distribution&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a97bd95a17383eabb3d8cb6bc89ff1a45">_create_action_distribution</a> (self, Union[List[pytorch.Tensor], pytorch.Tensor] action_values)</td></tr>
<tr class="memdesc:a97bd95a17383eabb3d8cb6bc89ff1a45"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected static method to create distributions from action logits.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a97bd95a17383eabb3d8cb6bc89ff1a45">More...</a><br /></td></tr>
<tr class="separator:a97bd95a17383eabb3d8cb6bc89ff1a45"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adfa080bcb55a6011a736a143d6276cff"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#adfa080bcb55a6011a736a143d6276cff">_finish_policy_outputs_exploration</a> (self)</td></tr>
<tr class="memdesc:adfa080bcb55a6011a736a143d6276cff"><td class="mdescLeft">&#160;</td><td class="mdescRight">Finishes exploration in policy outputs and sets the necessary statistics if requested for normalization.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#adfa080bcb55a6011a736a143d6276cff">More...</a><br /></td></tr>
<tr class="separator:adfa080bcb55a6011a736a143d6276cff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af2b590bfae29e2ba3f386d3c85208027"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af2b590bfae29e2ba3f386d3c85208027">_finish_transitions_exploration</a> (self)</td></tr>
<tr class="memdesc:af2b590bfae29e2ba3f386d3c85208027"><td class="mdescLeft">&#160;</td><td class="mdescRight">Finishes exploration in transitions and sets the necessary statistics if requested for normalization.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af2b590bfae29e2ba3f386d3c85208027">More...</a><br /></td></tr>
<tr class="separator:af2b590bfae29e2ba3f386d3c85208027"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a19374ccf78639131b609c67e747561bc"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a19374ccf78639131b609c67e747561bc">_forward_pass</a> (self)</td></tr>
<tr class="memdesc:a19374ccf78639131b609c67e747561bc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method to perform forward pass.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a19374ccf78639131b609c67e747561bc">More...</a><br /></td></tr>
<tr class="separator:a19374ccf78639131b609c67e747561bc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3b095ff9e0f3527fa9e27d9aba3eed6a"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3b095ff9e0f3527fa9e27d9aba3eed6a">_get_action_from_distribution</a> (self, pytorch_distributions.Distribution <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">distribution</a>, bool reparametrize=False)</td></tr>
<tr class="memdesc:a3b095ff9e0f3527fa9e27d9aba3eed6a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates action from the given distribution by sampling.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3b095ff9e0f3527fa9e27d9aba3eed6a">More...</a><br /></td></tr>
<tr class="separator:a3b095ff9e0f3527fa9e27d9aba3eed6a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac8f1674df6616eb003161c69ae61c7d7"><td class="memItemLeft" align="right" valign="top">pytorch.Size&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac8f1674df6616eb003161c69ae61c7d7">_get_action_sample_shape</a> (self)</td></tr>
<tr class="memdesc:ac8f1674df6616eb003161c69ae61c7d7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the action sample shape to be sampled from distribution.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac8f1674df6616eb003161c69ae61c7d7">More...</a><br /></td></tr>
<tr class="separator:ac8f1674df6616eb003161c69ae61c7d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afef85f43b81e3743e774ca8b47265b72"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#afef85f43b81e3743e774ca8b47265b72">_grad_mean_reduction</a> (self)</td></tr>
<tr class="memdesc:afef85f43b81e3743e774ca8b47265b72"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs mean reduction for accumulated gradients and assigns the policy model's parameter the mean reduced gradients.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#afef85f43b81e3743e774ca8b47265b72">More...</a><br /></td></tr>
<tr class="separator:afef85f43b81e3743e774ca8b47265b72"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5778fa370052e32f2846ea596e1d3d80"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a5778fa370052e32f2846ea596e1d3d80">_reset_exploration_tool</a> (self)</td></tr>
<tr class="memdesc:a5778fa370052e32f2846ea596e1d3d80"><td class="mdescLeft">&#160;</td><td class="mdescRight">Resets exploration tool for both policy and model if valid.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a5778fa370052e32f2846ea596e1d3d80">More...</a><br /></td></tr>
<tr class="separator:a5778fa370052e32f2846ea596e1d3d80"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aab5f143a05fe679a551706f494ae92d4"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aab5f143a05fe679a551706f494ae92d4">_run_optimizer</a> (self)</td></tr>
<tr class="memdesc:aab5f143a05fe679a551706f494ae92d4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to train to process gradients (gradient normalization and clipping), run optimizer and LR Scheduler.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aab5f143a05fe679a551706f494ae92d4">More...</a><br /></td></tr>
<tr class="separator:aab5f143a05fe679a551706f494ae92d4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a8e806ef833a33b2972c2df202a78c7"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9a8e806ef833a33b2972c2df202a78c7">_set_attribute_custom_values</a> (self)</td></tr>
<tr class="memdesc:a9a8e806ef833a33b2972c2df202a78c7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method to set attributes for <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html" title="The ActorCriticAgent is the base class for actor-critic methods.">ActorCriticAgent</a> with custom values.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9a8e806ef833a33b2972c2df202a78c7">More...</a><br /></td></tr>
<tr class="separator:a9a8e806ef833a33b2972c2df202a78c7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab36ef5c2f16a926750db603d7df2eb49"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab36ef5c2f16a926750db603d7df2eb49">_share_gradients</a> (self)</td></tr>
<tr class="memdesc:ab36ef5c2f16a926750db603d7df2eb49"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method to share gradients to a single model.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab36ef5c2f16a926750db603d7df2eb49">More...</a><br /></td></tr>
<tr class="separator:ab36ef5c2f16a926750db603d7df2eb49"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac435775014322900099c4ac1f0febfba"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac435775014322900099c4ac1f0febfba">_share_parameters</a> (self)</td></tr>
<tr class="memdesc:ac435775014322900099c4ac1f0febfba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method to share parameters from a single model.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac435775014322900099c4ac1f0febfba">More...</a><br /></td></tr>
<tr class="separator:ac435775014322900099c4ac1f0febfba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3e021e7fd100a12ebb8c9347749f96ab"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3e021e7fd100a12ebb8c9347749f96ab">_train_policy_model</a> (self)</td></tr>
<tr class="memdesc:a3e021e7fd100a12ebb8c9347749f96ab"><td class="mdescLeft">&#160;</td><td class="mdescRight">The method to train policy models.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3e021e7fd100a12ebb8c9347749f96ab">More...</a><br /></td></tr>
<tr class="separator:a3e021e7fd100a12ebb8c9347749f96ab"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-static-methods" name="pri-static-methods"></a>
Static Private Member Functions</h2></td></tr>
<tr class="memitem:a37fcd54c6561401de77d211d7a9ec97f"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a37fcd54c6561401de77d211d7a9ec97f">_get_rollout_buffer_size</a> (Union[int, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#abb8397a2c8306113ec191e551fab2f64">training_frequency</a>, Union[int, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2e03df778b8d4d48cc85892818d2a6e6">exploration_steps</a>)</td></tr>
<tr class="memdesc:a37fcd54c6561401de77d211d7a9ec97f"><td class="mdescLeft">&#160;</td><td class="mdescRight">A utility function to obtain the size of rollout buffer.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a37fcd54c6561401de77d211d7a9ec97f">More...</a><br /></td></tr>
<tr class="separator:a37fcd54c6561401de77d211d7a9ec97f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a62db06fc65eba4294c02c847618a105e"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a62db06fc65eba4294c02c847618a105e">_is_done</a> (Union[bool, int] done)</td></tr>
<tr class="memdesc:a62db06fc65eba4294c02c847618a105e"><td class="mdescLeft">&#160;</td><td class="mdescRight">A basic utility function to know if episode has terminated if there are different types of <code>done</code>  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a62db06fc65eba4294c02c847618a105e">More...</a><br /></td></tr>
<tr class="separator:a62db06fc65eba4294c02c847618a105e"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-attribs" name="pri-attribs"></a>
Private Attributes</h2></td></tr>
<tr class="memitem:ac3819f27e57c053e66dc560d57fecec9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac3819f27e57c053e66dc560d57fecec9">_agent_has_exploration_tool</a></td></tr>
<tr class="memdesc:ac3819f27e57c053e66dc560d57fecec9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Flag indicating the agent has an exploration initialized.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac3819f27e57c053e66dc560d57fecec9">More...</a><br /></td></tr>
<tr class="separator:ac3819f27e57c053e66dc560d57fecec9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aea7f62cfe7b129431fa4bf3598463708"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aea7f62cfe7b129431fa4bf3598463708">_grad_accumulator</a></td></tr>
<tr class="memdesc:aea7f62cfe7b129431fa4bf3598463708"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of gradients from each backward call.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aea7f62cfe7b129431fa4bf3598463708">More...</a><br /></td></tr>
<tr class="separator:aea7f62cfe7b129431fa4bf3598463708"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a760ac2fe8d82171dd05d79717b5d72b4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a760ac2fe8d82171dd05d79717b5d72b4">_master_process_rank</a></td></tr>
<tr class="memdesc:a760ac2fe8d82171dd05d79717b5d72b4"><td class="mdescLeft">&#160;</td><td class="mdescRight">The master process id for multi-agents.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a760ac2fe8d82171dd05d79717b5d72b4">More...</a><br /></td></tr>
<tr class="separator:a760ac2fe8d82171dd05d79717b5d72b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8ad137357b10ed57e1e47523b9f1b19"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af8ad137357b10ed57e1e47523b9f1b19">_normalization</a></td></tr>
<tr class="memdesc:af8ad137357b10ed57e1e47523b9f1b19"><td class="mdescLeft">&#160;</td><td class="mdescRight">The normalisation tool to be used for agent.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af8ad137357b10ed57e1e47523b9f1b19">More...</a><br /></td></tr>
<tr class="separator:af8ad137357b10ed57e1e47523b9f1b19"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a83b75688d5a410a29b8ff88df4dcaf79"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a83b75688d5a410a29b8ff88df4dcaf79">_perform_grad_processing</a></td></tr>
<tr class="memdesc:a83b75688d5a410a29b8ff88df4dcaf79"><td class="mdescLeft">&#160;</td><td class="mdescRight">The flag indicating weather to perform gradient processing (normalization and clipping).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a83b75688d5a410a29b8ff88df4dcaf79">More...</a><br /></td></tr>
<tr class="separator:a83b75688d5a410a29b8ff88df4dcaf79"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5a1316a4ebac35c07ed04ff1c52324d7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a5a1316a4ebac35c07ed04ff1c52324d7">_policy_model_has_exploration_tool</a></td></tr>
<tr class="memdesc:a5a1316a4ebac35c07ed04ff1c52324d7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Flag indicating the model has an exploration initialized.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a5a1316a4ebac35c07ed04ff1c52324d7">More...</a><br /></td></tr>
<tr class="separator:a5a1316a4ebac35c07ed04ff1c52324d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad03fd6be4b4b539401f8a26407087721"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ad03fd6be4b4b539401f8a26407087721">_policy_outputs_exploration</a></td></tr>
<tr class="memdesc:ad03fd6be4b4b539401f8a26407087721"><td class="mdescLeft">&#160;</td><td class="mdescRight">Flag indicating whether to perform post-forward exploration (for exploring policy outputs).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ad03fd6be4b4b539401f8a26407087721">More...</a><br /></td></tr>
<tr class="separator:ad03fd6be4b4b539401f8a26407087721"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a97aa15389fe7e32a8f52cbf4d02244fd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a97aa15389fe7e32a8f52cbf4d02244fd">_process_group</a></td></tr>
<tr class="memdesc:a97aa15389fe7e32a8f52cbf4d02244fd"><td class="mdescLeft">&#160;</td><td class="mdescRight">The process group currently being used.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a97aa15389fe7e32a8f52cbf4d02244fd">More...</a><br /></td></tr>
<tr class="separator:a97aa15389fe7e32a8f52cbf4d02244fd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3e0fca95ea870ecb20e1946a8bbe909b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3e0fca95ea870ecb20e1946a8bbe909b">_process_rank</a></td></tr>
<tr class="memdesc:a3e0fca95ea870ecb20e1946a8bbe909b"><td class="mdescLeft">&#160;</td><td class="mdescRight">The process rank for multi-agents.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3e0fca95ea870ecb20e1946a8bbe909b">More...</a><br /></td></tr>
<tr class="separator:a3e0fca95ea870ecb20e1946a8bbe909b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a34547eb17d04d4f5a476a2543f5a97f4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a34547eb17d04d4f5a476a2543f5a97f4">_rollout_buffer</a></td></tr>
<tr class="memdesc:a34547eb17d04d4f5a476a2543f5a97f4"><td class="mdescLeft">&#160;</td><td class="mdescRight">The rollout buffer to be used for agent to store necessary outputs.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a34547eb17d04d4f5a476a2543f5a97f4">More...</a><br /></td></tr>
<tr class="separator:a34547eb17d04d4f5a476a2543f5a97f4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acb5492ba3a7be76db7ef61b5ab46bd59"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#acb5492ba3a7be76db7ef61b5ab46bd59">_take_backward_step</a></td></tr>
<tr class="memdesc:acb5492ba3a7be76db7ef61b5ab46bd59"><td class="mdescLeft">&#160;</td><td class="mdescRight">The flag indicating weather to perform backward pass.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#acb5492ba3a7be76db7ef61b5ab46bd59">More...</a><br /></td></tr>
<tr class="separator:acb5492ba3a7be76db7ef61b5ab46bd59"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a83624aedbe1af4c1073c1aa9c540f27c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a83624aedbe1af4c1073c1aa9c540f27c">_take_forward_step</a></td></tr>
<tr class="memdesc:a83624aedbe1af4c1073c1aa9c540f27c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The flag indicating weather to perform forward pass.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a83624aedbe1af4c1073c1aa9c540f27c">More...</a><br /></td></tr>
<tr class="separator:a83624aedbe1af4c1073c1aa9c540f27c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4e11a94879f78f0bcc4d901d14d3ed34"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4e11a94879f78f0bcc4d901d14d3ed34">_take_lr_scheduler_step</a></td></tr>
<tr class="memdesc:a4e11a94879f78f0bcc4d901d14d3ed34"><td class="mdescLeft">&#160;</td><td class="mdescRight">The flag indicating weather to take LR scheduler step.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4e11a94879f78f0bcc4d901d14d3ed34">More...</a><br /></td></tr>
<tr class="separator:a4e11a94879f78f0bcc4d901d14d3ed34"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aee23902a5e5bf19132775848b3aad24f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aee23902a5e5bf19132775848b3aad24f">_take_optimizer_step</a></td></tr>
<tr class="memdesc:aee23902a5e5bf19132775848b3aad24f"><td class="mdescLeft">&#160;</td><td class="mdescRight">The flag indicating weather to take optimizer step.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aee23902a5e5bf19132775848b3aad24f">More...</a><br /></td></tr>
<tr class="separator:aee23902a5e5bf19132775848b3aad24f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0435f3555d4c757742f2fa5c6b66e56"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa0435f3555d4c757742f2fa5c6b66e56">_world_size</a></td></tr>
<tr class="memdesc:aa0435f3555d4c757742f2fa5c6b66e56"><td class="mdescLeft">&#160;</td><td class="mdescRight">The world size for multi-agents.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa0435f3555d4c757742f2fa5c6b66e56">More...</a><br /></td></tr>
<tr class="separator:aa0435f3555d4c757742f2fa5c6b66e56"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >The <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html" title="The ActorCriticAgent is the base class for actor-critic methods.">ActorCriticAgent</a> is the base class for actor-critic methods. </p>
<p >This class implements basic methods and abstract functions for actor-critic methods. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a0f56b79ec5543b3708e82ba2154ec206" name="a0f56b79ec5543b3708e82ba2154ec206"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0f56b79ec5543b3708e82ba2154ec206">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classrlpack_1_1utils_1_1base_1_1model_1_1_model.html">Model</a>&#160;</td>
          <td class="paramname"><em>policy_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.optim.Optimizer&#160;</td>
          <td class="paramname"><em>optimizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[<a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_l_r_scheduler.html">LRScheduler</a>, None]&#160;</td>
          <td class="paramname"><em>lr_scheduler</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_loss_function.html">LossFunction</a>&#160;</td>
          <td class="paramname"><em>loss_function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Type[pytorch_distributions.Distribution]&#160;</td>
          <td class="paramname"><em>distribution</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>entropy_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>state_value_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>lr_threshold</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, Tuple[int, Union[List[int], None]]]&#160;</td>
          <td class="paramname"><em>action_space</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>backup_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>save_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>gae_lambda</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>batch_size</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, None] &#160;</td>
          <td class="paramname"><em>exploration_steps</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>grad_accumulation_rounds</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, None] &#160;</td>
          <td class="paramname"><em>training_frequency</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[<a class="el" href="classrlpack_1_1exploration_1_1utils_1_1exploration_1_1_exploration.html">Exploration</a>, None] &#160;</td>
          <td class="paramname"><em>exploration_tool</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>device</em> = <code>&quot;cpu&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>dtype</em> = <code>&quot;float32&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[<a class="el" href="classrlpack_1_1utils_1_1normalization_1_1_normalization.html">Normalization</a>, None] &#160;</td>
          <td class="paramname"><em>normalization_tool</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, List[str]] &#160;</td>
          <td class="paramname"><em>apply_norm_to</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>max_grad_norm</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>grad_norm_p</em> = <code>2.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>clip_grad_value</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">timedelta &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>timedelta(minutes=30)</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">policy_model</td><td>Model: The policy model to be used. Policy model must return a tuple of action logits and state values. </td></tr>
    <tr><td class="paramname">optimizer</td><td>pytorch.optim.Optimizer: The optimizer to be used for policy model. Optimizer must be initialized and wrapped with policy model parameters. </td></tr>
    <tr><td class="paramname">lr_scheduler</td><td>Union[LRScheduler, None]: The LR Scheduler to be used to decay the learning rate. LR Scheduler must be initialized and wrapped with passed optimizer. </td></tr>
    <tr><td class="paramname">loss_function</td><td>LossFunction: A PyTorch loss function. </td></tr>
    <tr><td class="paramname">distribution</td><td>: Type[pytorch_distributions.Distribution]: The distribution of PyTorch to be used to sampled actions in action space. (See <code>action_space</code>). </td></tr>
    <tr><td class="paramname">gamma</td><td>float: The discounting factor for rewards. </td></tr>
    <tr><td class="paramname">entropy_coefficient</td><td>float: The coefficient to be used for entropy in policy loss computation. </td></tr>
    <tr><td class="paramname">state_value_coefficient</td><td>float: The coefficient to be used for state value in final loss computation. </td></tr>
    <tr><td class="paramname">lr_threshold</td><td>float: The threshold LR which once reached LR scheduler is not called further. </td></tr>
    <tr><td class="paramname">action_space</td><td>Union[int, Tuple[int, Union[List[int], None]]]: The action space of the environment.<ul>
<li>If discrete action set is used, number of actions can be passed.</li>
<li>If continuous action space is used, a list must be passed with first element representing the output features from model, second element representing the shape of action to be sampled. Second element can be an empty list, if you wish to sample the default no. of samples. </li>
</ul>
</td></tr>
    <tr><td class="paramname">backup_frequency</td><td>int: The timesteps after which policy model, optimizer states and lr scheduler states are backed up. </td></tr>
    <tr><td class="paramname">save_path</td><td>str: The path where policy model, optimizer states and lr scheduler states are to be saved. </td></tr>
    <tr><td class="paramname">gae_lambda</td><td>float: The Generalized Advantage Estimation coefficient (referred to as lambda), indicating the bias-variance trade-off. </td></tr>
    <tr><td class="paramname">batch_size</td><td>int: The batch size to be used while training policy model. Default: 1 </td></tr>
    <tr><td class="paramname">exploration_steps</td><td>Union[int, None]: The size of rollout buffer before performing optimizer step. Whole rollout buffer is used to fit the policy model and is cleared. By default, after every episode. Default: None. </td></tr>
    <tr><td class="paramname">grad_accumulation_rounds</td><td>int: The number of rounds until which gradients are to be accumulated before performing calling optimizer step. Gradients are mean reduced for grad_accumulation_rounds &gt; 1. Default: 1. </td></tr>
    <tr><td class="paramname">training_frequency</td><td>Union[int, None]: The number of timesteps after which policy model is to be trained. By default, training is done at the end of an episode: Default: None. </td></tr>
    <tr><td class="paramname">exploration_tool</td><td>Union[Exploration, None]: Exploration tool to be used to explore the environment. These tools can be found in <code><a class="el" href="namespacerlpack_1_1exploration.html" title="This package implements the exploration tools for RLPack to explore the environment.">rlpack.exploration</a></code>. </td></tr>
    <tr><td class="paramname">device</td><td>str: The device on which models are run. Default: "cpu". </td></tr>
    <tr><td class="paramname">dtype</td><td>str: The datatype for model parameters. Default: "float32". </td></tr>
    <tr><td class="paramname">normalization_tool</td><td>Union[Normalization, None]: The normalization tool to be used. This must be an instance of <a class="el" href="classrlpack_1_1utils_1_1normalization_1_1_normalization.html" title="Normalization class providing methods for normalization techniques.">rlpack.utils.normalization.Normalization</a> if passed. By default, is initialized to None and no normalization takes place. If passed, make sure a valid <code>apply_norm_to</code> is passed. </td></tr>
    <tr><td class="paramname">apply_norm_to</td><td>Union[int, List[str]]: The code to select the quantity to which normalization is to be applied. Direct list of quantities can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1. </td></tr>
    <tr><td class="paramname">max_grad_norm</td><td>Optional[float]: The max norm for gradients for gradient clipping. Default: None </td></tr>
    <tr><td class="paramname">grad_norm_p</td><td>float: The p-value for p-normalization of gradients. Default: 2.0 </td></tr>
    <tr><td class="paramname">clip_grad_value</td><td>Optional[float]: The gradient value for clipping gradients by value. Default: None </td></tr>
    <tr><td class="paramname">timeout</td><td>timedelta: The timeout for synchronous calls. Default is 30 minutes.</td></tr>
  </table>
  </dd>
</dl>
<p><b>Notes</b></p>
<p >The value accepted for <code>apply_norm_to</code> are as follows and must be passed in a list:</p><ul>
<li><code>"none"</code>: -1; Don't apply normalization to any quantity.</li>
<li><code>"states"</code>: 0; Apply normalization to states.</li>
<li><code>"state_values"</code>: 1; Apply normalization to state values.</li>
<li><code>"rewards"</code>: 2; Apply normalization to rewards.</li>
<li><code>"returns"</code>: 3; Apply normalization to rewards.</li>
<li><code>"td"</code>: 4; Apply normalization for TD values.</li>
<li><code>"advantage"</code>: 5; Apply normalization to advantage values</li>
</ul>
<p >If a valid <code>max_norm_grad</code> is passed, then gradient clipping takes place else gradient clipping step is skipped. If <code>max_norm_grad</code> value was invalid, error will be raised from PyTorch. If a valid <code>clip_grad_value</code> is passed, then gradients will be clipped by value. If <code>clip_grad_value</code> value was invalid, error will be raised from PyTorch. </p>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">rlpack.utils.base.agent.Agent</a>.</p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a0df43b14dcaa3227f064ab1a88b1a83d">rlpack.actor_critic.a2c.A2C</a>, <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#ab98e9964d08929fedd3f8d2d50467fdb">rlpack.actor_critic.a3c.A3C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html#aa350ba97954dd49960eaac6279d4063b">rlpack.actor_critic.ac.AC</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a85d179455a7941657e1d4cbd94e3522d" name="a85d179455a7941657e1d4cbd94e3522d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a85d179455a7941657e1d4cbd94e3522d">&#9670;&#160;</a></span>_add_noise_to_actions()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._add_noise_to_actions </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>action</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Adds noise to the given action according to the exploration tool being used. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">action</td><td>pytorch.Tensor: The action tensors </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>: pytorch.Tensor: The noisy action tensor. </dd></dl>

</div>
</div>
<a id="a22282972de3a62a2aa7b526c3e869a97" name="a22282972de3a62a2aa7b526c3e869a97"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a22282972de3a62a2aa7b526c3e869a97">&#9670;&#160;</a></span>_backward_pass()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> bool rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._backward_pass </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method to perform backward pass. </p>
<p >This method will compute gradients and accumulate gradients if required. </p><dl class="section return"><dt>Returns</dt><dd>bool: Indicates if gradients were accumulated when True; False if accumulated gradients were reduced and loaded into policy model. </dd></dl>

</div>
</div>
<a id="a047cbca9519b71255a812248a48c2cf6" name="a047cbca9519b71255a812248a48c2cf6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a047cbca9519b71255a812248a48c2cf6">&#9670;&#160;</a></span>_call_to_extend_transitions()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._call_to_extend_transitions </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method calling the method <a class="el" href="classrlpack_1_1___c_1_1rollout__buffer_1_1_rollout_buffer.html#a9d61053d9e8d7f6883e8a2e53c79303e" title="Method to extend the transitions.">RolloutBuffer.extend_transitions</a>. </p>
<p >Must be overriden by subclasses. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#af54ad5d4b3644f5436b2c7046914fb12">rlpack.actor_critic.a2c.A2C</a>, <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#adbe30f2d28526c27e692f0a84271db14">rlpack.actor_critic.a3c.A3C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html#a6d3c0ef8e7e91c9ae36ac06814f5a0ca">rlpack.actor_critic.ac.AC</a>.</p>

</div>
</div>
<a id="a79da689b8d6edfd4ae0f3128ac130d57" name="a79da689b8d6edfd4ae0f3128ac130d57"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a79da689b8d6edfd4ae0f3128ac130d57">&#9670;&#160;</a></span>_call_to_save()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._call_to_save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method calling the save method when required. </p>
<p >Must be overriden by subclasses. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a487c58b125fb7cd2e14972552bdf5ea6">rlpack.actor_critic.a2c.A2C</a>, <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#ac2f6faa17ef5e68d665f2e085636b5d2">rlpack.actor_critic.a3c.A3C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html#a007d495e6f8e3b960d7ff5c5f2ee2a15">rlpack.actor_critic.ac.AC</a>.</p>

</div>
</div>
<a id="a9811685e91f1fa1137029243b7f78101" name="a9811685e91f1fa1137029243b7f78101"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9811685e91f1fa1137029243b7f78101">&#9670;&#160;</a></span>_clear_rollout_buffer()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._clear_rollout_buffer </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to clear the rollout buffer (transitions and policy outputs) </p>

</div>
</div>
<a id="a0dddd26f1bfa9e729fb771a28b2cd4bb" name="a0dddd26f1bfa9e729fb771a28b2cd4bb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0dddd26f1bfa9e729fb771a28b2cd4bb">&#9670;&#160;</a></span>_compute_loss()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._compute_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method to compute total loss (from actor and critic). </p>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The loss tensor. </dd></dl>

</div>
</div>
<a id="a97bd95a17383eabb3d8cb6bc89ff1a45" name="a97bd95a17383eabb3d8cb6bc89ff1a45"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a97bd95a17383eabb3d8cb6bc89ff1a45">&#9670;&#160;</a></span>_create_action_distribution()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch_distributions.Distribution rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._create_action_distribution </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[List[pytorch.Tensor], pytorch.Tensor]&#160;</td>
          <td class="paramname"><em>action_values</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected static method to create distributions from action logits. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">action_values</td><td>Union[List[pytorch.Tensor], pytorch.Tensor]: The action values from policy model </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Distribution: A Distribution object initialized with given action logits </dd></dl>

</div>
</div>
<a id="adfa080bcb55a6011a736a143d6276cff" name="adfa080bcb55a6011a736a143d6276cff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adfa080bcb55a6011a736a143d6276cff">&#9670;&#160;</a></span>_finish_policy_outputs_exploration()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._finish_policy_outputs_exploration </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Finishes exploration in policy outputs and sets the necessary statistics if requested for normalization. </p>
<p >Sets the attribute <code>exploration_steps</code> to None. Sets the attribute <code>_policy_outputs_exploration</code> to False. </p>

</div>
</div>
<a id="af2b590bfae29e2ba3f386d3c85208027" name="af2b590bfae29e2ba3f386d3c85208027"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af2b590bfae29e2ba3f386d3c85208027">&#9670;&#160;</a></span>_finish_transitions_exploration()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._finish_transitions_exploration </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Finishes exploration in transitions and sets the necessary statistics if requested for normalization. </p>
<p >Sets the attribute <code>exploration_steps</code> to None </p>

</div>
</div>
<a id="a19374ccf78639131b609c67e747561bc" name="a19374ccf78639131b609c67e747561bc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a19374ccf78639131b609c67e747561bc">&#9670;&#160;</a></span>_forward_pass()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._forward_pass </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method to perform forward pass. </p>
<p >This method will iterate through batches of transitions and accumulate policy outputs in rollout buffer. </p>

</div>
</div>
<a id="a3b095ff9e0f3527fa9e27d9aba3eed6a" name="a3b095ff9e0f3527fa9e27d9aba3eed6a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3b095ff9e0f3527fa9e27d9aba3eed6a">&#9670;&#160;</a></span>_get_action_from_distribution()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._get_action_from_distribution </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch_distributions.Distribution&#160;</td>
          <td class="paramname"><em>distribution</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>reparametrize</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Creates action from the given distribution by sampling. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">distribution</td><td>pytorch_distributions.Distribution: The distribution object to be used. </td></tr>
    <tr><td class="paramname">reparametrize</td><td>bool: Whether to use reparametrization trick while sampling. When set to True, operations are attached to computation graph. This is only valid for continuous action spaces. Default: False </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The sampled action tensor. </dd></dl>

</div>
</div>
<a id="ac8f1674df6616eb003161c69ae61c7d7" name="ac8f1674df6616eb003161c69ae61c7d7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac8f1674df6616eb003161c69ae61c7d7">&#9670;&#160;</a></span>_get_action_sample_shape()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Size rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._get_action_sample_shape </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets the action sample shape to be sampled from distribution. </p>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Size: Sample shape of to-be sampled tensor from continuous distribution </dd></dl>

</div>
</div>
<a id="a37fcd54c6561401de77d211d7a9ec97f" name="a37fcd54c6561401de77d211d7a9ec97f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a37fcd54c6561401de77d211d7a9ec97f">&#9670;&#160;</a></span>_get_rollout_buffer_size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> int rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._get_rollout_buffer_size </td>
          <td>(</td>
          <td class="paramtype">Union[int, None]&#160;</td>
          <td class="paramname"><em>training_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, None]
    &#160;</td>
          <td class="paramname"><em>exploration_steps</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>A utility function to obtain the size of rollout buffer. </p>
<p >Note that space is reserved as per buffer size and exceeding buffer size can often make the program prone to error. If both arguments are None, this will be initialized to 1024.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">training_frequency</td><td>Union[int, None]: The <code>training_frequency</code> passed in <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0f56b79ec5543b3708e82ba2154ec206">ActorCriticAgent.__init__</a>. </td></tr>
    <tr><td class="paramname">exploration_steps</td><td>Union[int, None] The <code>exploration_steps</code> passed in <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0f56b79ec5543b3708e82ba2154ec206">ActorCriticAgent.__init__</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>: int: The buffer size set. </dd></dl>

</div>
</div>
<a id="afef85f43b81e3743e774ca8b47265b72" name="afef85f43b81e3743e774ca8b47265b72"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afef85f43b81e3743e774ca8b47265b72">&#9670;&#160;</a></span>_grad_mean_reduction()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._grad_mean_reduction </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Performs mean reduction for accumulated gradients and assigns the policy model's parameter the mean reduced gradients. </p>

</div>
</div>
<a id="a62db06fc65eba4294c02c847618a105e" name="a62db06fc65eba4294c02c847618a105e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a62db06fc65eba4294c02c847618a105e">&#9670;&#160;</a></span>_is_done()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> bool rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._is_done </td>
          <td>(</td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>A basic utility function to know if episode has terminated if there are different types of <code>done</code> </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">done</td><td>Union[bool, int]: The done flag from OpenAI gym. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>: bool: The done flag in boolean </dd></dl>

</div>
</div>
<a id="a5778fa370052e32f2846ea596e1d3d80" name="a5778fa370052e32f2846ea596e1d3d80"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5778fa370052e32f2846ea596e1d3d80">&#9670;&#160;</a></span>_reset_exploration_tool()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._reset_exploration_tool </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Resets exploration tool for both policy and model if valid. </p>

</div>
</div>
<a id="aab5f143a05fe679a551706f494ae92d4" name="aab5f143a05fe679a551706f494ae92d4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aab5f143a05fe679a551706f494ae92d4">&#9670;&#160;</a></span>_run_optimizer()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._run_optimizer </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to train to process gradients (gradient normalization and clipping), run optimizer and LR Scheduler. </p>

</div>
</div>
<a id="a9a8e806ef833a33b2972c2df202a78c7" name="a9a8e806ef833a33b2972c2df202a78c7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9a8e806ef833a33b2972c2df202a78c7">&#9670;&#160;</a></span>_set_attribute_custom_values()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._set_attribute_custom_values </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method to set attributes for <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html" title="The ActorCriticAgent is the base class for actor-critic methods.">ActorCriticAgent</a> with custom values. </p>
<p >This method is called in <b>init</b>. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a3ca76a98e5389e702f4a2f04dae6a032">rlpack.actor_critic.a2c.A2C</a>, <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#a2487eaf6b388b5ad1430824672259c59">rlpack.actor_critic.a3c.A3C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html#a96879aa0816a2f1da8b954ba1fa2f039">rlpack.actor_critic.ac.AC</a>.</p>

</div>
</div>
<a id="ab36ef5c2f16a926750db603d7df2eb49" name="ab36ef5c2f16a926750db603d7df2eb49"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab36ef5c2f16a926750db603d7df2eb49">&#9670;&#160;</a></span>_share_gradients()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._share_gradients </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method to share gradients to a single model. </p>
<p >This must typically implement reduce collective communication operation. Must be overriden by subclasses. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a92571d3fed20230cef09be3128493712">rlpack.actor_critic.a2c.A2C</a>, <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#aaa59aaec166391e1ebc8e2eb04683ac6">rlpack.actor_critic.a3c.A3C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html#a046f6c559232d908e2d2775ede74339c">rlpack.actor_critic.ac.AC</a>.</p>

</div>
</div>
<a id="ac435775014322900099c4ac1f0febfba" name="ac435775014322900099c4ac1f0febfba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac435775014322900099c4ac1f0febfba">&#9670;&#160;</a></span>_share_parameters()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._share_parameters </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method to share parameters from a single model. </p>
<p >This must typically implement broadcast collective communication operation. Must be overriden by subclasses. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#acdfdd9b1c1d04aca803128b6ba8c03f5">rlpack.actor_critic.a2c.A2C</a>, <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#a7e371a3aa442eb45be17a7f52be82d84">rlpack.actor_critic.a3c.A3C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html#a98be5cd789b09ae6f95fe613d61014ca">rlpack.actor_critic.ac.AC</a>.</p>

</div>
</div>
<a id="a3e021e7fd100a12ebb8c9347749f96ab" name="a3e021e7fd100a12ebb8c9347749f96ab"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3e021e7fd100a12ebb8c9347749f96ab">&#9670;&#160;</a></span>_train_policy_model()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._train_policy_model </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The method to train policy models. </p>
<p >This calls forward pass, backward pass and optimizer to perform training. </p>

</div>
</div>
<a id="a369d886df0d6762cb825f10682607581" name="a369d886df0d6762cb825f10682607581"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a369d886df0d6762cb825f10682607581">&#9670;&#160;</a></span>load()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.load </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to load the best model by a custom suffix supplied for evaluation. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="ae8e3441052d7b47fa91188a684e18659" name="ae8e3441052d7b47fa91188a684e18659"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae8e3441052d7b47fa91188a684e18659">&#9670;&#160;</a></span>policy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.ndarray rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.policy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The policy method to evaluate the agent. </p>
<p >This runs in pure inference mode. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned from gym environment </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>np.ndarray: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="ad0e78685793f897be63b701cb75b6fc5" name="ad0e78685793f897be63b701cb75b6fc5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad0e78685793f897be63b701cb75b6fc5">&#9670;&#160;</a></span>save()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<p >agent_states includes current memory and epsilon values in a dictionary. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to save best model by a custom suffix supplied during a train run. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="a378c9f9d11f38263c43bda87ca5d01ac" name="a378c9f9d11f38263c43bda87ca5d01ac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a378c9f9d11f38263c43bda87ca5d01ac">&#9670;&#160;</a></span>train()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.ndarray rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.train </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_next</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, float]&#160;</td>
          <td class="paramname"><em>reward</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The train method to train the agent and underlying policy model. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state. </td></tr>
    <tr><td class="paramname">state_next</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The next state returned. </td></tr>
    <tr><td class="paramname">reward</td><td>Union[int, float]: The reward returned from previous action </td></tr>
    <tr><td class="paramname">done</td><td>Union[bool, int]: Flag indicating if episode has terminated or not </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>np.ndarray: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<h2 class="groupheader">Field Documentation</h2>
<a id="ac3819f27e57c053e66dc560d57fecec9" name="ac3819f27e57c053e66dc560d57fecec9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac3819f27e57c053e66dc560d57fecec9">&#9670;&#160;</a></span>_agent_has_exploration_tool</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._agent_has_exploration_tool</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Flag indicating the agent has an exploration initialized. </p>

</div>
</div>
<a id="aea7f62cfe7b129431fa4bf3598463708" name="aea7f62cfe7b129431fa4bf3598463708"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aea7f62cfe7b129431fa4bf3598463708">&#9670;&#160;</a></span>_grad_accumulator</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._grad_accumulator</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The list of gradients from each backward call. </p>
<p >This is only used when boostrap_rounds &gt; 1 and is cleared after each boostrap round. The <a class="el" href="classrlpack_1_1___c_1_1grad__accumulator_1_1_grad_accumulator.html" title="This class provides the python interface to C_GradAccumulator, the C++ class which performs heavier w...">rlpack._C.grad_accumulator.GradAccumulator</a> object for grad accumulation. </p>

</div>
</div>
<a id="a760ac2fe8d82171dd05d79717b5d72b4" name="a760ac2fe8d82171dd05d79717b5d72b4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a760ac2fe8d82171dd05d79717b5d72b4">&#9670;&#160;</a></span>_master_process_rank</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._master_process_rank</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The master process id for multi-agents. </p>
<p >Is set to 0 as a standard. </p>

</div>
</div>
<a id="af8ad137357b10ed57e1e47523b9f1b19" name="af8ad137357b10ed57e1e47523b9f1b19"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af8ad137357b10ed57e1e47523b9f1b19">&#9670;&#160;</a></span>_normalization</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._normalization</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The normalisation tool to be used for agent. </p>
<p >An instance of <a class="el" href="classrlpack_1_1utils_1_1normalization_1_1_normalization.html" title="Normalization class providing methods for normalization techniques.">rlpack.utils.normalization.Normalization</a>. </p>

</div>
</div>
<a id="a83b75688d5a410a29b8ff88df4dcaf79" name="a83b75688d5a410a29b8ff88df4dcaf79"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a83b75688d5a410a29b8ff88df4dcaf79">&#9670;&#160;</a></span>_perform_grad_processing</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._perform_grad_processing</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The flag indicating weather to perform gradient processing (normalization and clipping). </p>

</div>
</div>
<a id="a5a1316a4ebac35c07ed04ff1c52324d7" name="a5a1316a4ebac35c07ed04ff1c52324d7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5a1316a4ebac35c07ed04ff1c52324d7">&#9670;&#160;</a></span>_policy_model_has_exploration_tool</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._policy_model_has_exploration_tool</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Flag indicating the model has an exploration initialized. </p>

</div>
</div>
<a id="ad03fd6be4b4b539401f8a26407087721" name="ad03fd6be4b4b539401f8a26407087721"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad03fd6be4b4b539401f8a26407087721">&#9670;&#160;</a></span>_policy_outputs_exploration</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._policy_outputs_exploration</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Flag indicating whether to perform post-forward exploration (for exploring policy outputs). </p>

</div>
</div>
<a id="a97aa15389fe7e32a8f52cbf4d02244fd" name="a97aa15389fe7e32a8f52cbf4d02244fd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a97aa15389fe7e32a8f52cbf4d02244fd">&#9670;&#160;</a></span>_process_group</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._process_group</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The process group currently being used. </p>
<p >For uni-agents, will be None. </p>

</div>
</div>
<a id="a3e0fca95ea870ecb20e1946a8bbe909b" name="a3e0fca95ea870ecb20e1946a8bbe909b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3e0fca95ea870ecb20e1946a8bbe909b">&#9670;&#160;</a></span>_process_rank</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._process_rank</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The process rank for multi-agents. </p>
<p >For uni-agents, will be None. </p>

</div>
</div>
<a id="a34547eb17d04d4f5a476a2543f5a97f4" name="a34547eb17d04d4f5a476a2543f5a97f4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a34547eb17d04d4f5a476a2543f5a97f4">&#9670;&#160;</a></span>_rollout_buffer</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._rollout_buffer</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The rollout buffer to be used for agent to store necessary outputs. </p>
<p >An instance of <a class="el" href="classrlpack_1_1___c_1_1rollout__buffer_1_1_rollout_buffer.html">rlpack._C.rollout_buffer.RolloutBuffer</a> </p>

</div>
</div>
<a id="acb5492ba3a7be76db7ef61b5ab46bd59" name="acb5492ba3a7be76db7ef61b5ab46bd59"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acb5492ba3a7be76db7ef61b5ab46bd59">&#9670;&#160;</a></span>_take_backward_step</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._take_backward_step</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The flag indicating weather to perform backward pass. </p>

</div>
</div>
<a id="a83624aedbe1af4c1073c1aa9c540f27c" name="a83624aedbe1af4c1073c1aa9c540f27c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a83624aedbe1af4c1073c1aa9c540f27c">&#9670;&#160;</a></span>_take_forward_step</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._take_forward_step</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The flag indicating weather to perform forward pass. </p>

</div>
</div>
<a id="a4e11a94879f78f0bcc4d901d14d3ed34" name="a4e11a94879f78f0bcc4d901d14d3ed34"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4e11a94879f78f0bcc4d901d14d3ed34">&#9670;&#160;</a></span>_take_lr_scheduler_step</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._take_lr_scheduler_step</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The flag indicating weather to take LR scheduler step. </p>
<p >It is dangerous to modify this externally. </p>

</div>
</div>
<a id="aee23902a5e5bf19132775848b3aad24f" name="aee23902a5e5bf19132775848b3aad24f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aee23902a5e5bf19132775848b3aad24f">&#9670;&#160;</a></span>_take_optimizer_step</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._take_optimizer_step</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The flag indicating weather to take optimizer step. </p>
<p >It is dangerous to modify this externally. <br  />
 </p>

</div>
</div>
<a id="aa0435f3555d4c757742f2fa5c6b66e56" name="aa0435f3555d4c757742f2fa5c6b66e56"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa0435f3555d4c757742f2fa5c6b66e56">&#9670;&#160;</a></span>_world_size</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._world_size</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The world size for multi-agents. </p>
<p >For uni-agents, will be None. </p>

</div>
</div>
<a id="a31e3fa147ea1d19490fb388ffe61aee3" name="a31e3fa147ea1d19490fb388ffe61aee3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a31e3fa147ea1d19490fb388ffe61aee3">&#9670;&#160;</a></span>action_space</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.action_space</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input number of actions. </p>

</div>
</div>
<a id="a7e6b39931e27a1d569945fdb177d40cc" name="a7e6b39931e27a1d569945fdb177d40cc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7e6b39931e27a1d569945fdb177d40cc">&#9670;&#160;</a></span>apply_norm_to</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.apply_norm_to</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm_to</code> argument; indicating the quantity to normalise. </p>

</div>
</div>
<a id="af19d87d26f908a6938f3b3ec1d25c70c" name="af19d87d26f908a6938f3b3ec1d25c70c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af19d87d26f908a6938f3b3ec1d25c70c">&#9670;&#160;</a></span>backup_frequency</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.backup_frequency</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input model backup frequency in terms of timesteps. </p>

</div>
</div>
<a id="a7b67cf1d9ca70e89e39fa80c4be140ba" name="a7b67cf1d9ca70e89e39fa80c4be140ba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7b67cf1d9ca70e89e39fa80c4be140ba">&#9670;&#160;</a></span>batch_size</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.batch_size</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input batch size (from argument <code>batch_size</code>). </p>

</div>
</div>
<a id="aa7a4a5ac2156c6877cbc0533601724e7" name="aa7a4a5ac2156c6877cbc0533601724e7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa7a4a5ac2156c6877cbc0533601724e7">&#9670;&#160;</a></span>clip_grad_value</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.clip_grad_value</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>clip_grad_value</code>; indicating the clipping range for gradients. </p>

</div>
</div>
<a id="a05e8b2d000bd94f5fb854516c84200e5" name="a05e8b2d000bd94f5fb854516c84200e5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a05e8b2d000bd94f5fb854516c84200e5">&#9670;&#160;</a></span>device</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.device</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>device</code> argument; indicating the device name as device type class. </p>

</div>
</div>
<a id="a9244a2ba98485e2bedf6cee830ca1a22" name="a9244a2ba98485e2bedf6cee830ca1a22"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9244a2ba98485e2bedf6cee830ca1a22">&#9670;&#160;</a></span>distribution</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.distribution</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input distribution object. </p>

</div>
</div>
<a id="a6dc53028e64988f4a9442a047736acb8" name="a6dc53028e64988f4a9442a047736acb8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6dc53028e64988f4a9442a047736acb8">&#9670;&#160;</a></span>dtype</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.dtype</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>dtype</code> argument; indicating the datatype class. </p>

</div>
</div>
<a id="affc2a52ddeb68b4c0a6c05793fa58261" name="affc2a52ddeb68b4c0a6c05793fa58261"></a>
<h2 class="memtitle"><span class="permalink"><a href="#affc2a52ddeb68b4c0a6c05793fa58261">&#9670;&#160;</a></span>entropy_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.entropy_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input entropy coefficient. </p>

</div>
</div>
<a id="a2e03df778b8d4d48cc85892818d2a6e6" name="a2e03df778b8d4d48cc85892818d2a6e6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2e03df778b8d4d48cc85892818d2a6e6">&#9670;&#160;</a></span>exploration_steps</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.exploration_steps</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>rollout_accumulation_size</code>. </p>

</div>
</div>
<a id="ac76827b34c015165373ec76358b60dea" name="ac76827b34c015165373ec76358b60dea"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac76827b34c015165373ec76358b60dea">&#9670;&#160;</a></span>exploration_tool</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.exploration_tool</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>exploration_tool</code>. </p>

</div>
</div>
<a id="afee0ff0f5722297192f97483defd926b" name="afee0ff0f5722297192f97483defd926b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afee0ff0f5722297192f97483defd926b">&#9670;&#160;</a></span>gae_lambda</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.gae_lambda</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input GAE Lambda (from argument <code>gae_lambda</code>). </p>

</div>
</div>
<a id="ab09c920ad70ce8b4cd3fa485b2f1bd55" name="ab09c920ad70ce8b4cd3fa485b2f1bd55"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab09c920ad70ce8b4cd3fa485b2f1bd55">&#9670;&#160;</a></span>gamma</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.gamma</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input discounting factor. </p>

</div>
</div>
<a id="a724bcb1e4d32cd2cd880911eed207af6" name="a724bcb1e4d32cd2cd880911eed207af6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a724bcb1e4d32cd2cd880911eed207af6">&#9670;&#160;</a></span>grad_accumulation_rounds</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.grad_accumulation_rounds</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>grad_accumulation_rounds</code>. </p>

</div>
</div>
<a id="aa2bfe032740a7146997162d125274fe9" name="aa2bfe032740a7146997162d125274fe9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2bfe032740a7146997162d125274fe9">&#9670;&#160;</a></span>grad_norm_p</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.grad_norm_p</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings. </p>

</div>
</div>
<a id="a4b605080b32f1199d314ded5efa4230a" name="a4b605080b32f1199d314ded5efa4230a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4b605080b32f1199d314ded5efa4230a">&#9670;&#160;</a></span>is_continuous_action_space</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.is_continuous_action_space</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Flag indicating if action space is continuous or discrete. </p>

</div>
</div>
<a id="af5485148744d68c770b872e62361ca10" name="af5485148744d68c770b872e62361ca10"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af5485148744d68c770b872e62361ca10">&#9670;&#160;</a></span>loss_function</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.loss_function</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input loss function. </p>

</div>
</div>
<a id="a081dcbd51017df1d26abbf15821089d7" name="a081dcbd51017df1d26abbf15821089d7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a081dcbd51017df1d26abbf15821089d7">&#9670;&#160;</a></span>lr_scheduler</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.lr_scheduler</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optional LR Scheduler (this can be None). </p>

</div>
</div>
<a id="a2238587b60c04bf2a7e83e497a030b1e" name="a2238587b60c04bf2a7e83e497a030b1e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2238587b60c04bf2a7e83e497a030b1e">&#9670;&#160;</a></span>lr_threshold</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.lr_threshold</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input LR Threshold. </p>

</div>
</div>
<a id="ae98b91d40567e6e9990304977809d997" name="ae98b91d40567e6e9990304977809d997"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae98b91d40567e6e9990304977809d997">&#9670;&#160;</a></span>max_grad_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.max_grad_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings. </p>

</div>
</div>
<a id="aa2f798b74bd64640ef35da8f672f3e95" name="aa2f798b74bd64640ef35da8f672f3e95"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2f798b74bd64640ef35da8f672f3e95">&#9670;&#160;</a></span>optimizer</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.optimizer</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optimizer wrapped with policy_model parameters. </p>

</div>
</div>
<a id="a3c0e7b3a8cc0a96cebe026d304c5590c" name="a3c0e7b3a8cc0a96cebe026d304c5590c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3c0e7b3a8cc0a96cebe026d304c5590c">&#9670;&#160;</a></span>policy_model</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.policy_model</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input policy model moved to desired device. </p>

</div>
</div>
<a id="a00c1fb74f19174eecd6fcd03dc0b9843" name="a00c1fb74f19174eecd6fcd03dc0b9843"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00c1fb74f19174eecd6fcd03dc0b9843">&#9670;&#160;</a></span>save_path</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.save_path</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input save path for backing up agent models. </p>

</div>
</div>
<a id="a073e3f0455059d575ce1bc9a28b3eee5" name="a073e3f0455059d575ce1bc9a28b3eee5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a073e3f0455059d575ce1bc9a28b3eee5">&#9670;&#160;</a></span>state_value_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.state_value_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input state value coefficient. </p>

</div>
</div>
<a id="a42a2685035dad0c95918126682a0d4ff" name="a42a2685035dad0c95918126682a0d4ff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a42a2685035dad0c95918126682a0d4ff">&#9670;&#160;</a></span>step_counter</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.step_counter</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The step counter; counting the total timesteps done so far. </p>

</div>
</div>
<a id="a341810c1e41d795b3c82dc375e606d08" name="a341810c1e41d795b3c82dc375e606d08"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a341810c1e41d795b3c82dc375e606d08">&#9670;&#160;</a></span>timeout</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.timeout</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>timeout</code>; indicating the timeout for synchronous calls. </p>

</div>
</div>
<a id="abb8397a2c8306113ec191e551fab2f64" name="abb8397a2c8306113ec191e551fab2f64"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abb8397a2c8306113ec191e551fab2f64">&#9670;&#160;</a></span>training_frequency</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.training_frequency</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>training_frequency</code>. </p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacerlpack.html">rlpack</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic.html">actor_critic</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic_1_1utils.html">utils</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent.html">actor_critic_agent</a></li><li class="navelem"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html">ActorCriticAgent</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
