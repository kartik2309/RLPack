<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RLPack: rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="RLPack-logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">RLPack
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Data Fields</a> &#124;
<a href="#pri-methods">Private Member Functions</a> &#124;
<a href="#pri-attribs">Private Attributes</a>  </div>
  <div class="headertitle"><div class="title">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>The <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html" title="The ActorCriticAgent is the base class for actor-critic methods.">ActorCriticAgent</a> is the base class for actor-critic methods.  
 <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#details">More...</a></p>
<div id="dynsection-0" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-0-trigger" src="closed.png" alt="+"/> Inheritance diagram for rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent:</div>
<div id="dynsection-0-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-0-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent__inherit__graph.png" border="0" usemap="#arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_inherit__map" alt="Inheritance graph"/></div>
<map name="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_inherit__map" id="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_inherit__map">
<area shape="rect" title="The ActorCriticAgent is the base class for actor&#45;critic methods." alt="" coords="201,167,397,207"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html" title="The A2C class implements the asynchronous Actor&#45;Critic method with synchronization." alt="" coords="5,255,185,280"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html" title="The A3C class implements the asynchronous Actor&#45;Critic method." alt="" coords="209,255,389,280"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html" title="The ActorCritic class implements the basic Actor&#45;Critic method with single agent." alt="" coords="413,255,580,280"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="221,79,378,119"/>
<area shape="rect" title=" " alt="" coords="274,5,325,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<div id="dynsection-1" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-1-trigger" src="closed.png" alt="+"/> Collaboration diagram for rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent:</div>
<div id="dynsection-1-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-1-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent__coll__graph.png" border="0" usemap="#arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_coll__map" alt="Collaboration graph"/></div>
<map name="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_coll__map" id="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_coll__map">
<area shape="rect" title="The ActorCriticAgent is the base class for actor&#45;critic methods." alt="" coords="5,167,201,207"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="25,79,182,119"/>
<area shape="rect" title=" " alt="" coords="78,5,129,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a8a01a254acf0c63cc4f7ed7990b0b222"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a8a01a254acf0c63cc4f7ed7990b0b222">__init__</a> (self, pytorch.nn.Module <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3c0e7b3a8cc0a96cebe026d304c5590c">policy_model</a>, pytorch.optim.Optimizer <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2f798b74bd64640ef35da8f672f3e95">optimizer</a>, Union[<a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_l_r_scheduler.html">LRScheduler</a>, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a081dcbd51017df1d26abbf15821089d7">lr_scheduler</a>, <a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_loss_function.html">LossFunction</a> <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af5485148744d68c770b872e62361ca10">loss_function</a>, Type[pytorch_distributions.Distribution] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">distribution</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab09c920ad70ce8b4cd3fa485b2f1bd55">gamma</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#affc2a52ddeb68b4c0a6c05793fa58261">entropy_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a073e3f0455059d575ce1bc9a28b3eee5">state_value_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2238587b60c04bf2a7e83e497a030b1e">lr_threshold</a>, Union[int, Tuple[int, Union[List[int], None]]] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a31e3fa147ea1d19490fb388ffe61aee3">action_space</a>, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af19d87d26f908a6938f3b3ec1d25c70c">backup_frequency</a>, str <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a00c1fb74f19174eecd6fcd03dc0b9843">save_path</a>, Union[int, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9e282f966f2bd18c24ffba49ceefd00b">rollout_accumulation_size</a>=None, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a724bcb1e4d32cd2cd880911eed207af6">grad_accumulation_rounds</a>=1, Union[<a class="el" href="classrlpack_1_1exploration_1_1utils_1_1exploration_1_1_exploration.html">Exploration</a>, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac76827b34c015165373ec76358b60dea">exploration_tool</a>=None, str <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a05e8b2d000bd94f5fb854516c84200e5">device</a>=&quot;cpu&quot;, str <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a6dc53028e64988f4a9442a047736acb8">dtype</a>=&quot;float32&quot;, Union[int, str] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aff3d0599e56caa4b5b0b36cccbf684b1">apply_norm</a>=-1, Union[int, List[str]] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7e6b39931e27a1d569945fdb177d40cc">apply_norm_to</a>=-1, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a940d6caf441fdc4b732ef0185e2694e9">eps_for_norm</a>=5e-12, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#acde06241652873c70e7d4d9523eab0b0">p_for_norm</a>=2, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a1db6c1588e7f4d11b3a723825809411a">dim_for_norm</a>=0, Optional[float] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae98b91d40567e6e9990304977809d997">max_grad_norm</a>=None, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2bfe032740a7146997162d125274fe9">grad_norm_p</a>=2.0, Optional[float] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa7a4a5ac2156c6877cbc0533601724e7">clip_grad_value</a>=None)</td></tr>
<tr class="separator:a8a01a254acf0c63cc4f7ed7990b0b222"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a369d886df0d6762cb825f10682607581"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a369d886df0d6762cb825f10682607581">load</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:a369d886df0d6762cb825f10682607581"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a369d886df0d6762cb825f10682607581">More...</a><br /></td></tr>
<tr class="separator:a369d886df0d6762cb825f10682607581"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae8e3441052d7b47fa91188a684e18659"><td class="memItemLeft" align="right" valign="top">np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae8e3441052d7b47fa91188a684e18659">policy</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, **kwargs)</td></tr>
<tr class="memdesc:ae8e3441052d7b47fa91188a684e18659"><td class="mdescLeft">&#160;</td><td class="mdescRight">The policy method to evaluate the agent.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae8e3441052d7b47fa91188a684e18659">More...</a><br /></td></tr>
<tr class="separator:ae8e3441052d7b47fa91188a684e18659"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad0e78685793f897be63b701cb75b6fc5"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ad0e78685793f897be63b701cb75b6fc5">save</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:ad0e78685793f897be63b701cb75b6fc5"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ad0e78685793f897be63b701cb75b6fc5">More...</a><br /></td></tr>
<tr class="separator:ad0e78685793f897be63b701cb75b6fc5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a5798a278a42acc9ce85d6d7797d889"><td class="memItemLeft" align="right" valign="top">np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4a5798a278a42acc9ce85d6d7797d889">train</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, Union[int, float] reward, Union[bool, int] done, **kwargs)</td></tr>
<tr class="memdesc:a4a5798a278a42acc9ce85d6d7797d889"><td class="mdescLeft">&#160;</td><td class="mdescRight">The train method to train the agent and underlying policy model.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4a5798a278a42acc9ce85d6d7797d889">More...</a><br /></td></tr>
<tr class="separator:a4a5798a278a42acc9ce85d6d7797d889"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Dict[str, Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">__getstate__</a> (self)</td></tr>
<tr class="memdesc:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To get the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">More...</a><br /></td></tr>
<tr class="separator:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">__init__</a> (self)</td></tr>
<tr class="memdesc:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The class initializer.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">More...</a><br /></td></tr>
<tr class="separator:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">__setstate__</a> (self, Dict[str, Any] state)</td></tr>
<tr class="memdesc:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To load the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">More...</a><br /></td></tr>
<tr class="separator:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">load</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">More...</a><br /></td></tr>
<tr class="separator:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">policy</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Policy method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">More...</a><br /></td></tr>
<tr class="separator:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">save</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">More...</a><br /></td></tr>
<tr class="separator:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">train</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Training method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">More...</a><br /></td></tr>
<tr class="separator:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Data Fields</h2></td></tr>
<tr class="memitem:a31e3fa147ea1d19490fb388ffe61aee3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a31e3fa147ea1d19490fb388ffe61aee3">action_space</a></td></tr>
<tr class="memdesc:a31e3fa147ea1d19490fb388ffe61aee3"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input number of actions.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a31e3fa147ea1d19490fb388ffe61aee3">More...</a><br /></td></tr>
<tr class="separator:a31e3fa147ea1d19490fb388ffe61aee3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aff3d0599e56caa4b5b0b36cccbf684b1"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aff3d0599e56caa4b5b0b36cccbf684b1">apply_norm</a></td></tr>
<tr class="memdesc:aff3d0599e56caa4b5b0b36cccbf684b1"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm</code> argument; indicating the normalisation to be used.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aff3d0599e56caa4b5b0b36cccbf684b1">More...</a><br /></td></tr>
<tr class="separator:aff3d0599e56caa4b5b0b36cccbf684b1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7e6b39931e27a1d569945fdb177d40cc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7e6b39931e27a1d569945fdb177d40cc">apply_norm_to</a></td></tr>
<tr class="memdesc:a7e6b39931e27a1d569945fdb177d40cc"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm_to</code> argument; indicating the quantity to normalise.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7e6b39931e27a1d569945fdb177d40cc">More...</a><br /></td></tr>
<tr class="separator:a7e6b39931e27a1d569945fdb177d40cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af19d87d26f908a6938f3b3ec1d25c70c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af19d87d26f908a6938f3b3ec1d25c70c">backup_frequency</a></td></tr>
<tr class="memdesc:af19d87d26f908a6938f3b3ec1d25c70c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input model backup frequency in terms of timesteps.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af19d87d26f908a6938f3b3ec1d25c70c">More...</a><br /></td></tr>
<tr class="separator:af19d87d26f908a6938f3b3ec1d25c70c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa7a4a5ac2156c6877cbc0533601724e7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa7a4a5ac2156c6877cbc0533601724e7">clip_grad_value</a></td></tr>
<tr class="memdesc:aa7a4a5ac2156c6877cbc0533601724e7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>clip_grad_value</code>; indicating the clipping range for gradients.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa7a4a5ac2156c6877cbc0533601724e7">More...</a><br /></td></tr>
<tr class="separator:aa7a4a5ac2156c6877cbc0533601724e7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a05e8b2d000bd94f5fb854516c84200e5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a05e8b2d000bd94f5fb854516c84200e5">device</a></td></tr>
<tr class="memdesc:a05e8b2d000bd94f5fb854516c84200e5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>device</code> argument; indicating the device name as device type class.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a05e8b2d000bd94f5fb854516c84200e5">More...</a><br /></td></tr>
<tr class="separator:a05e8b2d000bd94f5fb854516c84200e5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1db6c1588e7f4d11b3a723825809411a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a1db6c1588e7f4d11b3a723825809411a">dim_for_norm</a></td></tr>
<tr class="memdesc:a1db6c1588e7f4d11b3a723825809411a"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a1db6c1588e7f4d11b3a723825809411a">More...</a><br /></td></tr>
<tr class="separator:a1db6c1588e7f4d11b3a723825809411a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9244a2ba98485e2bedf6cee830ca1a22"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">distribution</a></td></tr>
<tr class="memdesc:a9244a2ba98485e2bedf6cee830ca1a22"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input distribution object.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">More...</a><br /></td></tr>
<tr class="separator:a9244a2ba98485e2bedf6cee830ca1a22"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6dc53028e64988f4a9442a047736acb8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a6dc53028e64988f4a9442a047736acb8">dtype</a></td></tr>
<tr class="memdesc:a6dc53028e64988f4a9442a047736acb8"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>dtype</code> argument; indicating the datatype class.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a6dc53028e64988f4a9442a047736acb8">More...</a><br /></td></tr>
<tr class="separator:a6dc53028e64988f4a9442a047736acb8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:affc2a52ddeb68b4c0a6c05793fa58261"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#affc2a52ddeb68b4c0a6c05793fa58261">entropy_coefficient</a></td></tr>
<tr class="memdesc:affc2a52ddeb68b4c0a6c05793fa58261"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input entropy coefficient.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#affc2a52ddeb68b4c0a6c05793fa58261">More...</a><br /></td></tr>
<tr class="separator:affc2a52ddeb68b4c0a6c05793fa58261"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a940d6caf441fdc4b732ef0185e2694e9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a940d6caf441fdc4b732ef0185e2694e9">eps_for_norm</a></td></tr>
<tr class="memdesc:a940d6caf441fdc4b732ef0185e2694e9"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a940d6caf441fdc4b732ef0185e2694e9">More...</a><br /></td></tr>
<tr class="separator:a940d6caf441fdc4b732ef0185e2694e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac76827b34c015165373ec76358b60dea"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac76827b34c015165373ec76358b60dea">exploration_tool</a></td></tr>
<tr class="memdesc:ac76827b34c015165373ec76358b60dea"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>exploration_tool</code>.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac76827b34c015165373ec76358b60dea">More...</a><br /></td></tr>
<tr class="separator:ac76827b34c015165373ec76358b60dea"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab09c920ad70ce8b4cd3fa485b2f1bd55"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab09c920ad70ce8b4cd3fa485b2f1bd55">gamma</a></td></tr>
<tr class="memdesc:ab09c920ad70ce8b4cd3fa485b2f1bd55"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input discounting factor.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab09c920ad70ce8b4cd3fa485b2f1bd55">More...</a><br /></td></tr>
<tr class="separator:ab09c920ad70ce8b4cd3fa485b2f1bd55"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a724bcb1e4d32cd2cd880911eed207af6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a724bcb1e4d32cd2cd880911eed207af6">grad_accumulation_rounds</a></td></tr>
<tr class="memdesc:a724bcb1e4d32cd2cd880911eed207af6"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>grad_accumulation_rounds</code>.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a724bcb1e4d32cd2cd880911eed207af6">More...</a><br /></td></tr>
<tr class="separator:a724bcb1e4d32cd2cd880911eed207af6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2bfe032740a7146997162d125274fe9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2bfe032740a7146997162d125274fe9">grad_norm_p</a></td></tr>
<tr class="memdesc:aa2bfe032740a7146997162d125274fe9"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2bfe032740a7146997162d125274fe9">More...</a><br /></td></tr>
<tr class="separator:aa2bfe032740a7146997162d125274fe9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4b605080b32f1199d314ded5efa4230a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4b605080b32f1199d314ded5efa4230a">is_continuous_action_space</a></td></tr>
<tr class="memdesc:a4b605080b32f1199d314ded5efa4230a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Flag indicating if action space is continuous or discrete.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4b605080b32f1199d314ded5efa4230a">More...</a><br /></td></tr>
<tr class="separator:a4b605080b32f1199d314ded5efa4230a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af5485148744d68c770b872e62361ca10"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af5485148744d68c770b872e62361ca10">loss_function</a></td></tr>
<tr class="memdesc:af5485148744d68c770b872e62361ca10"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input loss function.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af5485148744d68c770b872e62361ca10">More...</a><br /></td></tr>
<tr class="separator:af5485148744d68c770b872e62361ca10"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a081dcbd51017df1d26abbf15821089d7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a081dcbd51017df1d26abbf15821089d7">lr_scheduler</a></td></tr>
<tr class="memdesc:a081dcbd51017df1d26abbf15821089d7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optional LR Scheduler (this can be None).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a081dcbd51017df1d26abbf15821089d7">More...</a><br /></td></tr>
<tr class="separator:a081dcbd51017df1d26abbf15821089d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2238587b60c04bf2a7e83e497a030b1e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2238587b60c04bf2a7e83e497a030b1e">lr_threshold</a></td></tr>
<tr class="memdesc:a2238587b60c04bf2a7e83e497a030b1e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input LR Threshold.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2238587b60c04bf2a7e83e497a030b1e">More...</a><br /></td></tr>
<tr class="separator:a2238587b60c04bf2a7e83e497a030b1e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae98b91d40567e6e9990304977809d997"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae98b91d40567e6e9990304977809d997">max_grad_norm</a></td></tr>
<tr class="memdesc:ae98b91d40567e6e9990304977809d997"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae98b91d40567e6e9990304977809d997">More...</a><br /></td></tr>
<tr class="separator:ae98b91d40567e6e9990304977809d997"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2f798b74bd64640ef35da8f672f3e95"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2f798b74bd64640ef35da8f672f3e95">optimizer</a></td></tr>
<tr class="memdesc:aa2f798b74bd64640ef35da8f672f3e95"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optimizer wrapped with policy_model parameters.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2f798b74bd64640ef35da8f672f3e95">More...</a><br /></td></tr>
<tr class="separator:aa2f798b74bd64640ef35da8f672f3e95"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acde06241652873c70e7d4d9523eab0b0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#acde06241652873c70e7d4d9523eab0b0">p_for_norm</a></td></tr>
<tr class="memdesc:acde06241652873c70e7d4d9523eab0b0"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#acde06241652873c70e7d4d9523eab0b0">More...</a><br /></td></tr>
<tr class="separator:acde06241652873c70e7d4d9523eab0b0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c0e7b3a8cc0a96cebe026d304c5590c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3c0e7b3a8cc0a96cebe026d304c5590c">policy_model</a></td></tr>
<tr class="memdesc:a3c0e7b3a8cc0a96cebe026d304c5590c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input policy model moved to desired device.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3c0e7b3a8cc0a96cebe026d304c5590c">More...</a><br /></td></tr>
<tr class="separator:a3c0e7b3a8cc0a96cebe026d304c5590c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9e282f966f2bd18c24ffba49ceefd00b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9e282f966f2bd18c24ffba49ceefd00b">rollout_accumulation_size</a></td></tr>
<tr class="memdesc:a9e282f966f2bd18c24ffba49ceefd00b"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>rollout_accumulation_size</code>.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9e282f966f2bd18c24ffba49ceefd00b">More...</a><br /></td></tr>
<tr class="separator:a9e282f966f2bd18c24ffba49ceefd00b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a00c1fb74f19174eecd6fcd03dc0b9843"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a00c1fb74f19174eecd6fcd03dc0b9843">save_path</a></td></tr>
<tr class="memdesc:a00c1fb74f19174eecd6fcd03dc0b9843"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input save path for backing up agent models.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a00c1fb74f19174eecd6fcd03dc0b9843">More...</a><br /></td></tr>
<tr class="separator:a00c1fb74f19174eecd6fcd03dc0b9843"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a073e3f0455059d575ce1bc9a28b3eee5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a073e3f0455059d575ce1bc9a28b3eee5">state_value_coefficient</a></td></tr>
<tr class="memdesc:a073e3f0455059d575ce1bc9a28b3eee5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input state value coefficient.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a073e3f0455059d575ce1bc9a28b3eee5">More...</a><br /></td></tr>
<tr class="separator:a073e3f0455059d575ce1bc9a28b3eee5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a42a2685035dad0c95918126682a0d4ff"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a42a2685035dad0c95918126682a0d4ff">step_counter</a></td></tr>
<tr class="memdesc:a42a2685035dad0c95918126682a0d4ff"><td class="mdescLeft">&#160;</td><td class="mdescRight">The step counter; counting the total timesteps done so far.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a42a2685035dad0c95918126682a0d4ff">More...</a><br /></td></tr>
<tr class="separator:a42a2685035dad0c95918126682a0d4ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Data Fields inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4104e76aed3a37b4df4ef1069383aee8">gamma</a></td></tr>
<tr class="memdesc:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The default discounting factor for agents.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4104e76aed3a37b4df4ef1069383aee8">More...</a><br /></td></tr>
<tr class="separator:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">loss</a></td></tr>
<tr class="memdesc:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of losses accumulated after each backward call.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">More...</a><br /></td></tr>
<tr class="separator:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">save_path</a></td></tr>
<tr class="memdesc:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The path to save agent states and models.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">More...</a><br /></td></tr>
<tr class="separator:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-methods" name="pri-methods"></a>
Private Member Functions</h2></td></tr>
<tr class="memitem:aeb08cdf97dfeb1f6c1e0d5aee3e55e10"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aeb08cdf97dfeb1f6c1e0d5aee3e55e10">_call_to_add_noise_to_actions</a> (self, pytorch.Tensor action)</td></tr>
<tr class="separator:aeb08cdf97dfeb1f6c1e0d5aee3e55e10"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a491be8c30e2f58c229eccff0a15e9006"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a491be8c30e2f58c229eccff0a15e9006">_call_to_reset_exploration_tool</a> (self, Union[bool, int] done)</td></tr>
<tr class="separator:a491be8c30e2f58c229eccff0a15e9006"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:addb42dda96dcb4a5114ad63ecc4b1ff0"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#addb42dda96dcb4a5114ad63ecc4b1ff0">_call_to_run_optimizer</a> (self, Union[bool, int] done)</td></tr>
<tr class="memdesc:addb42dda96dcb4a5114ad63ecc4b1ff0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method to train the policy model.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#addb42dda96dcb4a5114ad63ecc4b1ff0">More...</a><br /></td></tr>
<tr class="separator:addb42dda96dcb4a5114ad63ecc4b1ff0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a79da689b8d6edfd4ae0f3128ac130d57"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a79da689b8d6edfd4ae0f3128ac130d57">_call_to_save</a> (self)</td></tr>
<tr class="memdesc:a79da689b8d6edfd4ae0f3128ac130d57"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method calling the save method when required.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a79da689b8d6edfd4ae0f3128ac130d57">More...</a><br /></td></tr>
<tr class="separator:a79da689b8d6edfd4ae0f3128ac130d57"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8055e40213b37ef9bcf3c0c750c3af73"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a8055e40213b37ef9bcf3c0c750c3af73">_clear</a> (self)</td></tr>
<tr class="memdesc:a8055e40213b37ef9bcf3c0c750c3af73"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to clear the lists of rewards, action_log_probs and state_values.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a8055e40213b37ef9bcf3c0c750c3af73">More...</a><br /></td></tr>
<tr class="separator:a8055e40213b37ef9bcf3c0c750c3af73"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac18f59ffac99d3f57916db692df9adf0"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac18f59ffac99d3f57916db692df9adf0">_compute_advantage</a> (self, pytorch.Tensor returns, pytorch.Tensor state_current_values)</td></tr>
<tr class="memdesc:ac18f59ffac99d3f57916db692df9adf0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the advantage from returns and state values.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac18f59ffac99d3f57916db692df9adf0">More...</a><br /></td></tr>
<tr class="separator:ac18f59ffac99d3f57916db692df9adf0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0dddd26f1bfa9e729fb771a28b2cd4bb"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0dddd26f1bfa9e729fb771a28b2cd4bb">_compute_loss</a> (self)</td></tr>
<tr class="memdesc:a0dddd26f1bfa9e729fb771a28b2cd4bb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method to compute total loss (from actor and critic).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0dddd26f1bfa9e729fb771a28b2cd4bb">More...</a><br /></td></tr>
<tr class="separator:a0dddd26f1bfa9e729fb771a28b2cd4bb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a97bd95a17383eabb3d8cb6bc89ff1a45"><td class="memItemLeft" align="right" valign="top">pytorch_distributions.Distribution&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a97bd95a17383eabb3d8cb6bc89ff1a45">_create_action_distribution</a> (self, Union[List[pytorch.Tensor], pytorch.Tensor] action_values)</td></tr>
<tr class="memdesc:a97bd95a17383eabb3d8cb6bc89ff1a45"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected static method to create distributions from action logits.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a97bd95a17383eabb3d8cb6bc89ff1a45">More...</a><br /></td></tr>
<tr class="separator:a97bd95a17383eabb3d8cb6bc89ff1a45"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a86d3550730c8f371bbfdc527b21d589d"><td class="memItemLeft" align="right" valign="top">pytorch_distributions.Normal&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a86d3550730c8f371bbfdc527b21d589d">_create_noise_distribution</a> (self)</td></tr>
<tr class="memdesc:a86d3550730c8f371bbfdc527b21d589d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Create the standard Gaussian Distribution object for adding noise to sampled actions.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a86d3550730c8f371bbfdc527b21d589d">More...</a><br /></td></tr>
<tr class="separator:a86d3550730c8f371bbfdc527b21d589d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a84eb3266451b8eb5210187c2f02f30a6"><td class="memItemLeft" align="right" valign="top">pytorch.Size&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a84eb3266451b8eb5210187c2f02f30a6">_get_action_sample_shape_for_continuous</a> (self)</td></tr>
<tr class="memdesc:a84eb3266451b8eb5210187c2f02f30a6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the action sample shape to be sampled from continuous distribution.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a84eb3266451b8eb5210187c2f02f30a6">More...</a><br /></td></tr>
<tr class="separator:a84eb3266451b8eb5210187c2f02f30a6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afef85f43b81e3743e774ca8b47265b72"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#afef85f43b81e3743e774ca8b47265b72">_grad_mean_reduction</a> (self)</td></tr>
<tr class="memdesc:afef85f43b81e3743e774ca8b47265b72"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs mean reduction and assigns the policy model's parameter the mean reduced gradients.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#afef85f43b81e3743e774ca8b47265b72">More...</a><br /></td></tr>
<tr class="separator:afef85f43b81e3743e774ca8b47265b72"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeffc0ad06fdbb9f54f81a9642623a215"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aeffc0ad06fdbb9f54f81a9642623a215">_run_optimizer</a> (self, pytorch.Tensor <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">loss</a>)</td></tr>
<tr class="memdesc:aeffc0ad06fdbb9f54f81a9642623a215"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to train the model or accumulate the gradients for training.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aeffc0ad06fdbb9f54f81a9642623a215">More...</a><br /></td></tr>
<tr class="separator:aeffc0ad06fdbb9f54f81a9642623a215"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-attribs" name="pri-attribs"></a>
Private Attributes</h2></td></tr>
<tr class="memitem:a7cbd5c618c84ad7b628878b5ff7f4f1c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7cbd5c618c84ad7b628878b5ff7f4f1c">_distributed_grad_reduce_method</a></td></tr>
<tr class="memdesc:a7cbd5c618c84ad7b628878b5ff7f4f1c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The callable method for gradient reduction in distributed environment.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7cbd5c618c84ad7b628878b5ff7f4f1c">More...</a><br /></td></tr>
<tr class="separator:a7cbd5c618c84ad7b628878b5ff7f4f1c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a98c14131b7db73bdf7ec4894012cef66"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a98c14131b7db73bdf7ec4894012cef66">_distributed_param_scatter_method</a></td></tr>
<tr class="memdesc:a98c14131b7db73bdf7ec4894012cef66"><td class="mdescLeft">&#160;</td><td class="mdescRight">The callable method for parameter scattering in distributed environment.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a98c14131b7db73bdf7ec4894012cef66">More...</a><br /></td></tr>
<tr class="separator:a98c14131b7db73bdf7ec4894012cef66"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aea7f62cfe7b129431fa4bf3598463708"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aea7f62cfe7b129431fa4bf3598463708">_grad_accumulator</a></td></tr>
<tr class="memdesc:aea7f62cfe7b129431fa4bf3598463708"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of gradients from each backward call.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aea7f62cfe7b129431fa4bf3598463708">More...</a><br /></td></tr>
<tr class="separator:aea7f62cfe7b129431fa4bf3598463708"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a760ac2fe8d82171dd05d79717b5d72b4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a760ac2fe8d82171dd05d79717b5d72b4">_master_process_rank</a></td></tr>
<tr class="memdesc:a760ac2fe8d82171dd05d79717b5d72b4"><td class="mdescLeft">&#160;</td><td class="mdescRight">The master process id for multi-agents.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a760ac2fe8d82171dd05d79717b5d72b4">More...</a><br /></td></tr>
<tr class="separator:a760ac2fe8d82171dd05d79717b5d72b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8ad137357b10ed57e1e47523b9f1b19"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af8ad137357b10ed57e1e47523b9f1b19">_normalization</a></td></tr>
<tr class="memdesc:af8ad137357b10ed57e1e47523b9f1b19"><td class="mdescLeft">&#160;</td><td class="mdescRight">The normalisation tool to be used for agent.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af8ad137357b10ed57e1e47523b9f1b19">More...</a><br /></td></tr>
<tr class="separator:af8ad137357b10ed57e1e47523b9f1b19"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3e0fca95ea870ecb20e1946a8bbe909b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3e0fca95ea870ecb20e1946a8bbe909b">_process_rank</a></td></tr>
<tr class="memdesc:a3e0fca95ea870ecb20e1946a8bbe909b"><td class="mdescLeft">&#160;</td><td class="mdescRight">The process rank for multi-agents.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3e0fca95ea870ecb20e1946a8bbe909b">More...</a><br /></td></tr>
<tr class="separator:a3e0fca95ea870ecb20e1946a8bbe909b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a34547eb17d04d4f5a476a2543f5a97f4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a34547eb17d04d4f5a476a2543f5a97f4">_rollout_buffer</a></td></tr>
<tr class="memdesc:a34547eb17d04d4f5a476a2543f5a97f4"><td class="mdescLeft">&#160;</td><td class="mdescRight">The rollout buffer to be used for agent to store necessary outputs.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a34547eb17d04d4f5a476a2543f5a97f4">More...</a><br /></td></tr>
<tr class="separator:a34547eb17d04d4f5a476a2543f5a97f4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0435f3555d4c757742f2fa5c6b66e56"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa0435f3555d4c757742f2fa5c6b66e56">_world_size</a></td></tr>
<tr class="memdesc:aa0435f3555d4c757742f2fa5c6b66e56"><td class="mdescLeft">&#160;</td><td class="mdescRight">The world size for multi-agents.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa0435f3555d4c757742f2fa5c6b66e56">More...</a><br /></td></tr>
<tr class="separator:aa0435f3555d4c757742f2fa5c6b66e56"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >The <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html" title="The ActorCriticAgent is the base class for actor-critic methods.">ActorCriticAgent</a> is the base class for actor-critic methods. </p>
<p >This class implements basic methods and abstract functions for actor-critic methods. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a8a01a254acf0c63cc4f7ed7990b0b222" name="a8a01a254acf0c63cc4f7ed7990b0b222"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8a01a254acf0c63cc4f7ed7990b0b222">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.nn.Module&#160;</td>
          <td class="paramname"><em>policy_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.optim.Optimizer&#160;</td>
          <td class="paramname"><em>optimizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[<a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_l_r_scheduler.html">LRScheduler</a>, None]&#160;</td>
          <td class="paramname"><em>lr_scheduler</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_loss_function.html">LossFunction</a>&#160;</td>
          <td class="paramname"><em>loss_function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Type[pytorch_distributions.Distribution]&#160;</td>
          <td class="paramname"><em>distribution</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>entropy_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>state_value_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>lr_threshold</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, Tuple[int, Union[List[int], None]]]&#160;</td>
          <td class="paramname"><em>action_space</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>backup_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>save_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, None] &#160;</td>
          <td class="paramname"><em>rollout_accumulation_size</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>grad_accumulation_rounds</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[<a class="el" href="classrlpack_1_1exploration_1_1utils_1_1exploration_1_1_exploration.html">Exploration</a>, None] &#160;</td>
          <td class="paramname"><em>exploration_tool</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>device</em> = <code>&quot;cpu&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>dtype</em> = <code>&quot;float32&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, str] &#160;</td>
          <td class="paramname"><em>apply_norm</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, List[str]] &#160;</td>
          <td class="paramname"><em>apply_norm_to</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>eps_for_norm</em> = <code>5e-12</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>p_for_norm</em> = <code>2</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>dim_for_norm</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>max_grad_norm</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>grad_norm_p</em> = <code>2.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>clip_grad_value</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">policy_model</td><td><em>pytorch.nn.Module</em>: The policy model to be used. Policy model must return a tuple of action logits and state values. </td></tr>
    <tr><td class="paramname">optimizer</td><td>pytorch.optim.Optimizer: The optimizer to be used for policy model. Optimizer must be initialized and wrapped with policy model parameters. </td></tr>
    <tr><td class="paramname">lr_scheduler</td><td>Union[LRScheduler, None]: The LR Scheduler to be used to decay the learning rate. LR Scheduler must be initialized and wrapped with passed optimizer. </td></tr>
    <tr><td class="paramname">loss_function</td><td>LossFunction: A PyTorch loss function. </td></tr>
    <tr><td class="paramname">distribution</td><td>: Type[pytorch_distributions.Distribution]: The distribution of PyTorch to be used to sampled actions in action space. (See <code>action_space</code>). </td></tr>
    <tr><td class="paramname">gamma</td><td>float: The discounting factor for rewards. </td></tr>
    <tr><td class="paramname">entropy_coefficient</td><td>float: The coefficient to be used for entropy in policy loss computation. </td></tr>
    <tr><td class="paramname">state_value_coefficient</td><td>float: The coefficient to be used for state value in final loss computation. </td></tr>
    <tr><td class="paramname">lr_threshold</td><td>float: The threshold LR which once reached LR scheduler is not called further. </td></tr>
    <tr><td class="paramname">action_space</td><td>Union[int, Tuple[int, Union[List[int], None]]]: The action space of the environment.<ul>
<li>If discrete action set is used, number of actions can be passed.</li>
<li>If continuous action space is used, a list must be passed with first element representing the output features from model, second element representing the shape of action to be sampled. Second element can be an empty list, if you wish to sample the default no. of samples. </li>
</ul>
</td></tr>
    <tr><td class="paramname">backup_frequency</td><td>int: The timesteps after which policy model, optimizer states and lr scheduler states are backed up. </td></tr>
    <tr><td class="paramname">save_path</td><td>str: The path where policy model, optimizer states and lr scheduler states are to be saved. </td></tr>
    <tr><td class="paramname">rollout_accumulation_size</td><td>Union[int, None]: The size of rollout buffer before performing optimizer step. Whole rollout buffer is used to fit the policy model and is cleared. By default, after every episode. Default: None. </td></tr>
    <tr><td class="paramname">grad_accumulation_rounds</td><td>int: The number of rounds until which gradients are to be accumulated before performing calling optimizer step. Gradients are mean reduced for grad_accumulation_rounds &gt; 1. Default: 1. </td></tr>
    <tr><td class="paramname">exploration_tool</td><td>Union[Exploration, None]: Exploration tool to be used to explore the environment. These tools can be found in <code><a class="el" href="namespacerlpack_1_1exploration.html" title="This package implements the exploration tools for RLPack to explore the environment.">rlpack.exploration</a></code>. </td></tr>
    <tr><td class="paramname">device</td><td>str: The device on which models are run. Default: "cpu". </td></tr>
    <tr><td class="paramname">dtype</td><td>str: The datatype for model parameters. Default: "float32" </td></tr>
    <tr><td class="paramname">apply_norm</td><td>Union[int, str]: The code to select the normalization procedure to be applied on selected quantities; selected by <code>apply_norm_to</code>: see below)). Direct string can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1 </td></tr>
    <tr><td class="paramname">apply_norm_to</td><td>Union[int, List[str]]: The code to select the quantity to which normalization is to be applied. Direct list of quantities can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1. </td></tr>
    <tr><td class="paramname">eps_for_norm</td><td>float: Epsilon value for normalization; for numeric stability. For min-max normalization and standardized normalization. Default: 5e-12. </td></tr>
    <tr><td class="paramname">p_for_norm</td><td>int: The p value for p-normalization. Default: 2; L2 Norm. </td></tr>
    <tr><td class="paramname">dim_for_norm</td><td>int: The dimension across which normalization is to be performed. Default: 0. </td></tr>
    <tr><td class="paramname">max_grad_norm</td><td>Optional[float]: The max norm for gradients for gradient clipping. Default: None </td></tr>
    <tr><td class="paramname">grad_norm_p</td><td>float: The p-value for p-normalization of gradients. Default: 2.0 </td></tr>
    <tr><td class="paramname">clip_grad_value</td><td>Optional[float]: The gradient value for clipping gradients by value. Default: None</td></tr>
  </table>
  </dd>
</dl>
<p><b>Notes</b></p>
<p >The values accepted for <code>apply_norm</code> are: -</p><ul>
<li>No Normalization: -1; <code>"none"</code></li>
<li>Min-Max Normalization: 0; <code>"min_max"</code></li>
<li>Standardization: 1; <code>"standardize"</code></li>
<li>P-Normalization: 2; <code>"p_norm"</code></li>
</ul>
<p >The value accepted for <code>apply_norm_to</code> are as follows and must be passed in a list:</p><ul>
<li><code>"none"</code>: -1; Don't apply normalization to any quantity.</li>
<li><code>"states"</code>: 0; Apply normalization to states.</li>
<li><code>"state_values"</code>: 1; Apply normalization to state values.</li>
<li><code>"rewards"</code>: 2; Apply normalization to rewards.</li>
<li><code>"returns"</code>: 3; Apply normalization to rewards.</li>
<li><code>"td"</code>: 4; Apply normalization for TD values.</li>
<li><code>"advantage"</code>: 5; Apply normalization to advantage values</li>
</ul>
<p >If a valid <code>max_norm_grad</code> is passed, then gradient clipping takes place else gradient clipping step is skipped. If <code>max_norm_grad</code> value was invalid, error will be raised from PyTorch. If a valid <code>clip_grad_value</code> is passed, then gradients will be clipped by value. If <code>clip_grad_value</code> value was invalid, error will be raised from PyTorch. </p>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">rlpack.utils.base.agent.Agent</a>.</p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a053aa4d80326d086eb58b34cba99a27d">rlpack.actor_critic.a2c.A2C</a>, <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#ae6ef32612103c3a672e2eb31697d01ba">rlpack.actor_critic.a3c.A3C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html#a7709778fc781c0d452641adbf693386a">rlpack.actor_critic.ac.AC</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="aeb08cdf97dfeb1f6c1e0d5aee3e55e10" name="aeb08cdf97dfeb1f6c1e0d5aee3e55e10"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeb08cdf97dfeb1f6c1e0d5aee3e55e10">&#9670;&#160;</a></span>_call_to_add_noise_to_actions()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._call_to_add_noise_to_actions </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>action</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a491be8c30e2f58c229eccff0a15e9006" name="a491be8c30e2f58c229eccff0a15e9006"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a491be8c30e2f58c229eccff0a15e9006">&#9670;&#160;</a></span>_call_to_reset_exploration_tool()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._call_to_reset_exploration_tool </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="addb42dda96dcb4a5114ad63ecc4b1ff0" name="addb42dda96dcb4a5114ad63ecc4b1ff0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#addb42dda96dcb4a5114ad63ecc4b1ff0">&#9670;&#160;</a></span>_call_to_run_optimizer()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._call_to_run_optimizer </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method to train the policy model. </p>
<p >If done flag is True, will compute the loss and run the optimizer. This method is meant to periodically check if episode hsa been terminated or and train policy models if episode has terminated. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">done</td><td>Union[bool, int]: Flag indicating if episode has terminated or not </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a79da689b8d6edfd4ae0f3128ac130d57" name="a79da689b8d6edfd4ae0f3128ac130d57"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a79da689b8d6edfd4ae0f3128ac130d57">&#9670;&#160;</a></span>_call_to_save()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._call_to_save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method calling the save method when required. </p>
<p >This method is to be overriden. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a487c58b125fb7cd2e14972552bdf5ea6">rlpack.actor_critic.a2c.A2C</a>, <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#ac2f6faa17ef5e68d665f2e085636b5d2">rlpack.actor_critic.a3c.A3C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1ac_1_1_a_c.html#a007d495e6f8e3b960d7ff5c5f2ee2a15">rlpack.actor_critic.ac.AC</a>.</p>

</div>
</div>
<a id="a8055e40213b37ef9bcf3c0c750c3af73" name="a8055e40213b37ef9bcf3c0c750c3af73"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8055e40213b37ef9bcf3c0c750c3af73">&#9670;&#160;</a></span>_clear()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._clear </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to clear the lists of rewards, action_log_probs and state_values. </p>

</div>
</div>
<a id="ac18f59ffac99d3f57916db692df9adf0" name="ac18f59ffac99d3f57916db692df9adf0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac18f59ffac99d3f57916db692df9adf0">&#9670;&#160;</a></span>_compute_advantage()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._compute_advantage </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>returns</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor
    &#160;</td>
          <td class="paramname"><em>state_current_values</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the advantage from returns and state values. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">returns</td><td>pytorch.Tensor: The discounted returns; computed from _compute_returns method </td></tr>
    <tr><td class="paramname">state_current_values</td><td>pytorch.Tensor: The corresponding state values </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The advantage for the given returns and state values </dd></dl>

</div>
</div>
<a id="a0dddd26f1bfa9e729fb771a28b2cd4bb" name="a0dddd26f1bfa9e729fb771a28b2cd4bb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0dddd26f1bfa9e729fb771a28b2cd4bb">&#9670;&#160;</a></span>_compute_loss()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._compute_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method to compute total loss (from actor and critic). </p>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The loss tensor. </dd></dl>

</div>
</div>
<a id="a97bd95a17383eabb3d8cb6bc89ff1a45" name="a97bd95a17383eabb3d8cb6bc89ff1a45"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a97bd95a17383eabb3d8cb6bc89ff1a45">&#9670;&#160;</a></span>_create_action_distribution()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch_distributions.Distribution rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._create_action_distribution </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[List[pytorch.Tensor], pytorch.Tensor]&#160;</td>
          <td class="paramname"><em>action_values</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected static method to create distributions from action logits. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">action_values</td><td>Union[List[pytorch.Tensor], pytorch.Tensor]: The action values from policy model </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Distribution: A Distribution object initialized with given action logits </dd></dl>

</div>
</div>
<a id="a86d3550730c8f371bbfdc527b21d589d" name="a86d3550730c8f371bbfdc527b21d589d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a86d3550730c8f371bbfdc527b21d589d">&#9670;&#160;</a></span>_create_noise_distribution()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch_distributions.Normal rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._create_noise_distribution </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Create the standard Gaussian Distribution object for adding noise to sampled actions. </p>

</div>
</div>
<a id="a84eb3266451b8eb5210187c2f02f30a6" name="a84eb3266451b8eb5210187c2f02f30a6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a84eb3266451b8eb5210187c2f02f30a6">&#9670;&#160;</a></span>_get_action_sample_shape_for_continuous()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Size rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._get_action_sample_shape_for_continuous </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets the action sample shape to be sampled from continuous distribution. </p>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Size: Sample shape of to-be sampled tensor from continuous distribution </dd></dl>

</div>
</div>
<a id="afef85f43b81e3743e774ca8b47265b72" name="afef85f43b81e3743e774ca8b47265b72"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afef85f43b81e3743e774ca8b47265b72">&#9670;&#160;</a></span>_grad_mean_reduction()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._grad_mean_reduction </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Performs mean reduction and assigns the policy model's parameter the mean reduced gradients. </p>

</div>
</div>
<a id="aeffc0ad06fdbb9f54f81a9642623a215" name="aeffc0ad06fdbb9f54f81a9642623a215"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeffc0ad06fdbb9f54f81a9642623a215">&#9670;&#160;</a></span>_run_optimizer()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._run_optimizer </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>loss</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to train the model or accumulate the gradients for training. </p>
<ul>
<li>If grad_accumulation_rounds is passed as 1 (default), model is trained each time the method is called.</li>
<li>If grad_accumulation_rounds &gt; 1, the gradients are accumulated in grad_accumulator and model is trained via _train_models method. <dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">loss</td><td>pytorch.Tensor: The loss scalar tensor </td></tr>
  </table>
  </dd>
</dl>
</li>
</ul>

</div>
</div>
<a id="a369d886df0d6762cb825f10682607581" name="a369d886df0d6762cb825f10682607581"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a369d886df0d6762cb825f10682607581">&#9670;&#160;</a></span>load()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.load </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to load the best model by a custom suffix supplied for evaluation. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="ae8e3441052d7b47fa91188a684e18659" name="ae8e3441052d7b47fa91188a684e18659"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae8e3441052d7b47fa91188a684e18659">&#9670;&#160;</a></span>policy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.ndarray rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.policy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The policy method to evaluate the agent. </p>
<p >This runs in pure inference mode. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned from gym environment </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>np.ndarray: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="ad0e78685793f897be63b701cb75b6fc5" name="ad0e78685793f897be63b701cb75b6fc5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad0e78685793f897be63b701cb75b6fc5">&#9670;&#160;</a></span>save()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<p >agent_states includes current memory and epsilon values in a dictionary. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to save best model by a custom suffix supplied during a train run. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="a4a5798a278a42acc9ce85d6d7797d889" name="a4a5798a278a42acc9ce85d6d7797d889"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4a5798a278a42acc9ce85d6d7797d889">&#9670;&#160;</a></span>train()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.ndarray rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.train </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, float]&#160;</td>
          <td class="paramname"><em>reward</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The train method to train the agent and underlying policy model. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned </td></tr>
    <tr><td class="paramname">reward</td><td>Union[int, float]: The reward returned from previous action </td></tr>
    <tr><td class="paramname">done</td><td>Union[bool, int]: Flag indicating if episode has terminated or not </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>np.ndarray: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<h2 class="groupheader">Field Documentation</h2>
<a id="a7cbd5c618c84ad7b628878b5ff7f4f1c" name="a7cbd5c618c84ad7b628878b5ff7f4f1c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7cbd5c618c84ad7b628878b5ff7f4f1c">&#9670;&#160;</a></span>_distributed_grad_reduce_method</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._distributed_grad_reduce_method</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The callable method for gradient reduction in distributed environment. </p>

</div>
</div>
<a id="a98c14131b7db73bdf7ec4894012cef66" name="a98c14131b7db73bdf7ec4894012cef66"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a98c14131b7db73bdf7ec4894012cef66">&#9670;&#160;</a></span>_distributed_param_scatter_method</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._distributed_param_scatter_method</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The callable method for parameter scattering in distributed environment. </p>

</div>
</div>
<a id="aea7f62cfe7b129431fa4bf3598463708" name="aea7f62cfe7b129431fa4bf3598463708"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aea7f62cfe7b129431fa4bf3598463708">&#9670;&#160;</a></span>_grad_accumulator</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._grad_accumulator</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The list of gradients from each backward call. </p>
<p >This is only used when boostrap_rounds &gt; 1 and is cleared after each boostrap round. The <a class="el" href="classrlpack_1_1___c_1_1grad__accumulator_1_1_grad_accumulator.html" title="This class provides the python interface to C_GradAccumulator, the C++ class which performs heavier w...">rlpack._C.grad_accumulator.GradAccumulator</a> object for grad accumulation. </p>

</div>
</div>
<a id="a760ac2fe8d82171dd05d79717b5d72b4" name="a760ac2fe8d82171dd05d79717b5d72b4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a760ac2fe8d82171dd05d79717b5d72b4">&#9670;&#160;</a></span>_master_process_rank</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._master_process_rank</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The master process id for multi-agents. </p>
<p >Is set to 0 as a standard. </p>

</div>
</div>
<a id="af8ad137357b10ed57e1e47523b9f1b19" name="af8ad137357b10ed57e1e47523b9f1b19"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af8ad137357b10ed57e1e47523b9f1b19">&#9670;&#160;</a></span>_normalization</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._normalization</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The normalisation tool to be used for agent. </p>
<p >An instance of <a class="el" href="classrlpack_1_1utils_1_1normalization_1_1_normalization.html" title="Normalization class providing methods for normalization techniques.">rlpack.utils.normalization.Normalization</a>. </p>

</div>
</div>
<a id="a3e0fca95ea870ecb20e1946a8bbe909b" name="a3e0fca95ea870ecb20e1946a8bbe909b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3e0fca95ea870ecb20e1946a8bbe909b">&#9670;&#160;</a></span>_process_rank</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._process_rank</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The process rank for multi-agents. </p>
<p >For uni-agents, will be None. </p>

</div>
</div>
<a id="a34547eb17d04d4f5a476a2543f5a97f4" name="a34547eb17d04d4f5a476a2543f5a97f4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a34547eb17d04d4f5a476a2543f5a97f4">&#9670;&#160;</a></span>_rollout_buffer</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._rollout_buffer</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The rollout buffer to be used for agent to store necessary outputs. </p>
<p >An instance of <a class="el" href="classrlpack_1_1___c_1_1rollout__buffer_1_1_rollout_buffer.html">rlpack._C.rollout_buffer.RolloutBuffer</a> </p>

</div>
</div>
<a id="aa0435f3555d4c757742f2fa5c6b66e56" name="aa0435f3555d4c757742f2fa5c6b66e56"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa0435f3555d4c757742f2fa5c6b66e56">&#9670;&#160;</a></span>_world_size</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._world_size</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The world size for multi-agents. </p>
<p >For uni-agents, will be None. </p>

</div>
</div>
<a id="a31e3fa147ea1d19490fb388ffe61aee3" name="a31e3fa147ea1d19490fb388ffe61aee3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a31e3fa147ea1d19490fb388ffe61aee3">&#9670;&#160;</a></span>action_space</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.action_space</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input number of actions. </p>

</div>
</div>
<a id="aff3d0599e56caa4b5b0b36cccbf684b1" name="aff3d0599e56caa4b5b0b36cccbf684b1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aff3d0599e56caa4b5b0b36cccbf684b1">&#9670;&#160;</a></span>apply_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.apply_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm</code> argument; indicating the normalisation to be used. </p>

</div>
</div>
<a id="a7e6b39931e27a1d569945fdb177d40cc" name="a7e6b39931e27a1d569945fdb177d40cc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7e6b39931e27a1d569945fdb177d40cc">&#9670;&#160;</a></span>apply_norm_to</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.apply_norm_to</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm_to</code> argument; indicating the quantity to normalise. </p>

</div>
</div>
<a id="af19d87d26f908a6938f3b3ec1d25c70c" name="af19d87d26f908a6938f3b3ec1d25c70c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af19d87d26f908a6938f3b3ec1d25c70c">&#9670;&#160;</a></span>backup_frequency</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.backup_frequency</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input model backup frequency in terms of timesteps. </p>

</div>
</div>
<a id="aa7a4a5ac2156c6877cbc0533601724e7" name="aa7a4a5ac2156c6877cbc0533601724e7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa7a4a5ac2156c6877cbc0533601724e7">&#9670;&#160;</a></span>clip_grad_value</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.clip_grad_value</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>clip_grad_value</code>; indicating the clipping range for gradients. </p>

</div>
</div>
<a id="a05e8b2d000bd94f5fb854516c84200e5" name="a05e8b2d000bd94f5fb854516c84200e5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a05e8b2d000bd94f5fb854516c84200e5">&#9670;&#160;</a></span>device</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.device</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>device</code> argument; indicating the device name as device type class. </p>

</div>
</div>
<a id="a1db6c1588e7f4d11b3a723825809411a" name="a1db6c1588e7f4d11b3a723825809411a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1db6c1588e7f4d11b3a723825809411a">&#9670;&#160;</a></span>dim_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.dim_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise. </p>

</div>
</div>
<a id="a9244a2ba98485e2bedf6cee830ca1a22" name="a9244a2ba98485e2bedf6cee830ca1a22"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9244a2ba98485e2bedf6cee830ca1a22">&#9670;&#160;</a></span>distribution</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.distribution</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input distribution object. </p>

</div>
</div>
<a id="a6dc53028e64988f4a9442a047736acb8" name="a6dc53028e64988f4a9442a047736acb8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6dc53028e64988f4a9442a047736acb8">&#9670;&#160;</a></span>dtype</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.dtype</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>dtype</code> argument; indicating the datatype class. </p>

</div>
</div>
<a id="affc2a52ddeb68b4c0a6c05793fa58261" name="affc2a52ddeb68b4c0a6c05793fa58261"></a>
<h2 class="memtitle"><span class="permalink"><a href="#affc2a52ddeb68b4c0a6c05793fa58261">&#9670;&#160;</a></span>entropy_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.entropy_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input entropy coefficient. </p>

</div>
</div>
<a id="a940d6caf441fdc4b732ef0185e2694e9" name="a940d6caf441fdc4b732ef0185e2694e9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a940d6caf441fdc4b732ef0185e2694e9">&#9670;&#160;</a></span>eps_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.eps_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation. </p>

</div>
</div>
<a id="ac76827b34c015165373ec76358b60dea" name="ac76827b34c015165373ec76358b60dea"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac76827b34c015165373ec76358b60dea">&#9670;&#160;</a></span>exploration_tool</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.exploration_tool</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>exploration_tool</code>. </p>

</div>
</div>
<a id="ab09c920ad70ce8b4cd3fa485b2f1bd55" name="ab09c920ad70ce8b4cd3fa485b2f1bd55"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab09c920ad70ce8b4cd3fa485b2f1bd55">&#9670;&#160;</a></span>gamma</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.gamma</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input discounting factor. </p>

</div>
</div>
<a id="a724bcb1e4d32cd2cd880911eed207af6" name="a724bcb1e4d32cd2cd880911eed207af6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a724bcb1e4d32cd2cd880911eed207af6">&#9670;&#160;</a></span>grad_accumulation_rounds</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.grad_accumulation_rounds</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>grad_accumulation_rounds</code>. </p>

</div>
</div>
<a id="aa2bfe032740a7146997162d125274fe9" name="aa2bfe032740a7146997162d125274fe9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2bfe032740a7146997162d125274fe9">&#9670;&#160;</a></span>grad_norm_p</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.grad_norm_p</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings. </p>

</div>
</div>
<a id="a4b605080b32f1199d314ded5efa4230a" name="a4b605080b32f1199d314ded5efa4230a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4b605080b32f1199d314ded5efa4230a">&#9670;&#160;</a></span>is_continuous_action_space</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.is_continuous_action_space</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Flag indicating if action space is continuous or discrete. </p>

</div>
</div>
<a id="af5485148744d68c770b872e62361ca10" name="af5485148744d68c770b872e62361ca10"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af5485148744d68c770b872e62361ca10">&#9670;&#160;</a></span>loss_function</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.loss_function</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input loss function. </p>

</div>
</div>
<a id="a081dcbd51017df1d26abbf15821089d7" name="a081dcbd51017df1d26abbf15821089d7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a081dcbd51017df1d26abbf15821089d7">&#9670;&#160;</a></span>lr_scheduler</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.lr_scheduler</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optional LR Scheduler (this can be None). </p>

</div>
</div>
<a id="a2238587b60c04bf2a7e83e497a030b1e" name="a2238587b60c04bf2a7e83e497a030b1e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2238587b60c04bf2a7e83e497a030b1e">&#9670;&#160;</a></span>lr_threshold</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.lr_threshold</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input LR Threshold. </p>

</div>
</div>
<a id="ae98b91d40567e6e9990304977809d997" name="ae98b91d40567e6e9990304977809d997"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae98b91d40567e6e9990304977809d997">&#9670;&#160;</a></span>max_grad_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.max_grad_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings. </p>

</div>
</div>
<a id="aa2f798b74bd64640ef35da8f672f3e95" name="aa2f798b74bd64640ef35da8f672f3e95"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2f798b74bd64640ef35da8f672f3e95">&#9670;&#160;</a></span>optimizer</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.optimizer</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optimizer wrapped with policy_model parameters. </p>

</div>
</div>
<a id="acde06241652873c70e7d4d9523eab0b0" name="acde06241652873c70e7d4d9523eab0b0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acde06241652873c70e7d4d9523eab0b0">&#9670;&#160;</a></span>p_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.p_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation. </p>

</div>
</div>
<a id="a3c0e7b3a8cc0a96cebe026d304c5590c" name="a3c0e7b3a8cc0a96cebe026d304c5590c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3c0e7b3a8cc0a96cebe026d304c5590c">&#9670;&#160;</a></span>policy_model</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.policy_model</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input policy model moved to desired device. </p>

</div>
</div>
<a id="a9e282f966f2bd18c24ffba49ceefd00b" name="a9e282f966f2bd18c24ffba49ceefd00b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9e282f966f2bd18c24ffba49ceefd00b">&#9670;&#160;</a></span>rollout_accumulation_size</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.rollout_accumulation_size</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>rollout_accumulation_size</code>. </p>

</div>
</div>
<a id="a00c1fb74f19174eecd6fcd03dc0b9843" name="a00c1fb74f19174eecd6fcd03dc0b9843"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00c1fb74f19174eecd6fcd03dc0b9843">&#9670;&#160;</a></span>save_path</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.save_path</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input save path for backing up agent models. </p>

</div>
</div>
<a id="a073e3f0455059d575ce1bc9a28b3eee5" name="a073e3f0455059d575ce1bc9a28b3eee5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a073e3f0455059d575ce1bc9a28b3eee5">&#9670;&#160;</a></span>state_value_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.state_value_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input state value coefficient. </p>

</div>
</div>
<a id="a42a2685035dad0c95918126682a0d4ff" name="a42a2685035dad0c95918126682a0d4ff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a42a2685035dad0c95918126682a0d4ff">&#9670;&#160;</a></span>step_counter</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.step_counter</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The step counter; counting the total timesteps done so far. </p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacerlpack.html">rlpack</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic.html">actor_critic</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic_1_1utils.html">utils</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent.html">actor_critic_agent</a></li><li class="navelem"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html">ActorCriticAgent</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
