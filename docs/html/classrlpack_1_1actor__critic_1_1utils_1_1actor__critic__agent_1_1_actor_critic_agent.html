<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RLPack: rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="RLPack-logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">RLPack
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Data Fields</a> &#124;
<a href="#pri-methods">Private Member Functions</a> &#124;
<a href="#pri-attribs">Private Attributes</a>  </div>
  <div class="headertitle"><div class="title">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>The <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html" title="The ActorCriticAgent is the base class for actor-critic methods.">ActorCriticAgent</a> is the base class for actor-critic methods.  
 <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#details">More...</a></p>
<div id="dynsection-0" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-0-trigger" src="closed.png" alt="+"/> Inheritance diagram for rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent:</div>
<div id="dynsection-0-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-0-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent__inherit__graph.png" border="0" usemap="#arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_inherit__map" alt="Inheritance graph"/></div>
<map name="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_inherit__map" id="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_inherit__map">
<area shape="rect" title="The ActorCriticAgent is the base class for actor&#45;critic methods." alt="" coords="99,167,295,207"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html" title="The A2C class implements the synchronous Actor&#45;Critic method." alt="" coords="5,255,185,280"/>
<area shape="rect" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html" title="The A3C class implements the asynchronous Actor&#45;Critic method." alt="" coords="209,255,389,280"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="118,79,275,119"/>
<area shape="rect" title=" " alt="" coords="168,5,225,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<div id="dynsection-1" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-1-trigger" src="closed.png" alt="+"/> Collaboration diagram for rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent:</div>
<div id="dynsection-1-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-1-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent__coll__graph.png" border="0" usemap="#arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_coll__map" alt="Collaboration graph"/></div>
<map name="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_coll__map" id="arlpack_8actor__critic_8utils_8actor__critic__agent_8_actor_critic_agent_coll__map">
<area shape="rect" title="The ActorCriticAgent is the base class for actor&#45;critic methods." alt="" coords="5,167,201,207"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="25,79,182,119"/>
<area shape="rect" title=" " alt="" coords="75,5,132,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:ae160cb14453efef932099ce2f526b7df"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae160cb14453efef932099ce2f526b7df">__init__</a> (self, pytorch.nn.Module <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3c0e7b3a8cc0a96cebe026d304c5590c">policy_model</a>, pytorch.optim.Optimizer <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2f798b74bd64640ef35da8f672f3e95">optimizer</a>, Union[LRScheduler, None] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a081dcbd51017df1d26abbf15821089d7">lr_scheduler</a>, <a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_loss_function.html">LossFunction</a> <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af5485148744d68c770b872e62361ca10">loss_function</a>, Type[pytorch_distributions.Distribution] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">distribution</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab09c920ad70ce8b4cd3fa485b2f1bd55">gamma</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#affc2a52ddeb68b4c0a6c05793fa58261">entropy_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a073e3f0455059d575ce1bc9a28b3eee5">state_value_coefficient</a>, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2238587b60c04bf2a7e83e497a030b1e">lr_threshold</a>, Union[int, Tuple[int, Union[List[int], None]]] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a31e3fa147ea1d19490fb388ffe61aee3">action_space</a>, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af19d87d26f908a6938f3b3ec1d25c70c">backup_frequency</a>, str <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a00c1fb74f19174eecd6fcd03dc0b9843">save_path</a>, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0ff772c54fb5bb2405a8fb9e7543f7d7">bootstrap_rounds</a>=1, bool <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af11744c1323ad074e2770b28e36807b1">add_gaussian_noise</a>=False, str <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a05e8b2d000bd94f5fb854516c84200e5">device</a>=&quot;cpu&quot;, str <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a6dc53028e64988f4a9442a047736acb8">dtype</a>=&quot;float32&quot;, Union[int, str] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aff3d0599e56caa4b5b0b36cccbf684b1">apply_norm</a>=-1, Union[int, List[str]] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7e6b39931e27a1d569945fdb177d40cc">apply_norm_to</a>=-1, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a940d6caf441fdc4b732ef0185e2694e9">eps_for_norm</a>=5e-12, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#acde06241652873c70e7d4d9523eab0b0">p_for_norm</a>=2, int <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a1db6c1588e7f4d11b3a723825809411a">dim_for_norm</a>=0, Optional[float] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae98b91d40567e6e9990304977809d997">max_grad_norm</a>=None, float <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2bfe032740a7146997162d125274fe9">grad_norm_p</a>=2.0, Optional[float] <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa7a4a5ac2156c6877cbc0533601724e7">clip_grad_value</a>=None, Optional[Tuple[Union[float, np.ndarray, pytorch.Tensor], Callable[[float, bool, int], float],]] variance=None, int max_timesteps=1000)</td></tr>
<tr class="separator:ae160cb14453efef932099ce2f526b7df"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a369d886df0d6762cb825f10682607581"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a369d886df0d6762cb825f10682607581">load</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:a369d886df0d6762cb825f10682607581"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a369d886df0d6762cb825f10682607581">More...</a><br /></td></tr>
<tr class="separator:a369d886df0d6762cb825f10682607581"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae8e3441052d7b47fa91188a684e18659"><td class="memItemLeft" align="right" valign="top">np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae8e3441052d7b47fa91188a684e18659">policy</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, **kwargs)</td></tr>
<tr class="memdesc:ae8e3441052d7b47fa91188a684e18659"><td class="mdescLeft">&#160;</td><td class="mdescRight">The policy method to evaluate the agent.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae8e3441052d7b47fa91188a684e18659">More...</a><br /></td></tr>
<tr class="separator:ae8e3441052d7b47fa91188a684e18659"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad0e78685793f897be63b701cb75b6fc5"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ad0e78685793f897be63b701cb75b6fc5">save</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:ad0e78685793f897be63b701cb75b6fc5"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ad0e78685793f897be63b701cb75b6fc5">More...</a><br /></td></tr>
<tr class="separator:ad0e78685793f897be63b701cb75b6fc5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a5798a278a42acc9ce85d6d7797d889"><td class="memItemLeft" align="right" valign="top">np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4a5798a278a42acc9ce85d6d7797d889">train</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, Union[int, float] reward, Union[bool, int] done, **kwargs)</td></tr>
<tr class="memdesc:a4a5798a278a42acc9ce85d6d7797d889"><td class="mdescLeft">&#160;</td><td class="mdescRight">The train method to train the agent and underlying policy model.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4a5798a278a42acc9ce85d6d7797d889">More...</a><br /></td></tr>
<tr class="separator:a4a5798a278a42acc9ce85d6d7797d889"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Dict[str, Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">__getstate__</a> (self)</td></tr>
<tr class="memdesc:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To get the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">More...</a><br /></td></tr>
<tr class="separator:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">__init__</a> (self)</td></tr>
<tr class="memdesc:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The class initializer.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">More...</a><br /></td></tr>
<tr class="separator:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">__setstate__</a> (self, Dict[str, Any] state)</td></tr>
<tr class="memdesc:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To load the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">More...</a><br /></td></tr>
<tr class="separator:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">load</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">More...</a><br /></td></tr>
<tr class="separator:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">policy</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Policy method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">More...</a><br /></td></tr>
<tr class="separator:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">save</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">More...</a><br /></td></tr>
<tr class="separator:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">train</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Training method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">More...</a><br /></td></tr>
<tr class="separator:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Data Fields</h2></td></tr>
<tr class="memitem:a31e3fa147ea1d19490fb388ffe61aee3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a31e3fa147ea1d19490fb388ffe61aee3">action_space</a></td></tr>
<tr class="memdesc:a31e3fa147ea1d19490fb388ffe61aee3"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input number of actions.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a31e3fa147ea1d19490fb388ffe61aee3">More...</a><br /></td></tr>
<tr class="separator:a31e3fa147ea1d19490fb388ffe61aee3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af11744c1323ad074e2770b28e36807b1"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af11744c1323ad074e2770b28e36807b1">add_gaussian_noise</a></td></tr>
<tr class="memdesc:af11744c1323ad074e2770b28e36807b1"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>add_gaussian_noise</code>.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af11744c1323ad074e2770b28e36807b1">More...</a><br /></td></tr>
<tr class="separator:af11744c1323ad074e2770b28e36807b1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aff3d0599e56caa4b5b0b36cccbf684b1"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aff3d0599e56caa4b5b0b36cccbf684b1">apply_norm</a></td></tr>
<tr class="memdesc:aff3d0599e56caa4b5b0b36cccbf684b1"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm</code> argument; indicating the normalisation to be used.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aff3d0599e56caa4b5b0b36cccbf684b1">More...</a><br /></td></tr>
<tr class="separator:aff3d0599e56caa4b5b0b36cccbf684b1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7e6b39931e27a1d569945fdb177d40cc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7e6b39931e27a1d569945fdb177d40cc">apply_norm_to</a></td></tr>
<tr class="memdesc:a7e6b39931e27a1d569945fdb177d40cc"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm_to</code> argument; indicating the quantity to normalise.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a7e6b39931e27a1d569945fdb177d40cc">More...</a><br /></td></tr>
<tr class="separator:a7e6b39931e27a1d569945fdb177d40cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af19d87d26f908a6938f3b3ec1d25c70c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af19d87d26f908a6938f3b3ec1d25c70c">backup_frequency</a></td></tr>
<tr class="memdesc:af19d87d26f908a6938f3b3ec1d25c70c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input model backup frequency in terms of timesteps.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af19d87d26f908a6938f3b3ec1d25c70c">More...</a><br /></td></tr>
<tr class="separator:af19d87d26f908a6938f3b3ec1d25c70c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0ff772c54fb5bb2405a8fb9e7543f7d7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0ff772c54fb5bb2405a8fb9e7543f7d7">bootstrap_rounds</a></td></tr>
<tr class="memdesc:a0ff772c54fb5bb2405a8fb9e7543f7d7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input boostrap rounds.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0ff772c54fb5bb2405a8fb9e7543f7d7">More...</a><br /></td></tr>
<tr class="separator:a0ff772c54fb5bb2405a8fb9e7543f7d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa7a4a5ac2156c6877cbc0533601724e7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa7a4a5ac2156c6877cbc0533601724e7">clip_grad_value</a></td></tr>
<tr class="memdesc:aa7a4a5ac2156c6877cbc0533601724e7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>clip_grad_value</code>; indicating the clipping range for gradients.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa7a4a5ac2156c6877cbc0533601724e7">More...</a><br /></td></tr>
<tr class="separator:aa7a4a5ac2156c6877cbc0533601724e7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a05e8b2d000bd94f5fb854516c84200e5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a05e8b2d000bd94f5fb854516c84200e5">device</a></td></tr>
<tr class="memdesc:a05e8b2d000bd94f5fb854516c84200e5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>device</code> argument; indicating the device name as device type class.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a05e8b2d000bd94f5fb854516c84200e5">More...</a><br /></td></tr>
<tr class="separator:a05e8b2d000bd94f5fb854516c84200e5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1db6c1588e7f4d11b3a723825809411a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a1db6c1588e7f4d11b3a723825809411a">dim_for_norm</a></td></tr>
<tr class="memdesc:a1db6c1588e7f4d11b3a723825809411a"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a1db6c1588e7f4d11b3a723825809411a">More...</a><br /></td></tr>
<tr class="separator:a1db6c1588e7f4d11b3a723825809411a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9244a2ba98485e2bedf6cee830ca1a22"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">distribution</a></td></tr>
<tr class="memdesc:a9244a2ba98485e2bedf6cee830ca1a22"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input distribution object.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a9244a2ba98485e2bedf6cee830ca1a22">More...</a><br /></td></tr>
<tr class="separator:a9244a2ba98485e2bedf6cee830ca1a22"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6dc53028e64988f4a9442a047736acb8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a6dc53028e64988f4a9442a047736acb8">dtype</a></td></tr>
<tr class="memdesc:a6dc53028e64988f4a9442a047736acb8"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>dtype</code> argument; indicating the datatype class.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a6dc53028e64988f4a9442a047736acb8">More...</a><br /></td></tr>
<tr class="separator:a6dc53028e64988f4a9442a047736acb8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:affc2a52ddeb68b4c0a6c05793fa58261"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#affc2a52ddeb68b4c0a6c05793fa58261">entropy_coefficient</a></td></tr>
<tr class="memdesc:affc2a52ddeb68b4c0a6c05793fa58261"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input entropy coefficient.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#affc2a52ddeb68b4c0a6c05793fa58261">More...</a><br /></td></tr>
<tr class="separator:affc2a52ddeb68b4c0a6c05793fa58261"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a940d6caf441fdc4b732ef0185e2694e9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a940d6caf441fdc4b732ef0185e2694e9">eps_for_norm</a></td></tr>
<tr class="memdesc:a940d6caf441fdc4b732ef0185e2694e9"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a940d6caf441fdc4b732ef0185e2694e9">More...</a><br /></td></tr>
<tr class="separator:a940d6caf441fdc4b732ef0185e2694e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab09c920ad70ce8b4cd3fa485b2f1bd55"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab09c920ad70ce8b4cd3fa485b2f1bd55">gamma</a></td></tr>
<tr class="memdesc:ab09c920ad70ce8b4cd3fa485b2f1bd55"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input discounting factor.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ab09c920ad70ce8b4cd3fa485b2f1bd55">More...</a><br /></td></tr>
<tr class="separator:ab09c920ad70ce8b4cd3fa485b2f1bd55"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a53dd48a82891207362f85da40e84ecf7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a53dd48a82891207362f85da40e84ecf7">gaussian_noise</a></td></tr>
<tr class="memdesc:a53dd48a82891207362f85da40e84ecf7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The PyTorch Normal distribution object initialized with standard mean and std.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a53dd48a82891207362f85da40e84ecf7">More...</a><br /></td></tr>
<tr class="separator:a53dd48a82891207362f85da40e84ecf7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2bfe032740a7146997162d125274fe9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2bfe032740a7146997162d125274fe9">grad_norm_p</a></td></tr>
<tr class="memdesc:aa2bfe032740a7146997162d125274fe9"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2bfe032740a7146997162d125274fe9">More...</a><br /></td></tr>
<tr class="separator:aa2bfe032740a7146997162d125274fe9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4b605080b32f1199d314ded5efa4230a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4b605080b32f1199d314ded5efa4230a">is_continuous_action_space</a></td></tr>
<tr class="memdesc:a4b605080b32f1199d314ded5efa4230a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Flag indicating if action space is continuous or discrete.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a4b605080b32f1199d314ded5efa4230a">More...</a><br /></td></tr>
<tr class="separator:a4b605080b32f1199d314ded5efa4230a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af5485148744d68c770b872e62361ca10"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af5485148744d68c770b872e62361ca10">loss_function</a></td></tr>
<tr class="memdesc:af5485148744d68c770b872e62361ca10"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input loss function.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af5485148744d68c770b872e62361ca10">More...</a><br /></td></tr>
<tr class="separator:af5485148744d68c770b872e62361ca10"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a081dcbd51017df1d26abbf15821089d7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a081dcbd51017df1d26abbf15821089d7">lr_scheduler</a></td></tr>
<tr class="memdesc:a081dcbd51017df1d26abbf15821089d7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optional LR Scheduler (this can be None).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a081dcbd51017df1d26abbf15821089d7">More...</a><br /></td></tr>
<tr class="separator:a081dcbd51017df1d26abbf15821089d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2238587b60c04bf2a7e83e497a030b1e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2238587b60c04bf2a7e83e497a030b1e">lr_threshold</a></td></tr>
<tr class="memdesc:a2238587b60c04bf2a7e83e497a030b1e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input LR Threshold.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a2238587b60c04bf2a7e83e497a030b1e">More...</a><br /></td></tr>
<tr class="separator:a2238587b60c04bf2a7e83e497a030b1e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae98b91d40567e6e9990304977809d997"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae98b91d40567e6e9990304977809d997">max_grad_norm</a></td></tr>
<tr class="memdesc:ae98b91d40567e6e9990304977809d997"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ae98b91d40567e6e9990304977809d997">More...</a><br /></td></tr>
<tr class="separator:ae98b91d40567e6e9990304977809d997"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2f798b74bd64640ef35da8f672f3e95"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2f798b74bd64640ef35da8f672f3e95">optimizer</a></td></tr>
<tr class="memdesc:aa2f798b74bd64640ef35da8f672f3e95"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optimizer wrapped with policy_model parameters.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aa2f798b74bd64640ef35da8f672f3e95">More...</a><br /></td></tr>
<tr class="separator:aa2f798b74bd64640ef35da8f672f3e95"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acde06241652873c70e7d4d9523eab0b0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#acde06241652873c70e7d4d9523eab0b0">p_for_norm</a></td></tr>
<tr class="memdesc:acde06241652873c70e7d4d9523eab0b0"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#acde06241652873c70e7d4d9523eab0b0">More...</a><br /></td></tr>
<tr class="separator:acde06241652873c70e7d4d9523eab0b0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c0e7b3a8cc0a96cebe026d304c5590c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3c0e7b3a8cc0a96cebe026d304c5590c">policy_model</a></td></tr>
<tr class="memdesc:a3c0e7b3a8cc0a96cebe026d304c5590c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input policy model moved to desired device.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3c0e7b3a8cc0a96cebe026d304c5590c">More...</a><br /></td></tr>
<tr class="separator:a3c0e7b3a8cc0a96cebe026d304c5590c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a00c1fb74f19174eecd6fcd03dc0b9843"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a00c1fb74f19174eecd6fcd03dc0b9843">save_path</a></td></tr>
<tr class="memdesc:a00c1fb74f19174eecd6fcd03dc0b9843"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input save path for backing up agent models.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a00c1fb74f19174eecd6fcd03dc0b9843">More...</a><br /></td></tr>
<tr class="separator:a00c1fb74f19174eecd6fcd03dc0b9843"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a073e3f0455059d575ce1bc9a28b3eee5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a073e3f0455059d575ce1bc9a28b3eee5">state_value_coefficient</a></td></tr>
<tr class="memdesc:a073e3f0455059d575ce1bc9a28b3eee5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input state value coefficient.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a073e3f0455059d575ce1bc9a28b3eee5">More...</a><br /></td></tr>
<tr class="separator:a073e3f0455059d575ce1bc9a28b3eee5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a42a2685035dad0c95918126682a0d4ff"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a42a2685035dad0c95918126682a0d4ff">step_counter</a></td></tr>
<tr class="memdesc:a42a2685035dad0c95918126682a0d4ff"><td class="mdescLeft">&#160;</td><td class="mdescRight">The step counter; counting the total timesteps done so far.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a42a2685035dad0c95918126682a0d4ff">More...</a><br /></td></tr>
<tr class="separator:a42a2685035dad0c95918126682a0d4ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af4c9087867a69b26c46e1c740d71f2b8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af4c9087867a69b26c46e1c740d71f2b8">variance_decay_fn</a></td></tr>
<tr class="memdesc:af4c9087867a69b26c46e1c740d71f2b8"><td class="mdescLeft">&#160;</td><td class="mdescRight">The variance decay method.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af4c9087867a69b26c46e1c740d71f2b8">More...</a><br /></td></tr>
<tr class="separator:af4c9087867a69b26c46e1c740d71f2b8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a401e5c40d7a31b3cc24c38dbcf40da92"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a401e5c40d7a31b3cc24c38dbcf40da92">variance_value</a></td></tr>
<tr class="memdesc:a401e5c40d7a31b3cc24c38dbcf40da92"><td class="mdescLeft">&#160;</td><td class="mdescRight">The current variance value.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a401e5c40d7a31b3cc24c38dbcf40da92">More...</a><br /></td></tr>
<tr class="separator:a401e5c40d7a31b3cc24c38dbcf40da92"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Data Fields inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4104e76aed3a37b4df4ef1069383aee8">gamma</a></td></tr>
<tr class="memdesc:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The default discounting factor for agents.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4104e76aed3a37b4df4ef1069383aee8">More...</a><br /></td></tr>
<tr class="separator:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">loss</a></td></tr>
<tr class="memdesc:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of losses accumulated after each backward call.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">More...</a><br /></td></tr>
<tr class="separator:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">save_path</a></td></tr>
<tr class="memdesc:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The path to save agent states and models.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">More...</a><br /></td></tr>
<tr class="separator:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-methods" name="pri-methods"></a>
Private Member Functions</h2></td></tr>
<tr class="memitem:a79da689b8d6edfd4ae0f3128ac130d57"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a79da689b8d6edfd4ae0f3128ac130d57">_call_to_save</a> (self)</td></tr>
<tr class="memdesc:a79da689b8d6edfd4ae0f3128ac130d57"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method calling the save method when required.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a79da689b8d6edfd4ae0f3128ac130d57">More...</a><br /></td></tr>
<tr class="separator:a79da689b8d6edfd4ae0f3128ac130d57"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3b679788c7bc1611e23eb122124d5301"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3b679788c7bc1611e23eb122124d5301">_call_to_train_policy_model</a> (self, Union[bool, int] done)</td></tr>
<tr class="memdesc:a3b679788c7bc1611e23eb122124d5301"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method to train the policy model.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a3b679788c7bc1611e23eb122124d5301">More...</a><br /></td></tr>
<tr class="separator:a3b679788c7bc1611e23eb122124d5301"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8055e40213b37ef9bcf3c0c750c3af73"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a8055e40213b37ef9bcf3c0c750c3af73">_clear</a> (self)</td></tr>
<tr class="memdesc:a8055e40213b37ef9bcf3c0c750c3af73"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to clear the lists of rewards, action_log_probs and state_values.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a8055e40213b37ef9bcf3c0c750c3af73">More...</a><br /></td></tr>
<tr class="separator:a8055e40213b37ef9bcf3c0c750c3af73"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac18f59ffac99d3f57916db692df9adf0"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac18f59ffac99d3f57916db692df9adf0">_compute_advantage</a> (self, pytorch.Tensor returns, pytorch.Tensor state_current_values)</td></tr>
<tr class="memdesc:ac18f59ffac99d3f57916db692df9adf0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the advantage from returns and state values.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#ac18f59ffac99d3f57916db692df9adf0">More...</a><br /></td></tr>
<tr class="separator:ac18f59ffac99d3f57916db692df9adf0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0dddd26f1bfa9e729fb771a28b2cd4bb"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0dddd26f1bfa9e729fb771a28b2cd4bb">_compute_loss</a> (self)</td></tr>
<tr class="memdesc:a0dddd26f1bfa9e729fb771a28b2cd4bb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Method to compute total loss (from actor and critic).  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a0dddd26f1bfa9e729fb771a28b2cd4bb">More...</a><br /></td></tr>
<tr class="separator:a0dddd26f1bfa9e729fb771a28b2cd4bb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a97bd95a17383eabb3d8cb6bc89ff1a45"><td class="memItemLeft" align="right" valign="top">pytorch_distributions.Distribution&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a97bd95a17383eabb3d8cb6bc89ff1a45">_create_action_distribution</a> (self, Union[List[pytorch.Tensor], pytorch.Tensor] action_values)</td></tr>
<tr class="memdesc:a97bd95a17383eabb3d8cb6bc89ff1a45"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected static method to create distributions from action logits.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a97bd95a17383eabb3d8cb6bc89ff1a45">More...</a><br /></td></tr>
<tr class="separator:a97bd95a17383eabb3d8cb6bc89ff1a45"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a86d3550730c8f371bbfdc527b21d589d"><td class="memItemLeft" align="right" valign="top">pytorch_distributions.Normal&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a86d3550730c8f371bbfdc527b21d589d">_create_noise_distribution</a> (self)</td></tr>
<tr class="memdesc:a86d3550730c8f371bbfdc527b21d589d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Create the standard Gaussian Distribution object for adding noise to sampled actions.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a86d3550730c8f371bbfdc527b21d589d">More...</a><br /></td></tr>
<tr class="separator:a86d3550730c8f371bbfdc527b21d589d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a84eb3266451b8eb5210187c2f02f30a6"><td class="memItemLeft" align="right" valign="top">pytorch.Size&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a84eb3266451b8eb5210187c2f02f30a6">_get_action_sample_shape_for_continuous</a> (self)</td></tr>
<tr class="memdesc:a84eb3266451b8eb5210187c2f02f30a6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the action sample shape to be sampled from continuous distribution.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a84eb3266451b8eb5210187c2f02f30a6">More...</a><br /></td></tr>
<tr class="separator:a84eb3266451b8eb5210187c2f02f30a6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afef85f43b81e3743e774ca8b47265b72"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#afef85f43b81e3743e774ca8b47265b72">_grad_mean_reduction</a> (self)</td></tr>
<tr class="memdesc:afef85f43b81e3743e774ca8b47265b72"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs mean reduction and assigns the policy model's parameter the mean reduced gradients.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#afef85f43b81e3743e774ca8b47265b72">More...</a><br /></td></tr>
<tr class="separator:afef85f43b81e3743e774ca8b47265b72"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af7d9eb93ae1c767f32827d148e484e54"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af7d9eb93ae1c767f32827d148e484e54">_run_optimizer</a> (self, <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">loss</a>)</td></tr>
<tr class="memdesc:af7d9eb93ae1c767f32827d148e484e54"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected void method to train the model or accumulate the gradients for training.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af7d9eb93ae1c767f32827d148e484e54">More...</a><br /></td></tr>
<tr class="separator:af7d9eb93ae1c767f32827d148e484e54"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-attribs" name="pri-attribs"></a>
Private Attributes</h2></td></tr>
<tr class="memitem:aea7f62cfe7b129431fa4bf3598463708"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aea7f62cfe7b129431fa4bf3598463708">_grad_accumulator</a></td></tr>
<tr class="memdesc:aea7f62cfe7b129431fa4bf3598463708"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of gradients from each backward call.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#aea7f62cfe7b129431fa4bf3598463708">More...</a><br /></td></tr>
<tr class="separator:aea7f62cfe7b129431fa4bf3598463708"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8ad137357b10ed57e1e47523b9f1b19"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af8ad137357b10ed57e1e47523b9f1b19">_normalization</a></td></tr>
<tr class="memdesc:af8ad137357b10ed57e1e47523b9f1b19"><td class="mdescLeft">&#160;</td><td class="mdescRight">The normalisation tool to be used for agent.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#af8ad137357b10ed57e1e47523b9f1b19">More...</a><br /></td></tr>
<tr class="separator:af8ad137357b10ed57e1e47523b9f1b19"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a367ac19a8cdfce31f6b2fb7a51950d66"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a367ac19a8cdfce31f6b2fb7a51950d66">_operate_with_variance</a></td></tr>
<tr class="memdesc:a367ac19a8cdfce31f6b2fb7a51950d66"><td class="mdescLeft">&#160;</td><td class="mdescRight">The boolean flag indicating if variance operations are to be used.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a367ac19a8cdfce31f6b2fb7a51950d66">More...</a><br /></td></tr>
<tr class="separator:a367ac19a8cdfce31f6b2fb7a51950d66"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a34547eb17d04d4f5a476a2543f5a97f4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a34547eb17d04d4f5a476a2543f5a97f4">_rollout_buffer</a></td></tr>
<tr class="memdesc:a34547eb17d04d4f5a476a2543f5a97f4"><td class="mdescLeft">&#160;</td><td class="mdescRight">The rollout buffer to be used for agent to store necessary outputs.  <a href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html#a34547eb17d04d4f5a476a2543f5a97f4">More...</a><br /></td></tr>
<tr class="separator:a34547eb17d04d4f5a476a2543f5a97f4"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >The <a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html" title="The ActorCriticAgent is the base class for actor-critic methods.">ActorCriticAgent</a> is the base class for actor-critic methods. </p>
<p >This class implements basic methods and abstract functions for actor-critic methods. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="ae160cb14453efef932099ce2f526b7df" name="ae160cb14453efef932099ce2f526b7df"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae160cb14453efef932099ce2f526b7df">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.nn.Module&#160;</td>
          <td class="paramname"><em>policy_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.optim.Optimizer&#160;</td>
          <td class="paramname"><em>optimizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[LRScheduler, None]&#160;</td>
          <td class="paramname"><em>lr_scheduler</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classrlpack_1_1utils_1_1typing__hints_1_1_loss_function.html">LossFunction</a>&#160;</td>
          <td class="paramname"><em>loss_function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Type[pytorch_distributions.Distribution]&#160;</td>
          <td class="paramname"><em>distribution</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>entropy_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>state_value_coefficient</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>lr_threshold</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, Tuple[int, Union[List[int], None]]]&#160;</td>
          <td class="paramname"><em>action_space</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>backup_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>save_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>bootstrap_rounds</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>add_gaussian_noise</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>device</em> = <code>&quot;cpu&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>dtype</em> = <code>&quot;float32&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, str] &#160;</td>
          <td class="paramname"><em>apply_norm</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, List[str]] &#160;</td>
          <td class="paramname"><em>apply_norm_to</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>eps_for_norm</em> = <code>5e-12</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>p_for_norm</em> = <code>2</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>dim_for_norm</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>max_grad_norm</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>grad_norm_p</em> = <code>2.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>clip_grad_value</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[
            Tuple[
                Union[float, np.ndarray, pytorch.Tensor],
                Callable[[float, bool, int], float],
            ]
        ] &#160;</td>
          <td class="paramname"><em>variance</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>max_timesteps</em> = <code>1000</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">policy_model</td><td><em>pytorch.nn.Module</em>: The policy model to be used. Policy model must return a tuple of action logits and state values. </td></tr>
    <tr><td class="paramname">optimizer</td><td>pytorch.optim.Optimizer: The optimizer to be used for policy model. Optimizer must be initialized and wrapped with policy model parameters. </td></tr>
    <tr><td class="paramname">lr_scheduler</td><td>Union[LRScheduler, None]: The LR Scheduler to be used to decay the learning rate. LR Scheduler must be initialized and wrapped with passed optimizer. </td></tr>
    <tr><td class="paramname">loss_function</td><td>LossFunction: A PyTorch loss function. </td></tr>
    <tr><td class="paramname">distribution</td><td>: Type[pytorch_distributions.Distribution]: The distribution of PyTorch to be used to sampled actions in action space. (See <code>action_space</code>). </td></tr>
    <tr><td class="paramname">gamma</td><td>float: The discounting factor for rewards. </td></tr>
    <tr><td class="paramname">entropy_coefficient</td><td>float: The coefficient to be used for entropy in policy loss computation. </td></tr>
    <tr><td class="paramname">state_value_coefficient</td><td>float: The coefficient to be used for state value in final loss computation. </td></tr>
    <tr><td class="paramname">lr_threshold</td><td>float: The threshold LR which once reached LR scheduler is not called further. </td></tr>
    <tr><td class="paramname">action_space</td><td>Union[int, Tuple[int, Union[List[int], None]]]: The action space of the environment.<ul>
<li>If discrete action set is used, number of actions can be passed.</li>
<li>If continuous action space is used, a list must be passed with first element representing the output features from model, second element representing the shape of action to be sampled. Second element can be an empty list or None, if you wish to sample the default no. of samples. </li>
</ul>
</td></tr>
    <tr><td class="paramname">backup_frequency</td><td>int: The timesteps after which policy model, optimizer states and lr scheduler states are backed up. </td></tr>
    <tr><td class="paramname">save_path</td><td>str: The path where policy model, optimizer states and lr scheduler states are to be saved. </td></tr>
    <tr><td class="paramname">bootstrap_rounds</td><td>int: The number of rounds until which gradients are to be accumulated before performing calling optimizer step. Gradients are mean reduced for bootstrap_rounds &gt; 1. Default: 1. </td></tr>
    <tr><td class="paramname">add_gaussian_noise</td><td>bool: Parameter indicating whether to add gaussian noise from standard normal distribution; N(0, 1) is used. Default: False. </td></tr>
    <tr><td class="paramname">device</td><td>str: The device on which models are run. Default: "cpu". </td></tr>
    <tr><td class="paramname">dtype</td><td>str: The datatype for model parameters. Default: "float32" </td></tr>
    <tr><td class="paramname">apply_norm</td><td>Union[int, str]: The code to select the normalization procedure to be applied on selected quantities; selected by <code>apply_norm_to</code>: see below)). Direct string can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1 </td></tr>
    <tr><td class="paramname">apply_norm_to</td><td>Union[int, List[str]]: The code to select the quantity to which normalization is to be applied. Direct list of quantities can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1. </td></tr>
    <tr><td class="paramname">eps_for_norm</td><td>float: Epsilon value for normalization; for numeric stability. For min-max normalization and standardized normalization. Default: 5e-12. </td></tr>
    <tr><td class="paramname">p_for_norm</td><td>int: The p value for p-normalization. Default: 2; L2 Norm. </td></tr>
    <tr><td class="paramname">dim_for_norm</td><td>int: The dimension across which normalization is to be performed. Default: 0. </td></tr>
    <tr><td class="paramname">max_grad_norm</td><td>Optional[float]: The max norm for gradients for gradient clipping. Default: None </td></tr>
    <tr><td class="paramname">grad_norm_p</td><td>float: The p-value for p-normalization of gradients. Default: 2.0 </td></tr>
    <tr><td class="paramname">clip_grad_value</td><td>Optional[float]: The gradient value for clipping gradients by value. Default: None </td></tr>
    <tr><td class="paramname">variance</td><td>Tuple[ Union[float, np.ndarray, pytorch.Tensor], Callable[[Union[float, np.ndarray, pytorch.Tensor], bool, int], float], ]: The tuple of variance to be used to sample actions for continuous action space and a method to be used to decay it. The passed method have the signature Callable[[float, int], float]. The first argument would be the variance value and second value be the boolean, done flag indicating if the state is terminal or not and third will be the timestep; returning the updated variance value. Default: None </td></tr>
    <tr><td class="paramname">max_timesteps</td><td>int: The maximum timesteps the environment will run for. This is used for memory preemptive allocation for improved efficiency.</td></tr>
  </table>
  </dd>
</dl>
<p><b>Notes</b></p>
<p >The codes for <code>apply_norm</code> are given as follows: -</p><ul>
<li>No Normalization: -1; (<code>"none"</code>)</li>
<li>Min-Max Normalization: 0; (<code>"min_max"</code>)</li>
<li>Standardization: 1; (<code>"standardize"</code>)</li>
<li>P-Normalization: 2; (<code>"p_norm"</code>)</li>
</ul>
<p >The codes for <code>apply_norm_to</code> are given as follows:</p><ul>
<li>No Normalization: -1; (<code>["none"]</code>)</li>
<li>On States only: 0; (<code>["states"]</code>)</li>
<li>On Rewards only: 1; (<code>["rewards"]</code>)</li>
<li>On TD value only: 2; (<code>["advantage"]</code>)</li>
<li>On States and Rewards: 3; (<code>["states", "rewards"]</code>)</li>
<li>On States and TD: 4; (<code>["states", "advantage"]</code>)</li>
</ul>
<p >If a valid <code>max_norm_grad</code> is passed, then gradient clipping takes place else gradient clipping step is skipped. If <code>max_norm_grad</code> value was invalid, error will be raised from PyTorch. If a valid <code>clip_grad_value</code> is passed, then gradients will be clipped by value. If <code>clip_grad_value</code> value was invalid, error will be raised from PyTorch. </p>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">rlpack.utils.base.agent.Agent</a>.</p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a90965a40cbe6a1519bfbdf818231f38c">rlpack.actor_critic.a2c.A2C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#aa0232107426b0bc5ad19bfd3db357439">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a79da689b8d6edfd4ae0f3128ac130d57" name="a79da689b8d6edfd4ae0f3128ac130d57"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a79da689b8d6edfd4ae0f3128ac130d57">&#9670;&#160;</a></span>_call_to_save()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._call_to_save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method calling the save method when required. </p>
<p >This method is to be overriden. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a487c58b125fb7cd2e14972552bdf5ea6">rlpack.actor_critic.a2c.A2C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#ac2f6faa17ef5e68d665f2e085636b5d2">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<a id="a3b679788c7bc1611e23eb122124d5301" name="a3b679788c7bc1611e23eb122124d5301"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3b679788c7bc1611e23eb122124d5301">&#9670;&#160;</a></span>_call_to_train_policy_model()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._call_to_train_policy_model </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method to train the policy model. </p>
<p >If done flag is True, will compute the loss and run the optimizer. This method is meant to periodically check if episode hsa been terminated or and train policy models if episode has terminated. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">done</td><td>Union[bool, int]: Flag indicating if episode has terminated or not </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a8055e40213b37ef9bcf3c0c750c3af73" name="a8055e40213b37ef9bcf3c0c750c3af73"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8055e40213b37ef9bcf3c0c750c3af73">&#9670;&#160;</a></span>_clear()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._clear </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to clear the lists of rewards, action_log_probs and state_values. </p>

</div>
</div>
<a id="ac18f59ffac99d3f57916db692df9adf0" name="ac18f59ffac99d3f57916db692df9adf0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac18f59ffac99d3f57916db692df9adf0">&#9670;&#160;</a></span>_compute_advantage()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._compute_advantage </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>returns</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor
    &#160;</td>
          <td class="paramname"><em>state_current_values</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the advantage from returns and state values. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">returns</td><td>pytorch.Tensor: The discounted returns; computed from _compute_returns method </td></tr>
    <tr><td class="paramname">state_current_values</td><td>pytorch.Tensor: The corresponding state values </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The advantage for the given returns and state values </dd></dl>

</div>
</div>
<a id="a0dddd26f1bfa9e729fb771a28b2cd4bb" name="a0dddd26f1bfa9e729fb771a28b2cd4bb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0dddd26f1bfa9e729fb771a28b2cd4bb">&#9670;&#160;</a></span>_compute_loss()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._compute_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Method to compute total loss (from actor and critic). </p>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The loss tensor. </dd></dl>

</div>
</div>
<a id="a97bd95a17383eabb3d8cb6bc89ff1a45" name="a97bd95a17383eabb3d8cb6bc89ff1a45"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a97bd95a17383eabb3d8cb6bc89ff1a45">&#9670;&#160;</a></span>_create_action_distribution()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch_distributions.Distribution rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._create_action_distribution </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[List[pytorch.Tensor], pytorch.Tensor]&#160;</td>
          <td class="paramname"><em>action_values</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected static method to create distributions from action logits. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">action_values</td><td>Union[List[pytorch.Tensor], pytorch.Tensor]: The action values from policy model </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Distribution: A Distribution object initialized with given action logits </dd></dl>

</div>
</div>
<a id="a86d3550730c8f371bbfdc527b21d589d" name="a86d3550730c8f371bbfdc527b21d589d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a86d3550730c8f371bbfdc527b21d589d">&#9670;&#160;</a></span>_create_noise_distribution()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch_distributions.Normal rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._create_noise_distribution </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Create the standard Gaussian Distribution object for adding noise to sampled actions. </p>

</div>
</div>
<a id="a84eb3266451b8eb5210187c2f02f30a6" name="a84eb3266451b8eb5210187c2f02f30a6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a84eb3266451b8eb5210187c2f02f30a6">&#9670;&#160;</a></span>_get_action_sample_shape_for_continuous()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Size rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._get_action_sample_shape_for_continuous </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets the action sample shape to be sampled from continuous distribution. </p>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Size: Sample shape of to-be sampled tensor from continuous distribution </dd></dl>

</div>
</div>
<a id="afef85f43b81e3743e774ca8b47265b72" name="afef85f43b81e3743e774ca8b47265b72"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afef85f43b81e3743e774ca8b47265b72">&#9670;&#160;</a></span>_grad_mean_reduction()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._grad_mean_reduction </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Performs mean reduction and assigns the policy model's parameter the mean reduced gradients. </p>

</div>
</div>
<a id="af7d9eb93ae1c767f32827d148e484e54" name="af7d9eb93ae1c767f32827d148e484e54"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af7d9eb93ae1c767f32827d148e484e54">&#9670;&#160;</a></span>_run_optimizer()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._run_optimizer </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected void method to train the model or accumulate the gradients for training. </p>
<p >This method is to be overriden. </p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1actor__critic_1_1a2c_1_1_a2_c.html#a1ff4a067c80053f646906c6603e196a8">rlpack.actor_critic.a2c.A2C</a>, and <a class="el" href="classrlpack_1_1actor__critic_1_1a3c_1_1_a3_c.html#a4436f7743be9e692d44b3af582c25193">rlpack.actor_critic.a3c.A3C</a>.</p>

</div>
</div>
<a id="a369d886df0d6762cb825f10682607581" name="a369d886df0d6762cb825f10682607581"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a369d886df0d6762cb825f10682607581">&#9670;&#160;</a></span>load()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.load </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to load the best model by a custom suffix supplied for evaluation. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="ae8e3441052d7b47fa91188a684e18659" name="ae8e3441052d7b47fa91188a684e18659"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae8e3441052d7b47fa91188a684e18659">&#9670;&#160;</a></span>policy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.ndarray rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.policy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The policy method to evaluate the agent. </p>
<p >This runs in pure inference mode. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned from gym environment </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>np.ndarray: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="ad0e78685793f897be63b701cb75b6fc5" name="ad0e78685793f897be63b701cb75b6fc5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad0e78685793f897be63b701cb75b6fc5">&#9670;&#160;</a></span>save()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<p >agent_states includes current memory and epsilon values in a dictionary. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to save best model by a custom suffix supplied during a train run. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="a4a5798a278a42acc9ce85d6d7797d889" name="a4a5798a278a42acc9ce85d6d7797d889"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4a5798a278a42acc9ce85d6d7797d889">&#9670;&#160;</a></span>train()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.ndarray rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.train </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, float]&#160;</td>
          <td class="paramname"><em>reward</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The train method to train the agent and underlying policy model. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state returned </td></tr>
    <tr><td class="paramname">reward</td><td>Union[int, float]: The reward returned from previous action </td></tr>
    <tr><td class="paramname">done</td><td>Union[bool, int]: Flag indicating if episode has terminated or not </td></tr>
    <tr><td class="paramname">kwargs</td><td>Other keyword arguments. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>np.ndarray: The action to be taken </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<h2 class="groupheader">Field Documentation</h2>
<a id="aea7f62cfe7b129431fa4bf3598463708" name="aea7f62cfe7b129431fa4bf3598463708"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aea7f62cfe7b129431fa4bf3598463708">&#9670;&#160;</a></span>_grad_accumulator</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._grad_accumulator</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The list of gradients from each backward call. </p>
<p >This is only used when boostrap_rounds &gt; 1 and is cleared after each boostrap round. The <a class="el" href="classrlpack_1_1___c_1_1grad__accumulator_1_1_grad_accumulator.html" title="This class provides the python interface to C_GradAccumulator, the C++ class which performs heavier w...">rlpack._C.grad_accumulator.GradAccumulator</a> object for grad accumulation. </p>

</div>
</div>
<a id="af8ad137357b10ed57e1e47523b9f1b19" name="af8ad137357b10ed57e1e47523b9f1b19"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af8ad137357b10ed57e1e47523b9f1b19">&#9670;&#160;</a></span>_normalization</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._normalization</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The normalisation tool to be used for agent. </p>
<p >An instance of <a class="el" href="classrlpack_1_1utils_1_1normalization_1_1_normalization.html" title="Normalization class providing methods for normalization techniques.">rlpack.utils.normalization.Normalization</a>. </p>

</div>
</div>
<a id="a367ac19a8cdfce31f6b2fb7a51950d66" name="a367ac19a8cdfce31f6b2fb7a51950d66"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a367ac19a8cdfce31f6b2fb7a51950d66">&#9670;&#160;</a></span>_operate_with_variance</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._operate_with_variance</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The boolean flag indicating if variance operations are to be used. </p>

</div>
</div>
<a id="a34547eb17d04d4f5a476a2543f5a97f4" name="a34547eb17d04d4f5a476a2543f5a97f4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a34547eb17d04d4f5a476a2543f5a97f4">&#9670;&#160;</a></span>_rollout_buffer</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent._rollout_buffer</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The rollout buffer to be used for agent to store necessary outputs. </p>
<p >An instance of <a class="el" href="classrlpack_1_1___c_1_1rollout__buffer_1_1_rollout_buffer.html">rlpack._C.rollout_buffer.RolloutBuffer</a> </p>

</div>
</div>
<a id="a31e3fa147ea1d19490fb388ffe61aee3" name="a31e3fa147ea1d19490fb388ffe61aee3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a31e3fa147ea1d19490fb388ffe61aee3">&#9670;&#160;</a></span>action_space</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.action_space</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input number of actions. </p>

</div>
</div>
<a id="af11744c1323ad074e2770b28e36807b1" name="af11744c1323ad074e2770b28e36807b1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af11744c1323ad074e2770b28e36807b1">&#9670;&#160;</a></span>add_gaussian_noise</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.add_gaussian_noise</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>add_gaussian_noise</code>. </p>

</div>
</div>
<a id="aff3d0599e56caa4b5b0b36cccbf684b1" name="aff3d0599e56caa4b5b0b36cccbf684b1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aff3d0599e56caa4b5b0b36cccbf684b1">&#9670;&#160;</a></span>apply_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.apply_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm</code> argument; indicating the normalisation to be used. </p>

</div>
</div>
<a id="a7e6b39931e27a1d569945fdb177d40cc" name="a7e6b39931e27a1d569945fdb177d40cc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7e6b39931e27a1d569945fdb177d40cc">&#9670;&#160;</a></span>apply_norm_to</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.apply_norm_to</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm_to</code> argument; indicating the quantity to normalise. </p>

</div>
</div>
<a id="af19d87d26f908a6938f3b3ec1d25c70c" name="af19d87d26f908a6938f3b3ec1d25c70c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af19d87d26f908a6938f3b3ec1d25c70c">&#9670;&#160;</a></span>backup_frequency</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.backup_frequency</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input model backup frequency in terms of timesteps. </p>

</div>
</div>
<a id="a0ff772c54fb5bb2405a8fb9e7543f7d7" name="a0ff772c54fb5bb2405a8fb9e7543f7d7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0ff772c54fb5bb2405a8fb9e7543f7d7">&#9670;&#160;</a></span>bootstrap_rounds</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.bootstrap_rounds</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input boostrap rounds. </p>

</div>
</div>
<a id="aa7a4a5ac2156c6877cbc0533601724e7" name="aa7a4a5ac2156c6877cbc0533601724e7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa7a4a5ac2156c6877cbc0533601724e7">&#9670;&#160;</a></span>clip_grad_value</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.clip_grad_value</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>clip_grad_value</code>; indicating the clipping range for gradients. </p>

</div>
</div>
<a id="a05e8b2d000bd94f5fb854516c84200e5" name="a05e8b2d000bd94f5fb854516c84200e5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a05e8b2d000bd94f5fb854516c84200e5">&#9670;&#160;</a></span>device</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.device</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>device</code> argument; indicating the device name as device type class. </p>

</div>
</div>
<a id="a1db6c1588e7f4d11b3a723825809411a" name="a1db6c1588e7f4d11b3a723825809411a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1db6c1588e7f4d11b3a723825809411a">&#9670;&#160;</a></span>dim_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.dim_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise. </p>

</div>
</div>
<a id="a9244a2ba98485e2bedf6cee830ca1a22" name="a9244a2ba98485e2bedf6cee830ca1a22"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9244a2ba98485e2bedf6cee830ca1a22">&#9670;&#160;</a></span>distribution</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.distribution</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input distribution object. </p>

</div>
</div>
<a id="a6dc53028e64988f4a9442a047736acb8" name="a6dc53028e64988f4a9442a047736acb8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6dc53028e64988f4a9442a047736acb8">&#9670;&#160;</a></span>dtype</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.dtype</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>dtype</code> argument; indicating the datatype class. </p>

</div>
</div>
<a id="affc2a52ddeb68b4c0a6c05793fa58261" name="affc2a52ddeb68b4c0a6c05793fa58261"></a>
<h2 class="memtitle"><span class="permalink"><a href="#affc2a52ddeb68b4c0a6c05793fa58261">&#9670;&#160;</a></span>entropy_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.entropy_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input entropy coefficient. </p>

</div>
</div>
<a id="a940d6caf441fdc4b732ef0185e2694e9" name="a940d6caf441fdc4b732ef0185e2694e9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a940d6caf441fdc4b732ef0185e2694e9">&#9670;&#160;</a></span>eps_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.eps_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation. </p>

</div>
</div>
<a id="ab09c920ad70ce8b4cd3fa485b2f1bd55" name="ab09c920ad70ce8b4cd3fa485b2f1bd55"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab09c920ad70ce8b4cd3fa485b2f1bd55">&#9670;&#160;</a></span>gamma</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.gamma</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input discounting factor. </p>

</div>
</div>
<a id="a53dd48a82891207362f85da40e84ecf7" name="a53dd48a82891207362f85da40e84ecf7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a53dd48a82891207362f85da40e84ecf7">&#9670;&#160;</a></span>gaussian_noise</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.gaussian_noise</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The PyTorch Normal distribution object initialized with standard mean and std. </p>

</div>
</div>
<a id="aa2bfe032740a7146997162d125274fe9" name="aa2bfe032740a7146997162d125274fe9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2bfe032740a7146997162d125274fe9">&#9670;&#160;</a></span>grad_norm_p</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.grad_norm_p</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings. </p>

</div>
</div>
<a id="a4b605080b32f1199d314ded5efa4230a" name="a4b605080b32f1199d314ded5efa4230a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4b605080b32f1199d314ded5efa4230a">&#9670;&#160;</a></span>is_continuous_action_space</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.is_continuous_action_space</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Flag indicating if action space is continuous or discrete. </p>

</div>
</div>
<a id="af5485148744d68c770b872e62361ca10" name="af5485148744d68c770b872e62361ca10"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af5485148744d68c770b872e62361ca10">&#9670;&#160;</a></span>loss_function</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.loss_function</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input loss function. </p>

</div>
</div>
<a id="a081dcbd51017df1d26abbf15821089d7" name="a081dcbd51017df1d26abbf15821089d7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a081dcbd51017df1d26abbf15821089d7">&#9670;&#160;</a></span>lr_scheduler</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.lr_scheduler</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optional LR Scheduler (this can be None). </p>

</div>
</div>
<a id="a2238587b60c04bf2a7e83e497a030b1e" name="a2238587b60c04bf2a7e83e497a030b1e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2238587b60c04bf2a7e83e497a030b1e">&#9670;&#160;</a></span>lr_threshold</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.lr_threshold</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input LR Threshold. </p>

</div>
</div>
<a id="ae98b91d40567e6e9990304977809d997" name="ae98b91d40567e6e9990304977809d997"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae98b91d40567e6e9990304977809d997">&#9670;&#160;</a></span>max_grad_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.max_grad_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings. </p>

</div>
</div>
<a id="aa2f798b74bd64640ef35da8f672f3e95" name="aa2f798b74bd64640ef35da8f672f3e95"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2f798b74bd64640ef35da8f672f3e95">&#9670;&#160;</a></span>optimizer</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.optimizer</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optimizer wrapped with policy_model parameters. </p>

</div>
</div>
<a id="acde06241652873c70e7d4d9523eab0b0" name="acde06241652873c70e7d4d9523eab0b0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acde06241652873c70e7d4d9523eab0b0">&#9670;&#160;</a></span>p_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.p_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation. </p>

</div>
</div>
<a id="a3c0e7b3a8cc0a96cebe026d304c5590c" name="a3c0e7b3a8cc0a96cebe026d304c5590c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3c0e7b3a8cc0a96cebe026d304c5590c">&#9670;&#160;</a></span>policy_model</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.policy_model</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input policy model moved to desired device. </p>

</div>
</div>
<a id="a00c1fb74f19174eecd6fcd03dc0b9843" name="a00c1fb74f19174eecd6fcd03dc0b9843"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00c1fb74f19174eecd6fcd03dc0b9843">&#9670;&#160;</a></span>save_path</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.save_path</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input save path for backing up agent models. </p>

</div>
</div>
<a id="a073e3f0455059d575ce1bc9a28b3eee5" name="a073e3f0455059d575ce1bc9a28b3eee5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a073e3f0455059d575ce1bc9a28b3eee5">&#9670;&#160;</a></span>state_value_coefficient</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.state_value_coefficient</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input state value coefficient. </p>

</div>
</div>
<a id="a42a2685035dad0c95918126682a0d4ff" name="a42a2685035dad0c95918126682a0d4ff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a42a2685035dad0c95918126682a0d4ff">&#9670;&#160;</a></span>step_counter</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.step_counter</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The step counter; counting the total timesteps done so far. </p>

</div>
</div>
<a id="af4c9087867a69b26c46e1c740d71f2b8" name="af4c9087867a69b26c46e1c740d71f2b8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af4c9087867a69b26c46e1c740d71f2b8">&#9670;&#160;</a></span>variance_decay_fn</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.variance_decay_fn</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The variance decay method. </p>
<p >This will be None if <code>variance</code> argument was not passed </p>

</div>
</div>
<a id="a401e5c40d7a31b3cc24c38dbcf40da92" name="a401e5c40d7a31b3cc24c38dbcf40da92"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a401e5c40d7a31b3cc24c38dbcf40da92">&#9670;&#160;</a></span>variance_value</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.actor_critic.utils.actor_critic_agent.ActorCriticAgent.variance_value</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The current variance value. </p>
<p >This will be None if <code>variance</code> argument was not passed </p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacerlpack.html">rlpack</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic.html">actor_critic</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic_1_1utils.html">utils</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent.html">actor_critic_agent</a></li><li class="navelem"><a class="el" href="classrlpack_1_1actor__critic_1_1utils_1_1actor__critic__agent_1_1_actor_critic_agent.html">ActorCriticAgent</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
