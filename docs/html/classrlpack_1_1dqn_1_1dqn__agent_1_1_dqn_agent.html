<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RLPack: rlpack.dqn.dqn_agent.DqnAgent Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="RLPack-logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">RLPack
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Data Fields</a> &#124;
<a href="#pri-methods">Private Member Functions</a> &#124;
<a href="#pri-attribs">Private Attributes</a>  </div>
  <div class="headertitle"><div class="title">rlpack.dqn.dqn_agent.DqnAgent Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>This class implements the basic DQN methodology, i.e.  
 <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#details">More...</a></p>
<div id="dynsection-0" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-0-trigger" src="closed.png" alt="+"/> Inheritance diagram for rlpack.dqn.dqn_agent.DqnAgent:</div>
<div id="dynsection-0-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-0-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent__inherit__graph.png" border="0" usemap="#arlpack_8dqn_8dqn__agent_8_dqn_agent_inherit__map" alt="Inheritance graph"/></div>
<map name="arlpack_8dqn_8dqn__agent_8_dqn_agent_inherit__map" id="arlpack_8dqn_8dqn__agent_8_dqn_agent_inherit__map">
<area shape="rect" title="This class implements the basic DQN methodology, i.e." alt="" coords="316,53,491,93"/>
<area shape="rect" href="classrlpack_1_1dqn_1_1dqn__proportional__prioritization__agent_1_1_dqn_proportional_prioritization_agent.html" title="This class implements the DQN with Proportional prioritization strategy." alt="" coords="553,5,787,60"/>
<area shape="rect" href="classrlpack_1_1dqn_1_1dqn__rank__based__prioritization__agent_1_1_dqn_rank_based_prioritization_agent.html" title="This class implements the DQN with Rank&#45;Based prioritization strategy." alt="" coords="539,84,801,139"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="111,53,268,93"/>
<area shape="rect" title=" " alt="" coords="5,60,63,85"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<div id="dynsection-1" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-1-trigger" src="closed.png" alt="+"/> Collaboration diagram for rlpack.dqn.dqn_agent.DqnAgent:</div>
<div id="dynsection-1-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-1-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent__coll__graph.png" border="0" usemap="#arlpack_8dqn_8dqn__agent_8_dqn_agent_coll__map" alt="Collaboration graph"/></div>
<map name="arlpack_8dqn_8dqn__agent_8_dqn_agent_coll__map" id="arlpack_8dqn_8dqn__agent_8_dqn_agent_coll__map">
<area shape="rect" title="This class implements the basic DQN methodology, i.e." alt="" coords="5,167,180,207"/>
<area shape="rect" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html" title="The base class for all agents." alt="" coords="14,79,171,119"/>
<area shape="rect" title=" " alt="" coords="64,5,121,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:adaafce130393bf44eadb52278cf1191c"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#adaafce130393bf44eadb52278cf1191c">__init__</a> (self, pytorch.nn.Module <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aba41d3fc796d221f77fc8e501b01e86e">target_model</a>, pytorch.nn.Module <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aa8adcc90bb59c4bd0f1a153f89c3116d">policy_model</a>, pytorch.optim.Optimizer <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a5bc53e8b41e11abbadb20139877910c7">optimizer</a>, Union[LRScheduler, None] <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac449f6bbe49ee6fb8005bb232b653147">lr_scheduler</a>, LossFunction <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a16b0b31d7e47cf9c56f09cf2b1c6c733">loss_function</a>, float <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a8599ea966ed65fd8cb9a1731b0b8cee6">gamma</a>, float <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a4c8074b05362bbcf9c2518938e43ce24">epsilon</a>, float <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a8983773caff8991f5a1429d44fbdd974">min_epsilon</a>, float <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a32122333d34d04772bc5a5882923c8fe">epsilon_decay_rate</a>, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ad9c0ee89d768180a64862811f202d57f">epsilon_decay_frequency</a>, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac4dd2c4a11614357b3215c406c97b90f">memory_buffer_size</a>, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac2f64d463f70a32e203e79d069722c84">target_model_update_rate</a>, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a37d7273d3b9e856c3858c34b6034c770">policy_model_update_rate</a>, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aeb63fe78cd7970482dd191f9186a54b4">backup_frequency</a>, float <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#adf407eb3047d5241143aad5eab202ee6">lr_threshold</a>, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a0dcef173b93afb6eeb2c2f2bbed8fe7d">batch_size</a>, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a5a6cabaa890c0f28fbedbce96869a500">num_actions</a>, str <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a7ebb3729ba7414e487b839ffa2198f7a">save_path</a>, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a1e74b6687e80197fc0764726533899bb">bootstrap_rounds</a>=1, str <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aa24c4cde1b3c86793aa0c163975f7e46">device</a>=&quot;cpu&quot;, str dtype=&quot;float32&quot;, Optional[Dict[str, Any]] <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aecf463bba48c45f10bdf5ba8d8d978bc">prioritization_params</a>=None, float <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ad8dae08b557f3cd26031efe5abc65225">force_terminal_state_selection_prob</a>=0.0, float <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a2a641f2954d39a51703ce14d3490ffd5">tau</a>=1.0, Union[int, str] <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#add872066ef9a8e4f43bbcb765fb66103">apply_norm</a>=-1, Union[int, List[str]] <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a65bbfa35c0a6c51cc2d7500a118aff5e">apply_norm_to</a>=-1, float <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac2f3ca72d4ac525130ddab5c3ab08f3c">eps_for_norm</a>=5e-12, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a9e82951d76c6c5db30d23c13d5286981">p_for_norm</a>=2, int <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ab587b11f42305490ece52cb5683cee6d">dim_for_norm</a>=0, Optional[float] <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a634dc4ae6c73036f3fbe223929765dcf">max_grad_norm</a>=None, float <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a1e27aba2fbce8d9ec056ae95fd97f1f1">grad_norm_p</a>=2.0, Optional[float] <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a7c4db3301269ef487097c5d7a89e0f70">clip_grad_value</a>=None)</td></tr>
<tr class="separator:adaafce130393bf44eadb52278cf1191c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adfd9b38d7e3b4f55eb7f857b0285ae0c"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#adfd9b38d7e3b4f55eb7f857b0285ae0c">load</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:adfd9b38d7e3b4f55eb7f857b0285ae0c"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#adfd9b38d7e3b4f55eb7f857b0285ae0c">More...</a><br /></td></tr>
<tr class="separator:adfd9b38d7e3b4f55eb7f857b0285ae0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4aef4b46067746d31a0015fa71698440"><td class="memItemLeft" align="right" valign="top">np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a4aef4b46067746d31a0015fa71698440">policy</a> (self, Union[ndarray, pytorch.Tensor, List[float]] state_current)</td></tr>
<tr class="memdesc:a4aef4b46067746d31a0015fa71698440"><td class="mdescLeft">&#160;</td><td class="mdescRight">The policy for the agent.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a4aef4b46067746d31a0015fa71698440">More...</a><br /></td></tr>
<tr class="separator:a4aef4b46067746d31a0015fa71698440"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aabbebafa737394e23228307873976a6b"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aabbebafa737394e23228307873976a6b">save</a> (self, Optional[str] custom_name_suffix=None)</td></tr>
<tr class="memdesc:aabbebafa737394e23228307873976a6b"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>).  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aabbebafa737394e23228307873976a6b">More...</a><br /></td></tr>
<tr class="separator:aabbebafa737394e23228307873976a6b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab70a7ecba4b187924c1886fc1060a341"><td class="memItemLeft" align="right" valign="top">np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ab70a7ecba4b187924c1886fc1060a341">train</a> (self, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_current, Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]] state_next, Union[int, float] reward, Union[int, float] action, Union[bool, int] done, Optional[Union[pytorch.Tensor, np.ndarray, float]] priority=1.0, Optional[Union[pytorch.Tensor, np.ndarray, float]] probability=1.0, Optional[Union[pytorch.Tensor, np.ndarray, float]] weight=1.0)</td></tr>
<tr class="separator:ab70a7ecba4b187924c1886fc1060a341"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Dict[str, Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">__getstate__</a> (self)</td></tr>
<tr class="memdesc:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To get the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#accaa47a12b6f65fee88824d3018b8c8e">More...</a><br /></td></tr>
<tr class="separator:accaa47a12b6f65fee88824d3018b8c8e inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">__init__</a> (self)</td></tr>
<tr class="memdesc:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The class initializer.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">More...</a><br /></td></tr>
<tr class="separator:a364aa41c59de32a363b2e4c241dfef3f inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">__setstate__</a> (self, Dict[str, Any] state)</td></tr>
<tr class="memdesc:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">To load the agent's current state (dict of attributes).  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a04286fc7bb9ca0a64bce5eaf3620db7b">More...</a><br /></td></tr>
<tr class="separator:a04286fc7bb9ca0a64bce5eaf3620db7b inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">load</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">More...</a><br /></td></tr>
<tr class="separator:ac493d40ce8bd5562822a01aba0265181 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">policy</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Policy method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">More...</a><br /></td></tr>
<tr class="separator:ab5e3e4db83e80ef7bb422a148cd3e1f6 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">save</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">More...</a><br /></td></tr>
<tr class="separator:aa61ea2248a43a7bbc9b7c9ab7c240564 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">train</a> (self, *args, **kwargs)</td></tr>
<tr class="memdesc:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">Training method for the agent.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">More...</a><br /></td></tr>
<tr class="separator:a38c313422ef6c713efd5ef9301b35111 inherit pub_methods_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Data Fields</h2></td></tr>
<tr class="memitem:add872066ef9a8e4f43bbcb765fb66103"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#add872066ef9a8e4f43bbcb765fb66103">apply_norm</a></td></tr>
<tr class="memdesc:add872066ef9a8e4f43bbcb765fb66103"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm</code> argument; indicating the normalisation to be used.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#add872066ef9a8e4f43bbcb765fb66103">More...</a><br /></td></tr>
<tr class="separator:add872066ef9a8e4f43bbcb765fb66103"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a65bbfa35c0a6c51cc2d7500a118aff5e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a65bbfa35c0a6c51cc2d7500a118aff5e">apply_norm_to</a></td></tr>
<tr class="memdesc:a65bbfa35c0a6c51cc2d7500a118aff5e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>apply_norm_to</code> argument; indicating the quantity to normalise.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a65bbfa35c0a6c51cc2d7500a118aff5e">More...</a><br /></td></tr>
<tr class="separator:a65bbfa35c0a6c51cc2d7500a118aff5e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeb63fe78cd7970482dd191f9186a54b4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aeb63fe78cd7970482dd191f9186a54b4">backup_frequency</a></td></tr>
<tr class="memdesc:aeb63fe78cd7970482dd191f9186a54b4"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input model backup frequency in terms of timesteps.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aeb63fe78cd7970482dd191f9186a54b4">More...</a><br /></td></tr>
<tr class="separator:aeb63fe78cd7970482dd191f9186a54b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0dcef173b93afb6eeb2c2f2bbed8fe7d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a0dcef173b93afb6eeb2c2f2bbed8fe7d">batch_size</a></td></tr>
<tr class="memdesc:a0dcef173b93afb6eeb2c2f2bbed8fe7d"><td class="mdescLeft">&#160;</td><td class="mdescRight">The batch size to be used when training policy model.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a0dcef173b93afb6eeb2c2f2bbed8fe7d">More...</a><br /></td></tr>
<tr class="separator:a0dcef173b93afb6eeb2c2f2bbed8fe7d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1e74b6687e80197fc0764726533899bb"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a1e74b6687e80197fc0764726533899bb">bootstrap_rounds</a></td></tr>
<tr class="memdesc:a1e74b6687e80197fc0764726533899bb"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input boostrap rounds.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a1e74b6687e80197fc0764726533899bb">More...</a><br /></td></tr>
<tr class="separator:a1e74b6687e80197fc0764726533899bb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7c4db3301269ef487097c5d7a89e0f70"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a7c4db3301269ef487097c5d7a89e0f70">clip_grad_value</a></td></tr>
<tr class="memdesc:a7c4db3301269ef487097c5d7a89e0f70"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>clip_grad_value</code>; indicating the clipping range for gradients.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a7c4db3301269ef487097c5d7a89e0f70">More...</a><br /></td></tr>
<tr class="separator:a7c4db3301269ef487097c5d7a89e0f70"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa24c4cde1b3c86793aa0c163975f7e46"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aa24c4cde1b3c86793aa0c163975f7e46">device</a></td></tr>
<tr class="memdesc:aa24c4cde1b3c86793aa0c163975f7e46"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>device</code> argument; indicating the device name.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aa24c4cde1b3c86793aa0c163975f7e46">More...</a><br /></td></tr>
<tr class="separator:aa24c4cde1b3c86793aa0c163975f7e46"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab587b11f42305490ece52cb5683cee6d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ab587b11f42305490ece52cb5683cee6d">dim_for_norm</a></td></tr>
<tr class="memdesc:ab587b11f42305490ece52cb5683cee6d"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ab587b11f42305490ece52cb5683cee6d">More...</a><br /></td></tr>
<tr class="separator:ab587b11f42305490ece52cb5683cee6d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac2f3ca72d4ac525130ddab5c3ab08f3c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac2f3ca72d4ac525130ddab5c3ab08f3c">eps_for_norm</a></td></tr>
<tr class="memdesc:ac2f3ca72d4ac525130ddab5c3ab08f3c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac2f3ca72d4ac525130ddab5c3ab08f3c">More...</a><br /></td></tr>
<tr class="separator:ac2f3ca72d4ac525130ddab5c3ab08f3c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4c8074b05362bbcf9c2518938e43ce24"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a4c8074b05362bbcf9c2518938e43ce24">epsilon</a></td></tr>
<tr class="memdesc:a4c8074b05362bbcf9c2518938e43ce24"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input exploration factor.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a4c8074b05362bbcf9c2518938e43ce24">More...</a><br /></td></tr>
<tr class="separator:a4c8074b05362bbcf9c2518938e43ce24"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad9c0ee89d768180a64862811f202d57f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ad9c0ee89d768180a64862811f202d57f">epsilon_decay_frequency</a></td></tr>
<tr class="memdesc:ad9c0ee89d768180a64862811f202d57f"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input epsilon decay frequency in terms of timesteps.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ad9c0ee89d768180a64862811f202d57f">More...</a><br /></td></tr>
<tr class="separator:ad9c0ee89d768180a64862811f202d57f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a32122333d34d04772bc5a5882923c8fe"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a32122333d34d04772bc5a5882923c8fe">epsilon_decay_rate</a></td></tr>
<tr class="memdesc:a32122333d34d04772bc5a5882923c8fe"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input epsilon decay rate.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a32122333d34d04772bc5a5882923c8fe">More...</a><br /></td></tr>
<tr class="separator:a32122333d34d04772bc5a5882923c8fe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad8dae08b557f3cd26031efe5abc65225"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ad8dae08b557f3cd26031efe5abc65225">force_terminal_state_selection_prob</a></td></tr>
<tr class="memdesc:ad8dae08b557f3cd26031efe5abc65225"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>force_terminal_state_selection_prob</code>.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ad8dae08b557f3cd26031efe5abc65225">More...</a><br /></td></tr>
<tr class="separator:ad8dae08b557f3cd26031efe5abc65225"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8599ea966ed65fd8cb9a1731b0b8cee6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a8599ea966ed65fd8cb9a1731b0b8cee6">gamma</a></td></tr>
<tr class="memdesc:a8599ea966ed65fd8cb9a1731b0b8cee6"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input discounting factor.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a8599ea966ed65fd8cb9a1731b0b8cee6">More...</a><br /></td></tr>
<tr class="separator:a8599ea966ed65fd8cb9a1731b0b8cee6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1e27aba2fbce8d9ec056ae95fd97f1f1"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a1e27aba2fbce8d9ec056ae95fd97f1f1">grad_norm_p</a></td></tr>
<tr class="memdesc:a1e27aba2fbce8d9ec056ae95fd97f1f1"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a1e27aba2fbce8d9ec056ae95fd97f1f1">More...</a><br /></td></tr>
<tr class="separator:a1e27aba2fbce8d9ec056ae95fd97f1f1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a16b0b31d7e47cf9c56f09cf2b1c6c733"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a16b0b31d7e47cf9c56f09cf2b1c6c733">loss_function</a></td></tr>
<tr class="memdesc:a16b0b31d7e47cf9c56f09cf2b1c6c733"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input loss function.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a16b0b31d7e47cf9c56f09cf2b1c6c733">More...</a><br /></td></tr>
<tr class="separator:a16b0b31d7e47cf9c56f09cf2b1c6c733"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac449f6bbe49ee6fb8005bb232b653147"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac449f6bbe49ee6fb8005bb232b653147">lr_scheduler</a></td></tr>
<tr class="memdesc:ac449f6bbe49ee6fb8005bb232b653147"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optional LR Scheduler (this can be None).  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac449f6bbe49ee6fb8005bb232b653147">More...</a><br /></td></tr>
<tr class="separator:ac449f6bbe49ee6fb8005bb232b653147"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adf407eb3047d5241143aad5eab202ee6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#adf407eb3047d5241143aad5eab202ee6">lr_threshold</a></td></tr>
<tr class="memdesc:adf407eb3047d5241143aad5eab202ee6"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input LR Threshold.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#adf407eb3047d5241143aad5eab202ee6">More...</a><br /></td></tr>
<tr class="separator:adf407eb3047d5241143aad5eab202ee6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a634dc4ae6c73036f3fbe223929765dcf"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a634dc4ae6c73036f3fbe223929765dcf">max_grad_norm</a></td></tr>
<tr class="memdesc:a634dc4ae6c73036f3fbe223929765dcf"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a634dc4ae6c73036f3fbe223929765dcf">More...</a><br /></td></tr>
<tr class="separator:a634dc4ae6c73036f3fbe223929765dcf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9855b6ae475270ab00627e7f9669adc2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a9855b6ae475270ab00627e7f9669adc2">memory</a></td></tr>
<tr class="memdesc:a9855b6ae475270ab00627e7f9669adc2"><td class="mdescLeft">&#160;</td><td class="mdescRight">The instance of <a class="el" href="classrlpack_1_1___c_1_1memory_1_1_memory.html">rlpack._C.memory.Memory</a> used for Replay buffer.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a9855b6ae475270ab00627e7f9669adc2">More...</a><br /></td></tr>
<tr class="separator:a9855b6ae475270ab00627e7f9669adc2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac4dd2c4a11614357b3215c406c97b90f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac4dd2c4a11614357b3215c406c97b90f">memory_buffer_size</a></td></tr>
<tr class="memdesc:ac4dd2c4a11614357b3215c406c97b90f"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input argument <code>memory_buffer_size</code>; indicating the buffer size used.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac4dd2c4a11614357b3215c406c97b90f">More...</a><br /></td></tr>
<tr class="separator:ac4dd2c4a11614357b3215c406c97b90f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8983773caff8991f5a1429d44fbdd974"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a8983773caff8991f5a1429d44fbdd974">min_epsilon</a></td></tr>
<tr class="memdesc:a8983773caff8991f5a1429d44fbdd974"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input minimum exploration factor after decays.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a8983773caff8991f5a1429d44fbdd974">More...</a><br /></td></tr>
<tr class="separator:a8983773caff8991f5a1429d44fbdd974"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5a6cabaa890c0f28fbedbce96869a500"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a5a6cabaa890c0f28fbedbce96869a500">num_actions</a></td></tr>
<tr class="memdesc:a5a6cabaa890c0f28fbedbce96869a500"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input number of actions.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a5a6cabaa890c0f28fbedbce96869a500">More...</a><br /></td></tr>
<tr class="separator:a5a6cabaa890c0f28fbedbce96869a500"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5bc53e8b41e11abbadb20139877910c7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a5bc53e8b41e11abbadb20139877910c7">optimizer</a></td></tr>
<tr class="memdesc:a5bc53e8b41e11abbadb20139877910c7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input optimizer wrapped with policy_model parameters.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a5bc53e8b41e11abbadb20139877910c7">More...</a><br /></td></tr>
<tr class="separator:a5bc53e8b41e11abbadb20139877910c7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9e82951d76c6c5db30d23c13d5286981"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a9e82951d76c6c5db30d23c13d5286981">p_for_norm</a></td></tr>
<tr class="memdesc:a9e82951d76c6c5db30d23c13d5286981"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a9e82951d76c6c5db30d23c13d5286981">More...</a><br /></td></tr>
<tr class="separator:a9e82951d76c6c5db30d23c13d5286981"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa8adcc90bb59c4bd0f1a153f89c3116d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aa8adcc90bb59c4bd0f1a153f89c3116d">policy_model</a></td></tr>
<tr class="memdesc:aa8adcc90bb59c4bd0f1a153f89c3116d"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input policy model.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aa8adcc90bb59c4bd0f1a153f89c3116d">More...</a><br /></td></tr>
<tr class="separator:aa8adcc90bb59c4bd0f1a153f89c3116d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a37d7273d3b9e856c3858c34b6034c770"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a37d7273d3b9e856c3858c34b6034c770">policy_model_update_rate</a></td></tr>
<tr class="memdesc:a37d7273d3b9e856c3858c34b6034c770"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input argument <code>policy_model_update_rate</code>; indicating the update rate of policy model.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a37d7273d3b9e856c3858c34b6034c770">More...</a><br /></td></tr>
<tr class="separator:a37d7273d3b9e856c3858c34b6034c770"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aecf463bba48c45f10bdf5ba8d8d978bc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aecf463bba48c45f10bdf5ba8d8d978bc">prioritization_params</a></td></tr>
<tr class="memdesc:aecf463bba48c45f10bdf5ba8d8d978bc"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input prioritization parameters.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aecf463bba48c45f10bdf5ba8d8d978bc">More...</a><br /></td></tr>
<tr class="separator:aecf463bba48c45f10bdf5ba8d8d978bc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7ebb3729ba7414e487b839ffa2198f7a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a7ebb3729ba7414e487b839ffa2198f7a">save_path</a></td></tr>
<tr class="memdesc:a7ebb3729ba7414e487b839ffa2198f7a"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input save path for backing up agent models.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a7ebb3729ba7414e487b839ffa2198f7a">More...</a><br /></td></tr>
<tr class="separator:a7ebb3729ba7414e487b839ffa2198f7a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1ba7d14eb2a55b43b904c7f89ff37148"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a1ba7d14eb2a55b43b904c7f89ff37148">step_counter</a></td></tr>
<tr class="memdesc:a1ba7d14eb2a55b43b904c7f89ff37148"><td class="mdescLeft">&#160;</td><td class="mdescRight">The step counter; counting the total timesteps done so far up to <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac4dd2c4a11614357b3215c406c97b90f">memory_buffer_size</a>.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a1ba7d14eb2a55b43b904c7f89ff37148">More...</a><br /></td></tr>
<tr class="separator:a1ba7d14eb2a55b43b904c7f89ff37148"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aba41d3fc796d221f77fc8e501b01e86e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aba41d3fc796d221f77fc8e501b01e86e">target_model</a></td></tr>
<tr class="memdesc:aba41d3fc796d221f77fc8e501b01e86e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input target model.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aba41d3fc796d221f77fc8e501b01e86e">More...</a><br /></td></tr>
<tr class="separator:aba41d3fc796d221f77fc8e501b01e86e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac2f64d463f70a32e203e79d069722c84"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac2f64d463f70a32e203e79d069722c84">target_model_update_rate</a></td></tr>
<tr class="memdesc:ac2f64d463f70a32e203e79d069722c84"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input argument <code>target_model_update_rate</code>; indicating the update rate of target model.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac2f64d463f70a32e203e79d069722c84">More...</a><br /></td></tr>
<tr class="separator:ac2f64d463f70a32e203e79d069722c84"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2a641f2954d39a51703ce14d3490ffd5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a2a641f2954d39a51703ce14d3490ffd5">tau</a></td></tr>
<tr class="memdesc:a2a641f2954d39a51703ce14d3490ffd5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input <code>tau</code>; indicating the soft update used to update <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aba41d3fc796d221f77fc8e501b01e86e">target_model</a> parameters.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a2a641f2954d39a51703ce14d3490ffd5">More...</a><br /></td></tr>
<tr class="separator:a2a641f2954d39a51703ce14d3490ffd5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent')"><img src="closed.png" alt="-"/>&#160;Data Fields inherited from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html">rlpack.utils.base.agent.Agent</a></td></tr>
<tr class="memitem:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4104e76aed3a37b4df4ef1069383aee8">gamma</a></td></tr>
<tr class="memdesc:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The default discounting factor for agents.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4104e76aed3a37b4df4ef1069383aee8">More...</a><br /></td></tr>
<tr class="separator:a4104e76aed3a37b4df4ef1069383aee8 inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">loss</a></td></tr>
<tr class="memdesc:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of losses accumulated after each backward call.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab3ce395269c69c095865fee40818db2e">More...</a><br /></td></tr>
<tr class="separator:ab3ce395269c69c095865fee40818db2e inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">save_path</a></td></tr>
<tr class="memdesc:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="mdescLeft">&#160;</td><td class="mdescRight">The path to save agent states and models.  <a href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a4779a7186807d901e8ffc8cc8951527a">More...</a><br /></td></tr>
<tr class="separator:a4779a7186807d901e8ffc8cc8951527a inherit pub_attribs_classrlpack_1_1utils_1_1base_1_1agent_1_1_agent"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-methods" name="pri-methods"></a>
Private Member Functions</h2></td></tr>
<tr class="memitem:a0d9b14c38d0d94d9daf6b98d14631ca7"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a0d9b14c38d0d94d9daf6b98d14631ca7">_anneal_alpha</a> (self)</td></tr>
<tr class="separator:a0d9b14c38d0d94d9daf6b98d14631ca7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad8de8220f415d66696d1d42855f4ea97"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ad8de8220f415d66696d1d42855f4ea97">_anneal_beta</a> (self)</td></tr>
<tr class="separator:ad8de8220f415d66696d1d42855f4ea97"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3968de174bcc4879f641136b85f9a876"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a3968de174bcc4879f641136b85f9a876">_apply_prioritization_strategy</a> (self, pytorch.Tensor td_value, pytorch.Tensor random_indices)</td></tr>
<tr class="memdesc:a3968de174bcc4879f641136b85f9a876"><td class="mdescLeft">&#160;</td><td class="mdescRight">Void protected method that applies the relevant prioritization strategy for the DQN.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a3968de174bcc4879f641136b85f9a876">More...</a><br /></td></tr>
<tr class="separator:a3968de174bcc4879f641136b85f9a876"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a49f009c7c02963bf41d79725a42be88d"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a49f009c7c02963bf41d79725a42be88d">_decay_epsilon</a> (self)</td></tr>
<tr class="memdesc:a49f009c7c02963bf41d79725a42be88d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method to decay epsilon.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a49f009c7c02963bf41d79725a42be88d">More...</a><br /></td></tr>
<tr class="separator:a49f009c7c02963bf41d79725a42be88d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a83c6dbcbc01d7051982e8c9d45479b81"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a83c6dbcbc01d7051982e8c9d45479b81">_grad_mean_reduction</a> (self)</td></tr>
<tr class="memdesc:a83c6dbcbc01d7051982e8c9d45479b81"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs mean reduction and assigns the policy model's parameter the mean reduced gradients.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a83c6dbcbc01d7051982e8c9d45479b81">More...</a><br /></td></tr>
<tr class="separator:a83c6dbcbc01d7051982e8c9d45479b81"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab6cf3494ffe6a356e429f1ed01d4f246"><td class="memItemLeft" align="right" valign="top">np.ndarray&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ab6cf3494ffe6a356e429f1ed01d4f246">_infer_action</a> (self, pytorch.Tensor state_current, bool call_from_policy=True)</td></tr>
<tr class="memdesc:ab6cf3494ffe6a356e429f1ed01d4f246"><td class="mdescLeft">&#160;</td><td class="mdescRight">Helper method to support action inference form policy model.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ab6cf3494ffe6a356e429f1ed01d4f246">More...</a><br /></td></tr>
<tr class="separator:ab6cf3494ffe6a356e429f1ed01d4f246"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac42d3b5e16d601cb8fe098cfac5150ca"><td class="memItemLeft" align="right" valign="top">Tuple[ pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor,]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac42d3b5e16d601cb8fe098cfac5150ca">_load_random_experiences</a> (self)</td></tr>
<tr class="memdesc:ac42d3b5e16d601cb8fe098cfac5150ca"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method loads random transitions from memory.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac42d3b5e16d601cb8fe098cfac5150ca">More...</a><br /></td></tr>
<tr class="separator:ac42d3b5e16d601cb8fe098cfac5150ca"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a91c03751ec814405b70c28affd48e3cd"><td class="memItemLeft" align="right" valign="top">pytorch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a91c03751ec814405b70c28affd48e3cd">_temporal_difference</a> (self, pytorch.Tensor rewards, pytorch.Tensor q_values, pytorch.Tensor dones)</td></tr>
<tr class="memdesc:a91c03751ec814405b70c28affd48e3cd"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method computes the temporal difference for given transitions.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a91c03751ec814405b70c28affd48e3cd">More...</a><br /></td></tr>
<tr class="separator:a91c03751ec814405b70c28affd48e3cd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a268d4194463b2d907864232da38ab95d"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a268d4194463b2d907864232da38ab95d">_train_policy_model</a> (self)</td></tr>
<tr class="memdesc:a268d4194463b2d907864232da38ab95d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method of the class to train the policy model.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a268d4194463b2d907864232da38ab95d">More...</a><br /></td></tr>
<tr class="separator:a268d4194463b2d907864232da38ab95d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aed71771ef5953d2926a911d70c41152c"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aed71771ef5953d2926a911d70c41152c">_update_target_model</a> (self)</td></tr>
<tr class="memdesc:aed71771ef5953d2926a911d70c41152c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method of the class to update the target model.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aed71771ef5953d2926a911d70c41152c">More...</a><br /></td></tr>
<tr class="separator:aed71771ef5953d2926a911d70c41152c"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-attribs" name="pri-attribs"></a>
Private Attributes</h2></td></tr>
<tr class="memitem:a9ed207f175d082efc510ccd9ada18fc5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a9ed207f175d082efc510ccd9ada18fc5">__prioritization_strategy_code</a></td></tr>
<tr class="memdesc:a9ed207f175d082efc510ccd9ada18fc5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The prioritization strategy code.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a9ed207f175d082efc510ccd9ada18fc5">More...</a><br /></td></tr>
<tr class="separator:a9ed207f175d082efc510ccd9ada18fc5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a37ae5ac7561ac6f7e276fb3b4b0bb320"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a37ae5ac7561ac6f7e276fb3b4b0bb320">_grad_accumulator</a></td></tr>
<tr class="memdesc:a37ae5ac7561ac6f7e276fb3b4b0bb320"><td class="mdescLeft">&#160;</td><td class="mdescRight">The list of gradients from each backward call.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a37ae5ac7561ac6f7e276fb3b4b0bb320">More...</a><br /></td></tr>
<tr class="separator:a37ae5ac7561ac6f7e276fb3b4b0bb320"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9c2100fbbd42aff6e85fe8b199b393e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ab9c2100fbbd42aff6e85fe8b199b393e">_normalization</a></td></tr>
<tr class="memdesc:ab9c2100fbbd42aff6e85fe8b199b393e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The normalisation tool to be used for agent.  <a href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ab9c2100fbbd42aff6e85fe8b199b393e">More...</a><br /></td></tr>
<tr class="separator:ab9c2100fbbd42aff6e85fe8b199b393e"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >This class implements the basic DQN methodology, i.e. </p>
<p >DQN without prioritization. This class also acts as a base class for other DQN variants all of which override the method <code>__apply_prioritization_strategy</code> to implement their prioritization strategy. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="adaafce130393bf44eadb52278cf1191c" name="adaafce130393bf44eadb52278cf1191c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adaafce130393bf44eadb52278cf1191c">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.dqn.dqn_agent.DqnAgent.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.nn.Module&#160;</td>
          <td class="paramname"><em>target_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.nn.Module&#160;</td>
          <td class="paramname"><em>policy_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.optim.Optimizer&#160;</td>
          <td class="paramname"><em>optimizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[LRScheduler, None]&#160;</td>
          <td class="paramname"><em>lr_scheduler</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">LossFunction&#160;</td>
          <td class="paramname"><em>loss_function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>epsilon</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>min_epsilon</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>epsilon_decay_rate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>epsilon_decay_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>memory_buffer_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>target_model_update_rate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>policy_model_update_rate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>backup_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>lr_threshold</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>batch_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>save_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>bootstrap_rounds</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>device</em> = <code>&quot;cpu&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>dtype</em> = <code>&quot;float32&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Dict[str, Any]] &#160;</td>
          <td class="paramname"><em>prioritization_params</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>force_terminal_state_selection_prob</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>tau</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, str] &#160;</td>
          <td class="paramname"><em>apply_norm</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, List[str]] &#160;</td>
          <td class="paramname"><em>apply_norm_to</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>eps_for_norm</em> = <code>5e-12</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>p_for_norm</em> = <code>2</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>dim_for_norm</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>max_grad_norm</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>grad_norm_p</em> = <code>2.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>clip_grad_value</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">target_model</td><td>nn.Module: The target network for DQN model. This the network which has its weights frozen. </td></tr>
    <tr><td class="paramname">policy_model</td><td>nn.Module: The policy network for DQN model. This is the network which is trained. </td></tr>
    <tr><td class="paramname">optimizer</td><td>optim.Optimizer: The optimizer wrapped with policy model's parameters. </td></tr>
    <tr><td class="paramname">lr_scheduler</td><td>Union[LRScheduler, None]: The PyTorch LR Scheduler with wrapped optimizer. </td></tr>
    <tr><td class="paramname">loss_function</td><td>LossFunction: The loss function from PyTorch's nn module. Initialized instance must be passed. </td></tr>
    <tr><td class="paramname">gamma</td><td>float: The gamma value for agent. </td></tr>
    <tr><td class="paramname">epsilon</td><td>float: The initial epsilon for the agent. </td></tr>
    <tr><td class="paramname">min_epsilon</td><td>float: The minimum epsilon for the agent. Once this value is reached, it is maintained for all further episodes. </td></tr>
    <tr><td class="paramname">epsilon_decay_rate</td><td>float: The decay multiplier to decay the epsilon. </td></tr>
    <tr><td class="paramname">epsilon_decay_frequency</td><td>int: The number of timesteps after which the epsilon is decayed. </td></tr>
    <tr><td class="paramname">memory_buffer_size</td><td>int: The buffer size of memory; or replay buffer for DQN. </td></tr>
    <tr><td class="paramname">target_model_update_rate</td><td>int: The timesteps after which target model's weights are updated with policy model weights: weights are weighted as per <code>tau</code>: see below)). </td></tr>
    <tr><td class="paramname">policy_model_update_rate</td><td>int: The timesteps after which policy model is trained. This involves backpropagation through the policy network. </td></tr>
    <tr><td class="paramname">backup_frequency</td><td>int: The timesteps after which models are backed up. This will also save optimizer, lr_scheduler and agent_states: epsilon the time of saving and memory. </td></tr>
    <tr><td class="paramname">lr_threshold</td><td>float: The threshold LR which once reached LR scheduler is not called further. </td></tr>
    <tr><td class="paramname">batch_size</td><td>int: The batch size used for inference through target_model and train through policy model </td></tr>
    <tr><td class="paramname">num_actions</td><td>int: Number of actions for the environment. </td></tr>
    <tr><td class="paramname">save_path</td><td>str: The save path for models: target_model and policy_model, optimizer, lr_scheduler and agent_states. </td></tr>
    <tr><td class="paramname">bootstrap_rounds</td><td>int: The number of rounds until which gradients are to be accumulated before performing calling optimizer step. Gradients are mean reduced for bootstrap_rounds &gt; 1. Default: 1. </td></tr>
    <tr><td class="paramname">device</td><td>str: The device on which models are run. Default: "cpu". </td></tr>
    <tr><td class="paramname">dtype</td><td>str: The datatype for model parameters. Default: "float32" </td></tr>
    <tr><td class="paramname">prioritization_params</td><td>Optional[Dict[str, Any]]: The parameters for prioritization in prioritized memory: or relay buffer). Default: None. </td></tr>
    <tr><td class="paramname">force_terminal_state_selection_prob</td><td>float: The probability for forcefully selecting a terminal state in a batch. Default: 0.0. </td></tr>
    <tr><td class="paramname">tau</td><td>float: The weighted update of weights from policy_model to target_model. This is done by formula target_weight = tau * policy_weight +: 1 - tau) * target_weight/. Default: -1. </td></tr>
    <tr><td class="paramname">apply_norm</td><td>Union[int, str]: The code to select the normalization procedure to be applied on selected quantities; selected by <code>apply_norm_to</code>: see below)). Direct string can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1 </td></tr>
    <tr><td class="paramname">apply_norm_to</td><td>Union[int, List[str]]: The code to select the quantity to which normalization is to be applied. Direct list of quantities can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1. </td></tr>
    <tr><td class="paramname">eps_for_norm</td><td>float: Epsilon value for normalization: for numeric stability. For min-max normalization and standardized normalization. Default: 5e-12. </td></tr>
    <tr><td class="paramname">p_for_norm</td><td>int: The p value for p-normalization. Default: 2: L2 Norm. </td></tr>
    <tr><td class="paramname">dim_for_norm</td><td>int: The dimension across which normalization is to be performed. Default: 0. </td></tr>
    <tr><td class="paramname">max_grad_norm</td><td>Optional[float]: The max norm for gradients for gradient clipping. Default: None </td></tr>
    <tr><td class="paramname">grad_norm_p</td><td>Optional[float]: The p-value for p-normalization of gradients. Default: 2.0. </td></tr>
    <tr><td class="paramname">clip_grad_value</td><td>Optional[float]: The gradient value for clipping gradients by value. Default: None</td></tr>
  </table>
  </dd>
</dl>
<p><b>Notes</b></p>
<p >The codes for <code>apply_norm</code> are given as follows: -</p><ul>
<li>No Normalization: -1; (<code>"none"</code>)</li>
<li>Min-Max Normalization: 0; (<code>"min_max"</code>)</li>
<li>Standardization: 1; (<code>"standardize"</code>)</li>
<li>P-Normalization: 2; (<code>"p_norm"</code>)</li>
</ul>
<p >The codes for <code>apply_norm_to</code> are given as follows:</p><ul>
<li>No Normalization: -1; (<code>["none"]</code>)</li>
<li>On States only: 0; (<code>["states"]</code>)</li>
<li>On Rewards only: 1; (<code>["rewards"]</code>)</li>
<li>On TD value only: 2; (<code>["td"]</code>)</li>
<li>On States and Rewards: 3; (<code>["states", "rewards"]</code>)</li>
<li>On States and TD: 4; (<code>["states", "td"]</code>)</li>
</ul>
<p >If a valid <code>max_norm_grad</code> is passed, then gradient clipping takes place else gradient clipping step is skipped. If <code>max_norm_grad</code> value was invalid, error will be raised from PyTorch.</p>
<p >If a valid <code>clip_grad_value</code> is passed, then gradients will be clipped by value. If <code>clip_grad_value</code> value was invalid, error will be raised from PyTorch. </p>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a364aa41c59de32a363b2e4c241dfef3f">rlpack.utils.base.agent.Agent</a>.</p>

<p>Reimplemented in <a class="el" href="classrlpack_1_1dqn_1_1dqn__proportional__prioritization__agent_1_1_dqn_proportional_prioritization_agent.html#a77e6554c9445a2f7abe72396efe5191c">rlpack.dqn.dqn_proportional_prioritization_agent.DqnProportionalPrioritizationAgent</a>, and <a class="el" href="classrlpack_1_1dqn_1_1dqn__rank__based__prioritization__agent_1_1_dqn_rank_based_prioritization_agent.html#a56d78e7bfc65b6e1d77cc8a7bb449557">rlpack.dqn.dqn_rank_based_prioritization_agent.DqnRankBasedPrioritizationAgent</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a0d9b14c38d0d94d9daf6b98d14631ca7" name="a0d9b14c38d0d94d9daf6b98d14631ca7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0d9b14c38d0d94d9daf6b98d14631ca7">&#9670;&#160;</a></span>_anneal_alpha()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.dqn.dqn_agent.DqnAgent._anneal_alpha </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="ad8de8220f415d66696d1d42855f4ea97" name="ad8de8220f415d66696d1d42855f4ea97"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad8de8220f415d66696d1d42855f4ea97">&#9670;&#160;</a></span>_anneal_beta()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.dqn.dqn_agent.DqnAgent._anneal_beta </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a3968de174bcc4879f641136b85f9a876" name="a3968de174bcc4879f641136b85f9a876"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3968de174bcc4879f641136b85f9a876">&#9670;&#160;</a></span>_apply_prioritization_strategy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.dqn.dqn_agent.DqnAgent._apply_prioritization_strategy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>td_value</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>random_indices</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Void protected method that applies the relevant prioritization strategy for the DQN. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">td_value</td><td>pytorch.Tensor: The computed TD value. </td></tr>
    <tr><td class="paramname">random_indices</td><td>The indices of randomly sampled transitions. </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented in <a class="el" href="classrlpack_1_1dqn_1_1dqn__proportional__prioritization__agent_1_1_dqn_proportional_prioritization_agent.html#a0ae9dd55db77107cbede76750eabf702">rlpack.dqn.dqn_proportional_prioritization_agent.DqnProportionalPrioritizationAgent</a>, and <a class="el" href="classrlpack_1_1dqn_1_1dqn__rank__based__prioritization__agent_1_1_dqn_rank_based_prioritization_agent.html#a83c7c78af254a2ab88dd023d6d0f9049">rlpack.dqn.dqn_rank_based_prioritization_agent.DqnRankBasedPrioritizationAgent</a>.</p>

</div>
</div>
<a id="a49f009c7c02963bf41d79725a42be88d" name="a49f009c7c02963bf41d79725a42be88d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a49f009c7c02963bf41d79725a42be88d">&#9670;&#160;</a></span>_decay_epsilon()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.dqn.dqn_agent.DqnAgent._decay_epsilon </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method to decay epsilon. </p>
<p >This method is called every <code>epsilon_decay_frequency</code> timesteps and decays the epsilon by <code>epsilon_decay_rate</code>, both supplied in <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html" title="This class implements the basic DQN methodology, i.e.">DqnAgent</a> class' constructor. </p>

</div>
</div>
<a id="a83c6dbcbc01d7051982e8c9d45479b81" name="a83c6dbcbc01d7051982e8c9d45479b81"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a83c6dbcbc01d7051982e8c9d45479b81">&#9670;&#160;</a></span>_grad_mean_reduction()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.dqn.dqn_agent.DqnAgent._grad_mean_reduction </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Performs mean reduction and assigns the policy model's parameter the mean reduced gradients. </p>

</div>
</div>
<a id="ab6cf3494ffe6a356e429f1ed01d4f246" name="ab6cf3494ffe6a356e429f1ed01d4f246"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab6cf3494ffe6a356e429f1ed01d4f246">&#9670;&#160;</a></span>_infer_action()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> np.ndarray rlpack.dqn.dqn_agent.DqnAgent._infer_action </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>call_from_policy</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Helper method to support action inference form policy model. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>pytorch.Tensor: The current state of the agent in the environment </td></tr>
    <tr><td class="paramname">call_from_policy</td><td>bool: The flag indicating if the method is being from <code><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a4aef4b46067746d31a0015fa71698440" title="The policy for the agent.">DqnAgent.policy</a></code> method or not. Default: True </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>np.ndarray: The discrete action </dd></dl>

</div>
</div>
<a id="ac42d3b5e16d601cb8fe098cfac5150ca" name="ac42d3b5e16d601cb8fe098cfac5150ca"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac42d3b5e16d601cb8fe098cfac5150ca">&#9670;&#160;</a></span>_load_random_experiences()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[
        pytorch.Tensor,
        pytorch.Tensor,
        pytorch.Tensor,
        pytorch.Tensor,
        pytorch.Tensor,
        pytorch.Tensor,
        pytorch.Tensor,
        pytorch.Tensor,
        pytorch.Tensor,
    ] rlpack.dqn.dqn_agent.DqnAgent._load_random_experiences </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This method loads random transitions from memory. </p>
<p >This may also include forced terminal states if supplied <code>force_terminal_state_selection_prob</code> &gt; 0 in <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html" title="This class implements the basic DQN methodology, i.e.">DqnAgent</a> constructor for each batch. i.e. if force_terminal_state_selection_prob = 0.1, approximately every 1 in 10 batches will have at least one terminal state forced by the loader.</p>
<dl class="section return"><dt>Returns</dt><dd>Tuple[ pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, pytorch.Tensor, ]: The tuple of tensors as (states_current, states_next, rewards, actions, dones, priorities, probabilities, weights, random_indices). </dd></dl>

</div>
</div>
<a id="a91c03751ec814405b70c28affd48e3cd" name="a91c03751ec814405b70c28affd48e3cd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a91c03751ec814405b70c28affd48e3cd">&#9670;&#160;</a></span>_temporal_difference()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> pytorch.Tensor rlpack.dqn.dqn_agent.DqnAgent._temporal_difference </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>rewards</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor&#160;</td>
          <td class="paramname"><em>q_values</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.Tensor
    &#160;</td>
          <td class="paramname"><em>dones</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This method computes the temporal difference for given transitions. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">rewards</td><td>pytorch.Tensor: The sampled batch of rewards. </td></tr>
    <tr><td class="paramname">q_values</td><td>pytorch.Tensor: The q-values inferred from target_model. </td></tr>
    <tr><td class="paramname">dones</td><td>pytorch.Tensor: The done values for each transition in the batch. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>pytorch.Tensor: The TD value for each sample in the batch. </dd></dl>

</div>
</div>
<a id="a268d4194463b2d907864232da38ab95d" name="a268d4194463b2d907864232da38ab95d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a268d4194463b2d907864232da38ab95d">&#9670;&#160;</a></span>_train_policy_model()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.dqn.dqn_agent.DqnAgent._train_policy_model </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method of the class to train the policy model. </p>
<p >This method is called every <code>policy_model_update_rate</code> timesteps supplied in the <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html" title="This class implements the basic DQN methodology, i.e.">DqnAgent</a> class constructor. This method will load the random samples from memory (number of samples depend on <code>batch_size</code> supplied in <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html" title="This class implements the basic DQN methodology, i.e.">DqnAgent</a> constructor), and train the policy_model. </p>

</div>
</div>
<a id="aed71771ef5953d2926a911d70c41152c" name="aed71771ef5953d2926a911d70c41152c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed71771ef5953d2926a911d70c41152c">&#9670;&#160;</a></span>_update_target_model()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.dqn.dqn_agent.DqnAgent._update_target_model </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method of the class to update the target model. </p>
<p >This method is called every <code>target_model_update_rate</code> timesteps supplied in the <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html" title="This class implements the basic DQN methodology, i.e.">DqnAgent</a> class constructor. </p>

</div>
</div>
<a id="adfd9b38d7e3b4f55eb7f857b0285ae0c" name="adfd9b38d7e3b4f55eb7f857b0285ae0c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adfd9b38d7e3b4f55eb7f857b0285ae0c">&#9670;&#160;</a></span>load()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.dqn.dqn_agent.DqnAgent.load </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method loads the target_model, policy_model, optimizer, lr_scheduler and agent_states from the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to load the best model by a custom suffix supplied for evaluation. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ac493d40ce8bd5562822a01aba0265181">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="a4aef4b46067746d31a0015fa71698440" name="a4aef4b46067746d31a0015fa71698440"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4aef4b46067746d31a0015fa71698440">&#9670;&#160;</a></span>policy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.ndarray rlpack.dqn.dqn_agent.DqnAgent.policy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[ndarray, pytorch.Tensor, List[float]]
    &#160;</td>
          <td class="paramname"><em>state_current</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The policy for the agent. </p>
<p >This runs the inference on policy model with <code>state_current</code> and uses q-values to obtain the best action. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[ndarray, pytorch.Tensor, List[float]]: The current state agent is in. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>np.ndarray: The action to be taken. </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#ab5e3e4db83e80ef7bb422a148cd3e1f6">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="aabbebafa737394e23228307873976a6b" name="aabbebafa737394e23228307873976a6b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aabbebafa737394e23228307873976a6b">&#9670;&#160;</a></span>save()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None rlpack.dqn.dqn_agent.DqnAgent.save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>custom_name_suffix</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This method saves the target_model, policy_model, optimizer, lr_scheduler and agent_states in the supplied <code>save_path</code> argument in the DQN Agent class' constructor (also called <b>init</b>). </p>
<p >agent_states includes current memory and epsilon values in a dictionary. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">custom_name_suffix</td><td>Optional[str]: If supplied, additional suffix is added to names of target_model, policy_model, optimizer and lr_scheduler. Useful to save best model by a custom suffix supplied during a train run. Default: None </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#aa61ea2248a43a7bbc9b7c9ab7c240564">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<a id="ab70a7ecba4b187924c1886fc1060a341" name="ab70a7ecba4b187924c1886fc1060a341"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab70a7ecba4b187924c1886fc1060a341">&#9670;&#160;</a></span>train()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> np.ndarray rlpack.dqn.dqn_agent.DqnAgent.train </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_current</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]&#160;</td>
          <td class="paramname"><em>state_next</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, float]&#160;</td>
          <td class="paramname"><em>reward</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, float]&#160;</td>
          <td class="paramname"><em>action</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[bool, int]&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Union[pytorch.Tensor, np.ndarray, float]] &#160;</td>
          <td class="paramname"><em>priority</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Union[pytorch.Tensor, np.ndarray, float]] &#160;</td>
          <td class="paramname"><em>probability</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Union[pytorch.Tensor, np.ndarray, float]] &#160;</td>
          <td class="paramname"><em>weight</em> = <code>1.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<ul>
<li>The training method for agent, which accepts a transition from environment and returns an action for next transition. Use this method when you intend to train the agent.</li>
<li>This method will also run the policy to yield the best action for the given state.</li>
<li>For each transition (or experience) being passed, associated priority, probability and weight can be passed.</li>
</ul>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state_current</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The current state in the environment. </td></tr>
    <tr><td class="paramname">state_next</td><td>Union[pytorch.Tensor, np.ndarray, List[Union[float, int]]]: The next state returned by the environment. </td></tr>
    <tr><td class="paramname">reward</td><td>Union[int, float]: Reward obtained by performing the action for the transition. </td></tr>
    <tr><td class="paramname">action</td><td>Union[int, float]: Action taken for the transition </td></tr>
    <tr><td class="paramname">done</td><td>Union[bool, int]: Indicates weather episode has terminated or not. </td></tr>
    <tr><td class="paramname">priority</td><td>Optional[Union[pytorch.Tensor, np.ndarray, float]]: The priority of the transition: for priority relay memory). Default: 1.0 </td></tr>
    <tr><td class="paramname">probability</td><td>Optional[Union[pytorch.Tensor, np.ndarray, float]]: The probability of the transition : for priority relay memory). Default: 1.0 </td></tr>
    <tr><td class="paramname">weight</td><td>Optional[Union[pytorch.Tensor, np.ndarray, float]]: The important sampling weight of the transition: for priority relay memory). Default: 1.0 </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>np.ndarray: The next action to be taken from <code>state_next</code>. </dd></dl>

<p>Reimplemented from <a class="el" href="classrlpack_1_1utils_1_1base_1_1agent_1_1_agent.html#a38c313422ef6c713efd5ef9301b35111">rlpack.utils.base.agent.Agent</a>.</p>

</div>
</div>
<h2 class="groupheader">Field Documentation</h2>
<a id="a9ed207f175d082efc510ccd9ada18fc5" name="a9ed207f175d082efc510ccd9ada18fc5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9ed207f175d082efc510ccd9ada18fc5">&#9670;&#160;</a></span>__prioritization_strategy_code</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.__prioritization_strategy_code</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The prioritization strategy code. </p>

</div>
</div>
<a id="a37ae5ac7561ac6f7e276fb3b4b0bb320" name="a37ae5ac7561ac6f7e276fb3b4b0bb320"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a37ae5ac7561ac6f7e276fb3b4b0bb320">&#9670;&#160;</a></span>_grad_accumulator</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent._grad_accumulator</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The list of gradients from each backward call. </p>
<p >This is only used when boostrap_rounds &gt; 1 and is cleared after each boostrap round. The <a class="el" href="classrlpack_1_1___c_1_1grad__accumulator_1_1_grad_accumulator.html" title="This class provides the python interface to C_GradAccumulator, the C++ class which performs heavier w...">rlpack._C.grad_accumulator.GradAccumulator</a> object for grad accumulation. </p>

</div>
</div>
<a id="ab9c2100fbbd42aff6e85fe8b199b393e" name="ab9c2100fbbd42aff6e85fe8b199b393e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab9c2100fbbd42aff6e85fe8b199b393e">&#9670;&#160;</a></span>_normalization</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent._normalization</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The normalisation tool to be used for agent. </p>
<p >An instance of <a class="el" href="classrlpack_1_1utils_1_1normalization_1_1_normalization.html" title="Normalization class providing methods for normalization techniques.">rlpack.utils.normalization.Normalization</a>. </p>

</div>
</div>
<a id="add872066ef9a8e4f43bbcb765fb66103" name="add872066ef9a8e4f43bbcb765fb66103"></a>
<h2 class="memtitle"><span class="permalink"><a href="#add872066ef9a8e4f43bbcb765fb66103">&#9670;&#160;</a></span>apply_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.apply_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm</code> argument; indicating the normalisation to be used. </p>

</div>
</div>
<a id="a65bbfa35c0a6c51cc2d7500a118aff5e" name="a65bbfa35c0a6c51cc2d7500a118aff5e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a65bbfa35c0a6c51cc2d7500a118aff5e">&#9670;&#160;</a></span>apply_norm_to</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.apply_norm_to</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>apply_norm_to</code> argument; indicating the quantity to normalise. </p>

</div>
</div>
<a id="aeb63fe78cd7970482dd191f9186a54b4" name="aeb63fe78cd7970482dd191f9186a54b4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeb63fe78cd7970482dd191f9186a54b4">&#9670;&#160;</a></span>backup_frequency</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.backup_frequency</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input model backup frequency in terms of timesteps. </p>

</div>
</div>
<a id="a0dcef173b93afb6eeb2c2f2bbed8fe7d" name="a0dcef173b93afb6eeb2c2f2bbed8fe7d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0dcef173b93afb6eeb2c2f2bbed8fe7d">&#9670;&#160;</a></span>batch_size</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.batch_size</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The batch size to be used when training policy model. </p>
<p >Corresponding number of samples are drawn from <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#a9855b6ae475270ab00627e7f9669adc2">memory</a> as per the prioritization strategy </p>

</div>
</div>
<a id="a1e74b6687e80197fc0764726533899bb" name="a1e74b6687e80197fc0764726533899bb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1e74b6687e80197fc0764726533899bb">&#9670;&#160;</a></span>bootstrap_rounds</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.bootstrap_rounds</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input boostrap rounds. </p>

</div>
</div>
<a id="a7c4db3301269ef487097c5d7a89e0f70" name="a7c4db3301269ef487097c5d7a89e0f70"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7c4db3301269ef487097c5d7a89e0f70">&#9670;&#160;</a></span>clip_grad_value</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.clip_grad_value</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>clip_grad_value</code>; indicating the clipping range for gradients. </p>

</div>
</div>
<a id="aa24c4cde1b3c86793aa0c163975f7e46" name="aa24c4cde1b3c86793aa0c163975f7e46"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa24c4cde1b3c86793aa0c163975f7e46">&#9670;&#160;</a></span>device</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.device</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>device</code> argument; indicating the device name. </p>

</div>
</div>
<a id="ab587b11f42305490ece52cb5683cee6d" name="ab587b11f42305490ece52cb5683cee6d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab587b11f42305490ece52cb5683cee6d">&#9670;&#160;</a></span>dim_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.dim_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>dim_for_norm</code> argument; indicating dimension along which we wish to normalise. </p>

</div>
</div>
<a id="ac2f3ca72d4ac525130ddab5c3ab08f3c" name="ac2f3ca72d4ac525130ddab5c3ab08f3c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac2f3ca72d4ac525130ddab5c3ab08f3c">&#9670;&#160;</a></span>eps_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.eps_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>eps_for_norm</code> argument; indicating epsilon to be used for normalisation. </p>

</div>
</div>
<a id="a4c8074b05362bbcf9c2518938e43ce24" name="a4c8074b05362bbcf9c2518938e43ce24"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4c8074b05362bbcf9c2518938e43ce24">&#9670;&#160;</a></span>epsilon</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.epsilon</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input exploration factor. </p>

</div>
</div>
<a id="ad9c0ee89d768180a64862811f202d57f" name="ad9c0ee89d768180a64862811f202d57f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad9c0ee89d768180a64862811f202d57f">&#9670;&#160;</a></span>epsilon_decay_frequency</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.epsilon_decay_frequency</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input epsilon decay frequency in terms of timesteps. </p>

</div>
</div>
<a id="a32122333d34d04772bc5a5882923c8fe" name="a32122333d34d04772bc5a5882923c8fe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a32122333d34d04772bc5a5882923c8fe">&#9670;&#160;</a></span>epsilon_decay_rate</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.epsilon_decay_rate</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input epsilon decay rate. </p>

</div>
</div>
<a id="ad8dae08b557f3cd26031efe5abc65225" name="ad8dae08b557f3cd26031efe5abc65225"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad8dae08b557f3cd26031efe5abc65225">&#9670;&#160;</a></span>force_terminal_state_selection_prob</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.force_terminal_state_selection_prob</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>force_terminal_state_selection_prob</code>. </p>
<p >This indicates the probability to force at least one terminal state sample in a batch. <br  />
 </p>

</div>
</div>
<a id="a8599ea966ed65fd8cb9a1731b0b8cee6" name="a8599ea966ed65fd8cb9a1731b0b8cee6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8599ea966ed65fd8cb9a1731b0b8cee6">&#9670;&#160;</a></span>gamma</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.gamma</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input discounting factor. </p>

</div>
</div>
<a id="a1e27aba2fbce8d9ec056ae95fd97f1f1" name="a1e27aba2fbce8d9ec056ae95fd97f1f1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1e27aba2fbce8d9ec056ae95fd97f1f1">&#9670;&#160;</a></span>grad_norm_p</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.grad_norm_p</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>grad_norm_p</code>; indicating the p-value for p-normalisation for gradient clippings. </p>

</div>
</div>
<a id="a16b0b31d7e47cf9c56f09cf2b1c6c733" name="a16b0b31d7e47cf9c56f09cf2b1c6c733"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a16b0b31d7e47cf9c56f09cf2b1c6c733">&#9670;&#160;</a></span>loss_function</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.loss_function</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input loss function. </p>

</div>
</div>
<a id="ac449f6bbe49ee6fb8005bb232b653147" name="ac449f6bbe49ee6fb8005bb232b653147"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac449f6bbe49ee6fb8005bb232b653147">&#9670;&#160;</a></span>lr_scheduler</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.lr_scheduler</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optional LR Scheduler (this can be None). </p>

</div>
</div>
<a id="adf407eb3047d5241143aad5eab202ee6" name="adf407eb3047d5241143aad5eab202ee6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adf407eb3047d5241143aad5eab202ee6">&#9670;&#160;</a></span>lr_threshold</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.lr_threshold</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input LR Threshold. </p>

</div>
</div>
<a id="a634dc4ae6c73036f3fbe223929765dcf" name="a634dc4ae6c73036f3fbe223929765dcf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a634dc4ae6c73036f3fbe223929765dcf">&#9670;&#160;</a></span>max_grad_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.max_grad_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>max_grad_norm</code>; indicating the maximum gradient norm for gradient clippings. </p>

</div>
</div>
<a id="a9855b6ae475270ab00627e7f9669adc2" name="a9855b6ae475270ab00627e7f9669adc2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9855b6ae475270ab00627e7f9669adc2">&#9670;&#160;</a></span>memory</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.memory</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The instance of <a class="el" href="classrlpack_1_1___c_1_1memory_1_1_memory.html">rlpack._C.memory.Memory</a> used for Replay buffer. </p>

</div>
</div>
<a id="ac4dd2c4a11614357b3215c406c97b90f" name="ac4dd2c4a11614357b3215c406c97b90f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac4dd2c4a11614357b3215c406c97b90f">&#9670;&#160;</a></span>memory_buffer_size</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.memory_buffer_size</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input argument <code>memory_buffer_size</code>; indicating the buffer size used. </p>

</div>
</div>
<a id="a8983773caff8991f5a1429d44fbdd974" name="a8983773caff8991f5a1429d44fbdd974"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8983773caff8991f5a1429d44fbdd974">&#9670;&#160;</a></span>min_epsilon</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.min_epsilon</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input minimum exploration factor after decays. </p>

</div>
</div>
<a id="a5a6cabaa890c0f28fbedbce96869a500" name="a5a6cabaa890c0f28fbedbce96869a500"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5a6cabaa890c0f28fbedbce96869a500">&#9670;&#160;</a></span>num_actions</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.num_actions</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input number of actions. </p>

</div>
</div>
<a id="a5bc53e8b41e11abbadb20139877910c7" name="a5bc53e8b41e11abbadb20139877910c7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5bc53e8b41e11abbadb20139877910c7">&#9670;&#160;</a></span>optimizer</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.optimizer</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input optimizer wrapped with policy_model parameters. </p>

</div>
</div>
<a id="a9e82951d76c6c5db30d23c13d5286981" name="a9e82951d76c6c5db30d23c13d5286981"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9e82951d76c6c5db30d23c13d5286981">&#9670;&#160;</a></span>p_for_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.p_for_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>p_for_norm</code> argument; indicating p-value for p-normalisation. </p>

</div>
</div>
<a id="aa8adcc90bb59c4bd0f1a153f89c3116d" name="aa8adcc90bb59c4bd0f1a153f89c3116d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa8adcc90bb59c4bd0f1a153f89c3116d">&#9670;&#160;</a></span>policy_model</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.policy_model</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input policy model. </p>

</div>
</div>
<a id="a37d7273d3b9e856c3858c34b6034c770" name="a37d7273d3b9e856c3858c34b6034c770"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a37d7273d3b9e856c3858c34b6034c770">&#9670;&#160;</a></span>policy_model_update_rate</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.policy_model_update_rate</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input argument <code>policy_model_update_rate</code>; indicating the update rate of policy model. </p>
<p >Optimizer is called every <code>policy_model_update_rate</code>. </p>

</div>
</div>
<a id="aecf463bba48c45f10bdf5ba8d8d978bc" name="aecf463bba48c45f10bdf5ba8d8d978bc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aecf463bba48c45f10bdf5ba8d8d978bc">&#9670;&#160;</a></span>prioritization_params</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.prioritization_params</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input prioritization parameters. </p>

</div>
</div>
<a id="a7ebb3729ba7414e487b839ffa2198f7a" name="a7ebb3729ba7414e487b839ffa2198f7a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7ebb3729ba7414e487b839ffa2198f7a">&#9670;&#160;</a></span>save_path</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.save_path</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input save path for backing up agent models. </p>

</div>
</div>
<a id="a1ba7d14eb2a55b43b904c7f89ff37148" name="a1ba7d14eb2a55b43b904c7f89ff37148"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1ba7d14eb2a55b43b904c7f89ff37148">&#9670;&#160;</a></span>step_counter</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.step_counter</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The step counter; counting the total timesteps done so far up to <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#ac4dd2c4a11614357b3215c406c97b90f">memory_buffer_size</a>. </p>

</div>
</div>
<a id="aba41d3fc796d221f77fc8e501b01e86e" name="aba41d3fc796d221f77fc8e501b01e86e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aba41d3fc796d221f77fc8e501b01e86e">&#9670;&#160;</a></span>target_model</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.target_model</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input target model. </p>
<p >This model's parameters are frozen. </p>

</div>
</div>
<a id="ac2f64d463f70a32e203e79d069722c84" name="ac2f64d463f70a32e203e79d069722c84"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac2f64d463f70a32e203e79d069722c84">&#9670;&#160;</a></span>target_model_update_rate</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.target_model_update_rate</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input argument <code>target_model_update_rate</code>; indicating the update rate of target model. </p>
<p >A soft copy of parameters takes place form policy_model to target model as per the update rate </p>

</div>
</div>
<a id="a2a641f2954d39a51703ce14d3490ffd5" name="a2a641f2954d39a51703ce14d3490ffd5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2a641f2954d39a51703ce14d3490ffd5">&#9670;&#160;</a></span>tau</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rlpack.dqn.dqn_agent.DqnAgent.tau</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input <code>tau</code>; indicating the soft update used to update <a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html#aba41d3fc796d221f77fc8e501b01e86e">target_model</a> parameters. </p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacerlpack.html">rlpack</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1dqn.html">dqn</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1dqn_1_1dqn__agent.html">dqn_agent</a></li><li class="navelem"><a class="el" href="classrlpack_1_1dqn_1_1dqn__agent_1_1_dqn_agent.html">DqnAgent</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
