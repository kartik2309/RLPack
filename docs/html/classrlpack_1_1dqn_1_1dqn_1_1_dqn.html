<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RLPack: rlpack.dqn.dqn.Dqn Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="RLPack-logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">RLPack
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classrlpack_1_1dqn_1_1dqn_1_1_dqn.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pri-static-methods">Static Private Member Functions</a>  </div>
  <div class="headertitle"><div class="title">rlpack.dqn.dqn.Dqn Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>This is a helper class that selects the correct the variant of DQN implementations based on prioritization strategy determined by the argument <code>prioritization_params</code>.  
 <a href="classrlpack_1_1dqn_1_1dqn_1_1_dqn.html#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a3f6af8d5c52eece10c6b950c37932c35"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn_1_1_dqn.html#a3f6af8d5c52eece10c6b950c37932c35">__new__</a> (cls, pytorch.nn.Module target_model, pytorch.nn.Module policy_model, pytorch.optim.Optimizer optimizer, Union[LRScheduler, None] lr_scheduler, LossFunction loss_function, float gamma, float epsilon, float min_epsilon, float epsilon_decay_rate, int epsilon_decay_frequency, int memory_buffer_size, int target_model_update_rate, int policy_model_update_rate, int backup_frequency, float lr_threshold, int batch_size, int num_actions, str save_path, int bootstrap_rounds=1, str device=&quot;cpu&quot;, Optional[Dict[str, Any]] prioritization_params=None, float force_terminal_state_selection_prob=0.0, float tau=1.0, int apply_norm=-1, int apply_norm_to=-1, float eps_for_norm=5e-12, int p_for_norm=2, int dim_for_norm=0, Optional[float] max_grad_norm=None, float grad_norm_p=2.0)</td></tr>
<tr class="separator:a3f6af8d5c52eece10c6b950c37932c35"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pri-static-methods" name="pri-static-methods"></a>
Static Private Member Functions</h2></td></tr>
<tr class="memitem:abc2eef64f62abc555261093ad2c7a1a7"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn_1_1_dqn.html#abc2eef64f62abc555261093ad2c7a1a7">__anneal_alpha_default_fn</a> (float alpha, float alpha_annealing_factor)</td></tr>
<tr class="memdesc:abc2eef64f62abc555261093ad2c7a1a7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method to anneal alpha parameter for important sampling weights.  <a href="classrlpack_1_1dqn_1_1dqn_1_1_dqn.html#abc2eef64f62abc555261093ad2c7a1a7">More...</a><br /></td></tr>
<tr class="separator:abc2eef64f62abc555261093ad2c7a1a7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae320a1e6550b7e479854ae7885e0e634"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn_1_1_dqn.html#ae320a1e6550b7e479854ae7885e0e634">__anneal_beta_default_fn</a> (float beta, float beta_annealing_factor)</td></tr>
<tr class="memdesc:ae320a1e6550b7e479854ae7885e0e634"><td class="mdescLeft">&#160;</td><td class="mdescRight">Protected method to anneal beta parameter for important sampling weights.  <a href="classrlpack_1_1dqn_1_1dqn_1_1_dqn.html#ae320a1e6550b7e479854ae7885e0e634">More...</a><br /></td></tr>
<tr class="separator:ae320a1e6550b7e479854ae7885e0e634"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad27f1bdffabfcd6b8b80c0014e3c0f76"><td class="memItemLeft" align="right" valign="top">Dict[str, Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrlpack_1_1dqn_1_1dqn_1_1_dqn.html#ad27f1bdffabfcd6b8b80c0014e3c0f76">__process_prioritization_params</a> (Dict[str, Any] prioritization_params, int prioritization_strategy_code, Callable[[float, float], float] anneal_alpha_default_fn, Callable[[float, float], float] anneal_beta_default_fn, int batch_size)</td></tr>
<tr class="memdesc:ad27f1bdffabfcd6b8b80c0014e3c0f76"><td class="mdescLeft">&#160;</td><td class="mdescRight">Private method to process the prioritization parameters.  <a href="classrlpack_1_1dqn_1_1dqn_1_1_dqn.html#ad27f1bdffabfcd6b8b80c0014e3c0f76">More...</a><br /></td></tr>
<tr class="separator:ad27f1bdffabfcd6b8b80c0014e3c0f76"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >This is a helper class that selects the correct the variant of DQN implementations based on prioritization strategy determined by the argument <code>prioritization_params</code>. </p>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a id="abc2eef64f62abc555261093ad2c7a1a7" name="abc2eef64f62abc555261093ad2c7a1a7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abc2eef64f62abc555261093ad2c7a1a7">&#9670;&#160;</a></span>__anneal_alpha_default_fn()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> float rlpack.dqn.dqn.Dqn.__anneal_alpha_default_fn </td>
          <td>(</td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>alpha_annealing_factor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method to anneal alpha parameter for important sampling weights. </p>
<p >This will be called every <code>alpha_annealing_frequency</code> times. <code>alpha_annealing_frequency</code> is a key to be passed in dictionary <code>prioritization_params</code> argument in the DqnAgent class' constructor. This method is called by default to anneal alpha.</p>
<p >If <code>alpha_annealing_frequency</code> is not passed in <code>prioritization_params</code>, the annealing of alpha will not take place. This method uses another value <code>alpha_annealing_factor</code> that must also be passed in <code>prioritization_params</code>. <code>alpha_annealing_factor</code> is typically below 1 to slowly annealed it to 0 or <code>min_alpha</code>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">alpha</td><td>float: The input alpha value to anneal. </td></tr>
    <tr><td class="paramname">alpha_annealing_factor</td><td>float: The annealing factor to be used to anneal alpha. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>float: Annealed alpha. </dd></dl>

</div>
</div>
<a id="ae320a1e6550b7e479854ae7885e0e634" name="ae320a1e6550b7e479854ae7885e0e634"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae320a1e6550b7e479854ae7885e0e634">&#9670;&#160;</a></span>__anneal_beta_default_fn()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> float rlpack.dqn.dqn.Dqn.__anneal_beta_default_fn </td>
          <td>(</td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>beta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>beta_annealing_factor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Protected method to anneal beta parameter for important sampling weights. </p>
<p >This will be called every <code>beta_annealing_frequency</code> times. <code>beta_annealing_frequency</code> is a key to be passed in dictionary <code>prioritization_params</code> argument in the DqnAgent class' constructor.</p>
<p >If <code>beta_annealing_frequency</code> is not passed in <code>prioritization_params</code>, the annealing of beta will not take place. This method uses another value <code>beta_annealing_factor</code> that must also be passed in <code>prioritization_params</code>. <code>beta_annealing_factor</code> is typically above 1 to slowly annealed it to 1 or <code>max_beta</code></p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">beta</td><td>float: The input beta value to anneal. </td></tr>
    <tr><td class="paramname">beta_annealing_factor</td><td>float: The annealing factor to be used to anneal beta. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>float: Annealed beta. </dd></dl>

</div>
</div>
<a id="a3f6af8d5c52eece10c6b950c37932c35" name="a3f6af8d5c52eece10c6b950c37932c35"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3f6af8d5c52eece10c6b950c37932c35">&#9670;&#160;</a></span>__new__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def rlpack.dqn.dqn.Dqn.__new__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>cls</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.nn.Module&#160;</td>
          <td class="paramname"><em>target_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.nn.Module&#160;</td>
          <td class="paramname"><em>policy_model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">pytorch.optim.Optimizer&#160;</td>
          <td class="paramname"><em>optimizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[LRScheduler, None]&#160;</td>
          <td class="paramname"><em>lr_scheduler</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">LossFunction&#160;</td>
          <td class="paramname"><em>loss_function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>epsilon</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>min_epsilon</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>epsilon_decay_rate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>epsilon_decay_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>memory_buffer_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>target_model_update_rate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>policy_model_update_rate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>backup_frequency</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>lr_threshold</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>batch_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>save_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>bootstrap_rounds</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>device</em> = <code>&quot;cpu&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Dict[str, Any]] &#160;</td>
          <td class="paramname"><em>prioritization_params</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>force_terminal_state_selection_prob</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>tau</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>apply_norm</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>apply_norm_to</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>eps_for_norm</em> = <code>5e-12</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>p_for_norm</em> = <code>2</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>dim_for_norm</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>max_grad_norm</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>grad_norm_p</em> = <code>2.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">target_model</td><td>nn.Module: The target network for DQN model. This the network which has its weights frozen. </td></tr>
    <tr><td class="paramname">policy_model</td><td>nn.Module: The policy network for DQN model. This is the network which is trained. </td></tr>
    <tr><td class="paramname">optimizer</td><td>optim.Optimizer: The optimizer wrapped with policy model's parameters. </td></tr>
    <tr><td class="paramname">lr_scheduler</td><td>Union[LRScheduler, None]: The PyTorch LR Scheduler with wrapped optimizer. </td></tr>
    <tr><td class="paramname">loss_function</td><td>LossFunction: The loss function from PyTorch's nn module. Initialized instance must be passed. </td></tr>
    <tr><td class="paramname">gamma</td><td>float: The gamma value for agent. </td></tr>
    <tr><td class="paramname">epsilon</td><td>float: The initial epsilon for the agent. </td></tr>
    <tr><td class="paramname">min_epsilon</td><td>float: The minimum epsilon for the agent. Once this value is reached, it is maintained for all further episodes. </td></tr>
    <tr><td class="paramname">epsilon_decay_rate</td><td>float: The decay multiplier to decay the epsilon. </td></tr>
    <tr><td class="paramname">epsilon_decay_frequency</td><td>int: The number of timesteps after which the epsilon is decayed. </td></tr>
    <tr><td class="paramname">memory_buffer_size</td><td>int: The buffer size of memory; or replay buffer for DQN. </td></tr>
    <tr><td class="paramname">target_model_update_rate</td><td>int: The timesteps after which target model's weights are updated with policy model weights: weights are weighted as per <code>tau</code>: see below)). </td></tr>
    <tr><td class="paramname">policy_model_update_rate</td><td>int: The timesteps after which policy model is trained. This involves backpropagation through the policy network. </td></tr>
    <tr><td class="paramname">backup_frequency</td><td>int: The timesteps after which models are backed up. This will also save optimizer, lr_scheduler and agent_states: epsilon the time of saving and memory. </td></tr>
    <tr><td class="paramname">lr_threshold</td><td>float: The threshold LR which once reached LR scheduler is not called further. </td></tr>
    <tr><td class="paramname">batch_size</td><td>int: The batch size used for inference through target_model and train through policy model </td></tr>
    <tr><td class="paramname">num_actions</td><td>int: Number of actions for the environment. </td></tr>
    <tr><td class="paramname">save_path</td><td>str: The save path for models: target_model and policy_model, optimizer, lr_scheduler and agent_states. </td></tr>
    <tr><td class="paramname">bootstrap_rounds</td><td>int: The number of rounds until which gradients are to be accumulated before performing calling optimizer step. Gradients are mean reduced for bootstrap_rounds &gt; 1. Default: 1. </td></tr>
    <tr><td class="paramname">device</td><td>str: The device on which models are run. Default: "cpu". </td></tr>
    <tr><td class="paramname">prioritization_params</td><td>Optional[Dict[str, Any]]: The parameters for prioritization in prioritized memory: or relay buffer). Default: None. </td></tr>
    <tr><td class="paramname">force_terminal_state_selection_prob</td><td>float: The probability for forcefully selecting a terminal state in a batch. Default: 0.0. </td></tr>
    <tr><td class="paramname">tau</td><td>float: The weighted update of weights from policy_model to target_model. This is done by formula target_weight = tau * policy_weight +: 1 - tau) * target_weight/. Default: -1. </td></tr>
    <tr><td class="paramname">apply_norm</td><td>Union[int, str]: The code to select the normalization procedure to be applied on selected quantities; selected by <code>apply_norm_to</code>: see below)). Direct string can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1 </td></tr>
    <tr><td class="paramname">apply_norm_to</td><td>Union[int, List[str]]: The code to select the quantity to which normalization is to be applied. Direct list of quantities can also be passed as per accepted keys. Refer below in Notes to see the accepted values. Default: -1. </td></tr>
    <tr><td class="paramname">eps_for_norm</td><td>float: Epsilon value for normalization: for numeric stability. For min-max normalization and standardized normalization. Default: 5e-12. </td></tr>
    <tr><td class="paramname">p_for_norm</td><td>int: The p value for p-normalization. Default: 2: L2 Norm. </td></tr>
    <tr><td class="paramname">dim_for_norm</td><td>int: The dimension across which normalization is to be performed. Default: 0. </td></tr>
    <tr><td class="paramname">max_grad_norm</td><td>Optional[float]: The max norm for gradients for gradient clipping. Default: None </td></tr>
    <tr><td class="paramname">grad_norm_p</td><td>Optional[float]: The p-value for p-normalization of gradients. Default: 2.0</td></tr>
  </table>
  </dd>
</dl>
<p><b>Notes</b></p>
<p >For prioritization_params, when None: the default is passed, prioritized memory is not used. To use prioritized memory, pass a dictionary with keys <code>alpha</code> and <code>beta</code>. You can also pass <code>alpha_decay_rate</code> and <code>beta_decay_rate</code> additionally.</p>
<p >The code for prioritization strategies are:</p><ul>
<li>Uniform: 0; <code>uniform</code></li>
<li>Proportional: 1; <code>proportional</code></li>
<li>Rank-Based: 2; <code>rank-based</code></li>
</ul>
<p >The codes for <code>apply_norm</code> are given as follows: -</p><ul>
<li>No Normalization: -1; (<code>"none"</code>)</li>
<li>Min-Max Normalization: 0; (<code>"min_max"</code>)</li>
<li>Standardization: 1; (<code>"standardize"</code>)</li>
<li>P-Normalization: 2; (<code>"p_norm"</code>)</li>
</ul>
<p >The codes for <code>apply_norm_to</code> are given as follows:</p><ul>
<li>No Normalization: -1; (<code>["none"]</code>)</li>
<li>On States only: 0; (<code>["states"]</code>)</li>
<li>On Rewards only: 1; (<code>["rewards"]</code>)</li>
<li>On TD value only: 2; (<code>["td"]</code>)</li>
<li>On States and Rewards: 3; (<code>["states", "rewards"]</code>)</li>
<li>On States and TD: 4; (<code>["states", "td"]</code>)</li>
</ul>
<p >If a valid <code>max_norm_grad</code> is passed, then gradient clipping takes place else gradient clipping step is skipped. If <code>max_norm_grad</code> value was invalid, error will be raised from PyTorch. </p>

</div>
</div>
<a id="ad27f1bdffabfcd6b8b80c0014e3c0f76" name="ad27f1bdffabfcd6b8b80c0014e3c0f76"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad27f1bdffabfcd6b8b80c0014e3c0f76">&#9670;&#160;</a></span>__process_prioritization_params()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> Dict[str, Any] rlpack.dqn.dqn.Dqn.__process_prioritization_params </td>
          <td>(</td>
          <td class="paramtype">Dict[str, Any]&#160;</td>
          <td class="paramname"><em>prioritization_params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>prioritization_strategy_code</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Callable[[float, float], float]&#160;</td>
          <td class="paramname"><em>anneal_alpha_default_fn</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Callable[[float, float], float]&#160;</td>
          <td class="paramname"><em>anneal_beta_default_fn</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>batch_size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Private method to process the prioritization parameters. </p>
<p >This includes sanity check and loading of default values of mandatory parameters. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">prioritization_params</td><td>Dict[str, Any]: The prioritization parameters for when we use prioritized memory. </td></tr>
    <tr><td class="paramname">prioritization_strategy_code</td><td>int: The prioritization code corresponding to the given prioritization strategy string. </td></tr>
    <tr><td class="paramname">anneal_alpha_default_fn</td><td>Callable[[float, float], float]: The default annealing function for alpha. </td></tr>
    <tr><td class="paramname">anneal_beta_default_fn</td><td>Callable[[float, float], float]: The default annealing function for beta. </td></tr>
    <tr><td class="paramname">batch_size</td><td>int: The requested batch size; used in rank-based prioritization to determine the number of segments. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Dict[str, Any]: The processed prioritization parameters with necessary parameters loaded. </dd></dl>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacerlpack.html">rlpack</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1dqn.html">dqn</a></li><li class="navelem"><a class="el" href="namespacerlpack_1_1dqn_1_1dqn.html">dqn</a></li><li class="navelem"><a class="el" href="classrlpack_1_1dqn_1_1dqn_1_1_dqn.html">Dqn</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
